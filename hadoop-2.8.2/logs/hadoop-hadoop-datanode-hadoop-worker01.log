2017-12-25 11:42:00,405 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   user = hadoop
STARTUP_MSG:   host = hadoop-worker01.local/192.168.28.131
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.2
STARTUP_MSG:   classpath = /opt/hadoop-2.8.2/etc/hadoop:/opt/hadoop/share/hadoop/common/lib/activation-1.1.jar:/opt/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/opt/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/opt/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/opt/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/opt/hadoop/share/hadoop/common/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/opt/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/opt/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/opt/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/opt/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/opt/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-annotations-2.8.2.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-auth-2.8.2.jar:/opt/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/opt/hadoop/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/opt/hadoop/share/hadoop/common/lib/httpclient-4.5.2.jar:/opt/hadoop/share/hadoop/common/lib/httpcore-4.4.4.jar:/opt/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/common/lib/jcip-annotations-1.0.jar:/opt/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/opt/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/opt/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jsch-0.1.54.jar:/opt/hadoop/share/hadoop/common/lib/json-smart-1.1.1.jar:/opt/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/opt/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/common/lib/junit-4.11.jar:/opt/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/opt/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-3.9.jar:/opt/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/opt/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/opt/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/opt/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/opt/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/opt/hadoop/share/hadoop/common/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/opt/hadoop/share/hadoop/common/hadoop-common-2.8.2-tests.jar:/opt/hadoop/share/hadoop/common/hadoop-common-2.8.2.jar:/opt/hadoop/share/hadoop/common/hadoop-nfs-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs:/opt/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/okio-1.4.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/opt/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/opt/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-math-2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/opt/hadoop/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/opt/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/opt/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/opt/hadoop/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/opt/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/opt/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/opt/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/opt/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/opt/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/opt/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/opt/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/opt/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.2-tests.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.2.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 66c47f2a01ad9637879e95f80c41f798373828fb; compiled by 'jdu' on 2017-10-19T20:39Z
STARTUP_MSG:   java = 1.8.0_152
************************************************************/
2017-12-25 11:42:00,429 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2017-12-25 11:42:02,151 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2017-12-25 11:42:02,328 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2017-12-25 11:42:02,328 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2017-12-25 11:42:02,337 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2017-12-25 11:42:02,339 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is hadoop-worker01.local
2017-12-25 11:42:02,344 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2017-12-25 11:42:02,394 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2017-12-25 11:42:02,398 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 10485760 bytes/s
2017-12-25 11:42:02,398 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 50
2017-12-25 11:42:02,603 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2017-12-25 11:42:02,622 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2017-12-25 11:42:02,639 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2017-12-25 11:42:02,651 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-12-25 11:42:02,654 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2017-12-25 11:42:02,654 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-12-25 11:42:02,654 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2017-12-25 11:42:02,714 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 35285
2017-12-25 11:42:02,715 INFO org.mortbay.log: jetty-6.1.26
2017-12-25 11:42:03,224 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:35285
2017-12-25 11:42:03,906 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2017-12-25 11:42:03,916 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2017-12-25 11:42:04,756 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hadoop
2017-12-25 11:42:04,756 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2017-12-25 11:42:05,045 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2017-12-25 11:42:05,093 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2017-12-25 11:42:05,343 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2017-12-25 11:42:05,372 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2017-12-25 11:42:05,410 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2017-12-25 11:42:05,435 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to hadoop-master/192.168.28.129:8020 starting to offer service
2017-12-25 11:42:05,470 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2017-12-25 11:42:05,472 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2017-12-25 11:42:05,927 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to hadoop-master/192.168.28.129:8020
2017-12-25 11:42:05,935 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2017-12-25 11:42:05,966 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /var/lib/hadoop2/dfs/data/1/in_use.lock acquired by nodename 1839@hadoop-worker01.local
2017-12-25 11:42:05,968 INFO org.apache.hadoop.hdfs.server.common.Storage: Storage directory /var/lib/hadoop2/dfs/data/1 is not formatted for namespace 1794692519. Formatting...
2017-12-25 11:42:05,969 INFO org.apache.hadoop.hdfs.server.common.Storage: Generated new storageID DS-db2b9f5a-2dbc-404d-8b87-9ff8e79ccdd2 for directory /var/lib/hadoop2/dfs/data/1
2017-12-25 11:42:06,048 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /var/lib/hadoop2/dfs/data/2/in_use.lock acquired by nodename 1839@hadoop-worker01.local
2017-12-25 11:42:06,048 INFO org.apache.hadoop.hdfs.server.common.Storage: Storage directory /var/lib/hadoop2/dfs/data/2 is not formatted for namespace 1794692519. Formatting...
2017-12-25 11:42:06,048 INFO org.apache.hadoop.hdfs.server.common.Storage: Generated new storageID DS-99e39a6f-a3c8-4fc0-9709-0aafdcfc87d6 for directory /var/lib/hadoop2/dfs/data/2
2017-12-25 11:42:06,203 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-199680249-192.168.28.129-1514169681712
2017-12-25 11:42:06,203 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712
2017-12-25 11:42:06,204 INFO org.apache.hadoop.hdfs.server.common.Storage: Block pool storage directory /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712 is not formatted for BP-199680249-192.168.28.129-1514169681712. Formatting ...
2017-12-25 11:42:06,204 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting block pool BP-199680249-192.168.28.129-1514169681712 directory /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current
2017-12-25 11:42:06,354 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-199680249-192.168.28.129-1514169681712
2017-12-25 11:42:06,354 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712
2017-12-25 11:42:06,354 INFO org.apache.hadoop.hdfs.server.common.Storage: Block pool storage directory /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712 is not formatted for BP-199680249-192.168.28.129-1514169681712. Formatting ...
2017-12-25 11:42:06,354 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting block pool BP-199680249-192.168.28.129-1514169681712 directory /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current
2017-12-25 11:42:06,370 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1794692519;bpid=BP-199680249-192.168.28.129-1514169681712;lv=-57;nsInfo=lv=-63;cid=CID-a84222ca-b260-441d-81c3-93c4f4f5d131;nsid=1794692519;c=1514169681712;bpid=BP-199680249-192.168.28.129-1514169681712;dnuuid=null
2017-12-25 11:42:06,377 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Generated and persisted new Datanode UUID 46e3cb45-0c76-4bc1-9342-af90beb9d892
2017-12-25 11:42:06,599 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-db2b9f5a-2dbc-404d-8b87-9ff8e79ccdd2
2017-12-25 11:42:06,600 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /var/lib/hadoop2/dfs/data/1/current, StorageType: DISK
2017-12-25 11:42:06,616 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-99e39a6f-a3c8-4fc0-9709-0aafdcfc87d6
2017-12-25 11:42:06,616 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /var/lib/hadoop2/dfs/data/2/current, StorageType: DISK
2017-12-25 11:42:06,635 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2017-12-25 11:42:06,667 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Volume reference is released.
2017-12-25 11:42:06,667 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-199680249-192.168.28.129-1514169681712
2017-12-25 11:42:06,669 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current...
2017-12-25 11:42:06,670 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current...
2017-12-25 11:42:06,720 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-199680249-192.168.28.129-1514169681712 on /var/lib/hadoop2/dfs/data/1/current: 51ms
2017-12-25 11:42:06,720 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-199680249-192.168.28.129-1514169681712 on /var/lib/hadoop2/dfs/data/2/current: 50ms
2017-12-25 11:42:06,721 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-199680249-192.168.28.129-1514169681712: 54ms
2017-12-25 11:42:06,724 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current...
2017-12-25 11:42:06,724 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current...
2017-12-25 11:42:06,725 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/replicas doesn't exist 
2017-12-25 11:42:06,725 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current: 0ms
2017-12-25 11:42:06,725 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/replicas doesn't exist 
2017-12-25 11:42:06,726 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current: 1ms
2017-12-25 11:42:06,726 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 3ms
2017-12-25 11:42:06,728 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: Now scanning bpid BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2
2017-12-25 11:42:06,730 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/var/lib/hadoop2/dfs/data/2, DS-99e39a6f-a3c8-4fc0-9709-0aafdcfc87d6): finished scanning block pool BP-199680249-192.168.28.129-1514169681712
2017-12-25 11:42:06,730 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: Now scanning bpid BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1
2017-12-25 11:42:06,731 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/var/lib/hadoop2/dfs/data/1, DS-db2b9f5a-2dbc-404d-8b87-9ff8e79ccdd2): finished scanning block pool BP-199680249-192.168.28.129-1514169681712
2017-12-25 11:42:06,752 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 12/25/17 1:50 PM with interval of 21600000ms
2017-12-25 11:42:06,775 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-199680249-192.168.28.129-1514169681712 (Datanode Uuid 46e3cb45-0c76-4bc1-9342-af90beb9d892) service to hadoop-master/192.168.28.129:8020 beginning handshake with NN
2017-12-25 11:42:06,818 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/var/lib/hadoop2/dfs/data/2, DS-99e39a6f-a3c8-4fc0-9709-0aafdcfc87d6): no suitable block pools found to scan.  Waiting 1814399910 ms.
2017-12-25 11:42:06,828 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/var/lib/hadoop2/dfs/data/1, DS-db2b9f5a-2dbc-404d-8b87-9ff8e79ccdd2): no suitable block pools found to scan.  Waiting 1814399900 ms.
2017-12-25 11:42:06,862 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-199680249-192.168.28.129-1514169681712 (Datanode Uuid 46e3cb45-0c76-4bc1-9342-af90beb9d892) service to hadoop-master/192.168.28.129:8020 successfully registered with NN
2017-12-25 11:42:06,862 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode hadoop-master/192.168.28.129:8020 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2017-12-25 11:42:07,119 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x68ddc967ef01566c,  containing 2 storage report(s), of which we sent 2. The reports had 0 total blocks and used 1 RPC(s). This took 9 msec to generate and 112 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2017-12-25 11:42:07,119 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-199680249-192.168.28.129-1514169681712
2017-12-25 12:11:50,722 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "hadoop-worker01.local/192.168.28.131"; destination host is: "hadoop-master":8020; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1487)
	at org.apache.hadoop.ipc.Client.call(Client.java:1429)
	at org.apache.hadoop.ipc.Client.call(Client.java:1339)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:154)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:459)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:581)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:775)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1788)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1157)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1053)
2017-12-25 12:11:54,456 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2017-12-25 12:11:54,462 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at hadoop-worker01.local/192.168.28.131
************************************************************/
2017-12-25 12:23:51,214 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   user = hadoop
STARTUP_MSG:   host = hadoop-worker01.local/192.168.28.131
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.2
STARTUP_MSG:   classpath = /opt/hadoop-2.8.2/etc/hadoop:/opt/hadoop/share/hadoop/common/lib/activation-1.1.jar:/opt/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/opt/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/opt/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/opt/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/opt/hadoop/share/hadoop/common/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/opt/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/opt/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/opt/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/opt/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/opt/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-annotations-2.8.2.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-auth-2.8.2.jar:/opt/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/opt/hadoop/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/opt/hadoop/share/hadoop/common/lib/httpclient-4.5.2.jar:/opt/hadoop/share/hadoop/common/lib/httpcore-4.4.4.jar:/opt/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/common/lib/jcip-annotations-1.0.jar:/opt/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/opt/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/opt/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jsch-0.1.54.jar:/opt/hadoop/share/hadoop/common/lib/json-smart-1.1.1.jar:/opt/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/opt/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/common/lib/junit-4.11.jar:/opt/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/opt/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-3.9.jar:/opt/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/opt/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/opt/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/opt/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/opt/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/opt/hadoop/share/hadoop/common/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/opt/hadoop/share/hadoop/common/hadoop-common-2.8.2-tests.jar:/opt/hadoop/share/hadoop/common/hadoop-common-2.8.2.jar:/opt/hadoop/share/hadoop/common/hadoop-nfs-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs:/opt/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/okio-1.4.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/opt/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/opt/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-math-2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/opt/hadoop/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/opt/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/opt/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/opt/hadoop/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/opt/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/opt/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/opt/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/opt/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/opt/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/opt/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/opt/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/opt/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.2-tests.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.2.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 66c47f2a01ad9637879e95f80c41f798373828fb; compiled by 'jdu' on 2017-10-19T20:39Z
STARTUP_MSG:   java = 1.8.0_152
************************************************************/
2017-12-25 12:23:51,227 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2017-12-25 12:23:52,595 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2017-12-25 12:23:52,704 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2017-12-25 12:23:52,704 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2017-12-25 12:23:52,713 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2017-12-25 12:23:52,716 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is hadoop-worker01.local
2017-12-25 12:23:52,722 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2017-12-25 12:23:52,765 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2017-12-25 12:23:52,768 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 10485760 bytes/s
2017-12-25 12:23:52,768 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 50
2017-12-25 12:23:53,011 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2017-12-25 12:23:53,052 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2017-12-25 12:23:53,062 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2017-12-25 12:23:53,075 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-12-25 12:23:53,078 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2017-12-25 12:23:53,078 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-12-25 12:23:53,078 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2017-12-25 12:23:53,109 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 37596
2017-12-25 12:23:53,109 INFO org.mortbay.log: jetty-6.1.26
2017-12-25 12:23:53,484 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:37596
2017-12-25 12:23:53,803 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2017-12-25 12:23:53,808 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2017-12-25 12:23:54,210 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hadoop
2017-12-25 12:23:54,210 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2017-12-25 12:23:54,327 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2017-12-25 12:23:54,365 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2017-12-25 12:23:54,545 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2017-12-25 12:23:54,564 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2017-12-25 12:23:54,610 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2017-12-25 12:23:54,633 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to hadoop-master/192.168.28.129:8020 starting to offer service
2017-12-25 12:23:54,709 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2017-12-25 12:23:54,720 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2017-12-25 12:23:55,369 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to hadoop-master/192.168.28.129:8020
2017-12-25 12:23:55,400 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2017-12-25 12:23:55,450 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /var/lib/hadoop2/dfs/data/1/in_use.lock acquired by nodename 2970@hadoop-worker01.local
2017-12-25 12:23:55,470 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /var/lib/hadoop2/dfs/data/2/in_use.lock acquired by nodename 2970@hadoop-worker01.local
2017-12-25 12:23:55,619 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-199680249-192.168.28.129-1514169681712
2017-12-25 12:23:55,619 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712
2017-12-25 12:23:55,890 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-199680249-192.168.28.129-1514169681712
2017-12-25 12:23:55,890 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712
2017-12-25 12:23:55,893 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1794692519;bpid=BP-199680249-192.168.28.129-1514169681712;lv=-57;nsInfo=lv=-63;cid=CID-a84222ca-b260-441d-81c3-93c4f4f5d131;nsid=1794692519;c=1514169681712;bpid=BP-199680249-192.168.28.129-1514169681712;dnuuid=46e3cb45-0c76-4bc1-9342-af90beb9d892
2017-12-25 12:23:56,045 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-db2b9f5a-2dbc-404d-8b87-9ff8e79ccdd2
2017-12-25 12:23:56,045 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /var/lib/hadoop2/dfs/data/1/current, StorageType: DISK
2017-12-25 12:23:56,050 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-99e39a6f-a3c8-4fc0-9709-0aafdcfc87d6
2017-12-25 12:23:56,050 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /var/lib/hadoop2/dfs/data/2/current, StorageType: DISK
2017-12-25 12:23:56,081 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2017-12-25 12:23:56,091 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Volume reference is released.
2017-12-25 12:23:56,092 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-199680249-192.168.28.129-1514169681712
2017-12-25 12:23:56,098 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current...
2017-12-25 12:23:56,114 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current...
2017-12-25 12:23:56,295 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-199680249-192.168.28.129-1514169681712 on /var/lib/hadoop2/dfs/data/1/current: 184ms
2017-12-25 12:23:56,295 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-199680249-192.168.28.129-1514169681712 on /var/lib/hadoop2/dfs/data/2/current: 181ms
2017-12-25 12:23:56,295 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-199680249-192.168.28.129-1514169681712: 203ms
2017-12-25 12:23:56,299 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current...
2017-12-25 12:23:56,299 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current...
2017-12-25 12:23:56,299 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/replicas doesn't exist 
2017-12-25 12:23:56,299 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/replicas doesn't exist 
2017-12-25 12:23:56,300 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current: 1ms
2017-12-25 12:23:56,300 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current: 1ms
2017-12-25 12:23:56,300 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 2ms
2017-12-25 12:23:56,413 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/var/lib/hadoop2/dfs/data/2, DS-99e39a6f-a3c8-4fc0-9709-0aafdcfc87d6): no suitable block pools found to scan.  Waiting 1811890316 ms.
2017-12-25 12:23:56,413 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/var/lib/hadoop2/dfs/data/1, DS-db2b9f5a-2dbc-404d-8b87-9ff8e79ccdd2): no suitable block pools found to scan.  Waiting 1811890315 ms.
2017-12-25 12:23:56,442 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 12/25/17 1:53 PM with interval of 21600000ms
2017-12-25 12:23:56,527 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-199680249-192.168.28.129-1514169681712 (Datanode Uuid 46e3cb45-0c76-4bc1-9342-af90beb9d892) service to hadoop-master/192.168.28.129:8020 beginning handshake with NN
2017-12-25 12:23:56,693 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-199680249-192.168.28.129-1514169681712 (Datanode Uuid 46e3cb45-0c76-4bc1-9342-af90beb9d892) service to hadoop-master/192.168.28.129:8020 successfully registered with NN
2017-12-25 12:23:56,693 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode hadoop-master/192.168.28.129:8020 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2017-12-25 12:23:58,290 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xd8358eb9be88554e,  containing 2 storage report(s), of which we sent 2. The reports had 0 total blocks and used 1 RPC(s). This took 10 msec to generate and 128 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2017-12-25 12:23:58,290 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-199680249-192.168.28.129-1514169681712
2017-12-25 12:26:36,806 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741825_1001 src: /192.168.28.130:48930 dest: /192.168.28.131:50010
2017-12-25 12:26:40,840 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.130:48930, dest: /192.168.28.131:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-251657011_1, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741825_1001, duration: 3888569222
2017-12-25 12:26:40,840 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741825_1001, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.28.132:50010] terminating
2017-12-25 12:26:40,879 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741826_1002 src: /192.168.28.130:48932 dest: /192.168.28.131:50010
2017-12-25 12:26:42,640 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.130:48932, dest: /192.168.28.131:50010, bytes: 74090689, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-251657011_1, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741826_1002, duration: 1754199036
2017-12-25 12:26:42,641 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741826_1002, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.28.132:50010] terminating
2017-12-25 12:26:42,856 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741827_1003 src: /192.168.28.130:48934 dest: /192.168.28.131:50010
2017-12-25 12:26:42,873 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.130:48934, dest: /192.168.28.131:50010, bytes: 65985, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-251657011_1, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741827_1003, duration: 12259443
2017-12-25 12:26:42,873 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741827_1003, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.28.132:50010] terminating
2017-12-25 12:27:43,077 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741825_1001 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741825 for deletion
2017-12-25 12:27:43,080 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741826_1002 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741826 for deletion
2017-12-25 12:27:43,081 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741827_1003 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741827 for deletion
2017-12-25 12:27:43,082 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741826_1002 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741826
2017-12-25 12:27:43,083 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741825_1001 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741825
2017-12-25 12:27:43,084 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741827_1003 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741827
2017-12-25 12:43:30,733 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "hadoop-worker01.local/192.168.28.131"; destination host is: "hadoop-master":8020; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1487)
	at org.apache.hadoop.ipc.Client.call(Client.java:1429)
	at org.apache.hadoop.ipc.Client.call(Client.java:1339)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:154)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:459)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:581)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:775)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1788)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1157)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1053)
2017-12-25 12:43:34,358 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2017-12-25 12:43:34,365 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at hadoop-worker01.local/192.168.28.131
************************************************************/
2017-12-25 20:38:40,031 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   user = hadoop
STARTUP_MSG:   host = hadoop-worker01.local/192.168.28.131
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.2
STARTUP_MSG:   classpath = /opt/hadoop-2.8.2/etc/hadoop:/opt/hadoop/share/hadoop/common/lib/activation-1.1.jar:/opt/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/opt/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/opt/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/opt/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/opt/hadoop/share/hadoop/common/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/opt/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/opt/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/opt/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/opt/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/opt/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-annotations-2.8.2.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-auth-2.8.2.jar:/opt/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/opt/hadoop/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/opt/hadoop/share/hadoop/common/lib/httpclient-4.5.2.jar:/opt/hadoop/share/hadoop/common/lib/httpcore-4.4.4.jar:/opt/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/common/lib/jcip-annotations-1.0.jar:/opt/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/opt/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/opt/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jsch-0.1.54.jar:/opt/hadoop/share/hadoop/common/lib/json-smart-1.1.1.jar:/opt/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/opt/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/common/lib/junit-4.11.jar:/opt/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/opt/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-3.9.jar:/opt/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/opt/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/opt/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/opt/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/opt/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/opt/hadoop/share/hadoop/common/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/opt/hadoop/share/hadoop/common/hadoop-common-2.8.2-tests.jar:/opt/hadoop/share/hadoop/common/hadoop-common-2.8.2.jar:/opt/hadoop/share/hadoop/common/hadoop-nfs-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs:/opt/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/okio-1.4.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/opt/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/opt/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-math-2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/opt/hadoop/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/opt/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/opt/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/opt/hadoop/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/opt/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/opt/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/opt/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/opt/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/opt/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/opt/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/opt/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/opt/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.2-tests.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.2.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 66c47f2a01ad9637879e95f80c41f798373828fb; compiled by 'jdu' on 2017-10-19T20:39Z
STARTUP_MSG:   java = 1.8.0_152
************************************************************/
2017-12-25 20:38:40,053 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2017-12-25 20:38:41,739 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2017-12-25 20:38:42,029 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2017-12-25 20:38:42,030 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2017-12-25 20:38:42,045 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2017-12-25 20:38:42,047 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is hadoop-worker01.local
2017-12-25 20:38:42,054 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2017-12-25 20:38:42,113 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2017-12-25 20:38:42,118 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 10485760 bytes/s
2017-12-25 20:38:42,118 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 50
2017-12-25 20:38:42,537 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2017-12-25 20:38:42,581 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2017-12-25 20:38:42,605 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2017-12-25 20:38:42,617 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-12-25 20:38:42,620 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2017-12-25 20:38:42,621 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-12-25 20:38:42,621 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2017-12-25 20:38:42,657 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 43839
2017-12-25 20:38:42,658 INFO org.mortbay.log: jetty-6.1.26
2017-12-25 20:38:43,447 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:43839
2017-12-25 20:38:44,198 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2017-12-25 20:38:44,227 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2017-12-25 20:38:44,893 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hadoop
2017-12-25 20:38:44,893 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2017-12-25 20:38:45,113 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2017-12-25 20:38:45,145 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2017-12-25 20:38:45,338 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2017-12-25 20:38:45,378 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2017-12-25 20:38:45,471 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2017-12-25 20:38:45,492 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to hadoop-master/192.168.28.129:8020 starting to offer service
2017-12-25 20:38:45,549 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2017-12-25 20:38:45,551 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2017-12-25 20:38:46,235 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to hadoop-master/192.168.28.129:8020
2017-12-25 20:38:46,240 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2017-12-25 20:38:46,280 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /var/lib/hadoop2/dfs/data/1/in_use.lock acquired by nodename 1982@hadoop-worker01.local
2017-12-25 20:38:46,313 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /var/lib/hadoop2/dfs/data/2/in_use.lock acquired by nodename 1982@hadoop-worker01.local
2017-12-25 20:38:46,463 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-199680249-192.168.28.129-1514169681712
2017-12-25 20:38:46,463 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712
2017-12-25 20:38:46,732 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-199680249-192.168.28.129-1514169681712
2017-12-25 20:38:46,732 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712
2017-12-25 20:38:46,746 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1794692519;bpid=BP-199680249-192.168.28.129-1514169681712;lv=-57;nsInfo=lv=-63;cid=CID-a84222ca-b260-441d-81c3-93c4f4f5d131;nsid=1794692519;c=1514169681712;bpid=BP-199680249-192.168.28.129-1514169681712;dnuuid=46e3cb45-0c76-4bc1-9342-af90beb9d892
2017-12-25 20:38:46,929 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-db2b9f5a-2dbc-404d-8b87-9ff8e79ccdd2
2017-12-25 20:38:46,929 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /var/lib/hadoop2/dfs/data/1/current, StorageType: DISK
2017-12-25 20:38:46,933 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-99e39a6f-a3c8-4fc0-9709-0aafdcfc87d6
2017-12-25 20:38:46,934 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /var/lib/hadoop2/dfs/data/2/current, StorageType: DISK
2017-12-25 20:38:46,958 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2017-12-25 20:38:46,982 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Volume reference is released.
2017-12-25 20:38:46,982 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-199680249-192.168.28.129-1514169681712
2017-12-25 20:38:46,984 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current...
2017-12-25 20:38:47,022 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current...
2017-12-25 20:38:47,110 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-199680249-192.168.28.129-1514169681712 on /var/lib/hadoop2/dfs/data/2/current: 85ms
2017-12-25 20:38:47,111 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-199680249-192.168.28.129-1514169681712 on /var/lib/hadoop2/dfs/data/1/current: 92ms
2017-12-25 20:38:47,112 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-199680249-192.168.28.129-1514169681712: 130ms
2017-12-25 20:38:47,115 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current...
2017-12-25 20:38:47,116 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/replicas doesn't exist 
2017-12-25 20:38:47,116 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current...
2017-12-25 20:38:47,119 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current: 3ms
2017-12-25 20:38:47,116 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/replicas doesn't exist 
2017-12-25 20:38:47,119 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current: 3ms
2017-12-25 20:38:47,120 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 5ms
2017-12-25 20:38:47,206 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/var/lib/hadoop2/dfs/data/2, DS-99e39a6f-a3c8-4fc0-9709-0aafdcfc87d6): no suitable block pools found to scan.  Waiting 1782199522 ms.
2017-12-25 20:38:47,207 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/var/lib/hadoop2/dfs/data/1, DS-db2b9f5a-2dbc-404d-8b87-9ff8e79ccdd2): no suitable block pools found to scan.  Waiting 1782199521 ms.
2017-12-25 20:38:47,220 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 12/26/17 1:53 AM with interval of 21600000ms
2017-12-25 20:38:47,232 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-199680249-192.168.28.129-1514169681712 (Datanode Uuid 46e3cb45-0c76-4bc1-9342-af90beb9d892) service to hadoop-master/192.168.28.129:8020 beginning handshake with NN
2017-12-25 20:38:47,274 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-199680249-192.168.28.129-1514169681712 (Datanode Uuid 46e3cb45-0c76-4bc1-9342-af90beb9d892) service to hadoop-master/192.168.28.129:8020 successfully registered with NN
2017-12-25 20:38:47,274 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode hadoop-master/192.168.28.129:8020 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2017-12-25 20:38:47,370 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xa7310a1dd393786b,  containing 2 storage report(s), of which we sent 2. The reports had 0 total blocks and used 1 RPC(s). This took 5 msec to generate and 37 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2017-12-25 20:38:47,370 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-199680249-192.168.28.129-1514169681712
2017-12-25 20:40:03,856 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741828_1004 src: /192.168.28.132:47428 dest: /192.168.28.131:50010
2017-12-25 20:40:11,476 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.132:47428, dest: /192.168.28.131:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1316553783_1, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741828_1004, duration: 7494185981
2017-12-25 20:40:11,477 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741828_1004, type=LAST_IN_PIPELINE terminating
2017-12-25 20:40:11,504 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741829_1005 src: /192.168.28.130:37712 dest: /192.168.28.131:50010
2017-12-25 20:40:13,874 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.130:37712, dest: /192.168.28.131:50010, bytes: 74090689, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1316553783_1, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741829_1005, duration: 2332253481
2017-12-25 20:40:13,875 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741829_1005, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.28.132:50010] terminating
2017-12-25 20:40:14,142 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741830_1006 src: /192.168.28.132:47460 dest: /192.168.28.131:50010
2017-12-25 20:40:14,156 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.132:47460, dest: /192.168.28.131:50010, bytes: 65985, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1316553783_1, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741830_1006, duration: 8825610
2017-12-25 20:40:14,157 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741830_1006, type=LAST_IN_PIPELINE terminating
2017-12-25 21:08:42,302 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "hadoop-worker01.local/192.168.28.131"; destination host is: "hadoop-master":8020; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1487)
	at org.apache.hadoop.ipc.Client.call(Client.java:1429)
	at org.apache.hadoop.ipc.Client.call(Client.java:1339)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:154)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:459)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:581)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:775)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1788)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1157)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1053)
2017-12-25 21:08:46,276 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop-master/192.168.28.129:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2017-12-25 21:08:47,087 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2017-12-25 21:08:47,097 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at hadoop-worker01.local/192.168.28.131
************************************************************/
2017-12-26 22:10:52,875 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   user = hadoop
STARTUP_MSG:   host = hadoop-worker01.local/192.168.28.131
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.2
STARTUP_MSG:   classpath = /opt/hadoop-2.8.2/etc/hadoop:/opt/hadoop/share/hadoop/common/lib/activation-1.1.jar:/opt/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/opt/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/opt/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/opt/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/opt/hadoop/share/hadoop/common/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/opt/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/opt/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/opt/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/opt/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/opt/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-annotations-2.8.2.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-auth-2.8.2.jar:/opt/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/opt/hadoop/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/opt/hadoop/share/hadoop/common/lib/httpclient-4.5.2.jar:/opt/hadoop/share/hadoop/common/lib/httpcore-4.4.4.jar:/opt/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/common/lib/jcip-annotations-1.0.jar:/opt/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/opt/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/opt/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jsch-0.1.54.jar:/opt/hadoop/share/hadoop/common/lib/json-smart-1.1.1.jar:/opt/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/opt/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/common/lib/junit-4.11.jar:/opt/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/opt/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-3.9.jar:/opt/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/opt/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/opt/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/opt/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/opt/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/opt/hadoop/share/hadoop/common/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/opt/hadoop/share/hadoop/common/hadoop-common-2.8.2-tests.jar:/opt/hadoop/share/hadoop/common/hadoop-common-2.8.2.jar:/opt/hadoop/share/hadoop/common/hadoop-nfs-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs:/opt/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/okio-1.4.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/opt/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/opt/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-math-2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/opt/hadoop/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/opt/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/opt/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/opt/hadoop/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/opt/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/opt/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/opt/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/opt/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/opt/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/opt/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/opt/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/opt/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.2-tests.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.2.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 66c47f2a01ad9637879e95f80c41f798373828fb; compiled by 'jdu' on 2017-10-19T20:39Z
STARTUP_MSG:   java = 1.8.0_152
************************************************************/
2017-12-26 22:10:52,892 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2017-12-26 22:10:54,609 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2017-12-26 22:10:54,892 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2017-12-26 22:10:54,892 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2017-12-26 22:10:54,911 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2017-12-26 22:10:54,914 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is hadoop-worker01.local
2017-12-26 22:10:54,920 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2017-12-26 22:10:54,977 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2017-12-26 22:10:54,981 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 10485760 bytes/s
2017-12-26 22:10:54,981 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 50
2017-12-26 22:10:55,372 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2017-12-26 22:10:55,398 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2017-12-26 22:10:55,423 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2017-12-26 22:10:55,436 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-12-26 22:10:55,439 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2017-12-26 22:10:55,439 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-12-26 22:10:55,440 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2017-12-26 22:10:55,484 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 37958
2017-12-26 22:10:55,485 INFO org.mortbay.log: jetty-6.1.26
2017-12-26 22:10:56,122 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:37958
2017-12-26 22:10:56,901 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2017-12-26 22:10:56,909 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2017-12-26 22:10:57,681 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hadoop
2017-12-26 22:10:57,681 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2017-12-26 22:10:57,887 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2017-12-26 22:10:57,921 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2017-12-26 22:10:58,109 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2017-12-26 22:10:58,132 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2017-12-26 22:10:58,170 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2017-12-26 22:10:58,216 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to hadoop-master/192.168.28.129:8020 starting to offer service
2017-12-26 22:10:58,260 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2017-12-26 22:10:58,308 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2017-12-26 22:10:59,218 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to hadoop-master/192.168.28.129:8020
2017-12-26 22:10:59,248 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2017-12-26 22:10:59,273 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /var/lib/hadoop2/dfs/data/1/in_use.lock acquired by nodename 2302@hadoop-worker01.local
2017-12-26 22:10:59,286 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /var/lib/hadoop2/dfs/data/2/in_use.lock acquired by nodename 2302@hadoop-worker01.local
2017-12-26 22:10:59,502 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-199680249-192.168.28.129-1514169681712
2017-12-26 22:10:59,502 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712
2017-12-26 22:10:59,849 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-199680249-192.168.28.129-1514169681712
2017-12-26 22:10:59,866 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712
2017-12-26 22:10:59,910 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1794692519;bpid=BP-199680249-192.168.28.129-1514169681712;lv=-57;nsInfo=lv=-63;cid=CID-a84222ca-b260-441d-81c3-93c4f4f5d131;nsid=1794692519;c=1514169681712;bpid=BP-199680249-192.168.28.129-1514169681712;dnuuid=46e3cb45-0c76-4bc1-9342-af90beb9d892
2017-12-26 22:11:00,115 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-db2b9f5a-2dbc-404d-8b87-9ff8e79ccdd2
2017-12-26 22:11:00,115 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /var/lib/hadoop2/dfs/data/1/current, StorageType: DISK
2017-12-26 22:11:00,119 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-99e39a6f-a3c8-4fc0-9709-0aafdcfc87d6
2017-12-26 22:11:00,119 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /var/lib/hadoop2/dfs/data/2/current, StorageType: DISK
2017-12-26 22:11:00,129 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2017-12-26 22:11:00,158 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Volume reference is released.
2017-12-26 22:11:00,158 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-199680249-192.168.28.129-1514169681712
2017-12-26 22:11:00,163 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current...
2017-12-26 22:11:00,167 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current...
2017-12-26 22:11:00,258 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-199680249-192.168.28.129-1514169681712 on /var/lib/hadoop2/dfs/data/2/current: 91ms
2017-12-26 22:11:00,258 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-199680249-192.168.28.129-1514169681712 on /var/lib/hadoop2/dfs/data/1/current: 92ms
2017-12-26 22:11:00,260 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-199680249-192.168.28.129-1514169681712: 102ms
2017-12-26 22:11:00,265 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current...
2017-12-26 22:11:00,266 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/replicas doesn't exist 
2017-12-26 22:11:00,270 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current...
2017-12-26 22:11:00,270 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/replicas doesn't exist 
2017-12-26 22:11:00,275 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current: 4ms
2017-12-26 22:11:00,275 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current: 10ms
2017-12-26 22:11:00,276 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 12ms
2017-12-26 22:11:00,422 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/var/lib/hadoop2/dfs/data/2, DS-99e39a6f-a3c8-4fc0-9709-0aafdcfc87d6): no suitable block pools found to scan.  Waiting 1690266307 ms.
2017-12-26 22:11:00,439 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/var/lib/hadoop2/dfs/data/1, DS-db2b9f5a-2dbc-404d-8b87-9ff8e79ccdd2): no suitable block pools found to scan.  Waiting 1690266289 ms.
2017-12-26 22:11:00,463 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 12/27/17 1:39 AM with interval of 21600000ms
2017-12-26 22:11:00,479 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-199680249-192.168.28.129-1514169681712 (Datanode Uuid 46e3cb45-0c76-4bc1-9342-af90beb9d892) service to hadoop-master/192.168.28.129:8020 beginning handshake with NN
2017-12-26 22:11:00,529 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-199680249-192.168.28.129-1514169681712 (Datanode Uuid 46e3cb45-0c76-4bc1-9342-af90beb9d892) service to hadoop-master/192.168.28.129:8020 successfully registered with NN
2017-12-26 22:11:00,529 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode hadoop-master/192.168.28.129:8020 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2017-12-26 22:11:00,656 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x8a915c9f9a1bb587,  containing 2 storage report(s), of which we sent 2. The reports had 3 total blocks and used 1 RPC(s). This took 8 msec to generate and 46 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2017-12-26 22:11:00,656 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-199680249-192.168.28.129-1514169681712
2017-12-26 22:17:53,819 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741831_1007 src: /192.168.28.131:59384 dest: /192.168.28.131:50010
2017-12-26 22:18:24,057 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.131:59384, dest: /192.168.28.131:50010, bytes: 239, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-66565203_29, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741831_1007, duration: 26662462973
2017-12-26 22:18:24,058 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741831_1007, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.28.132:50010] terminating
2017-12-26 22:18:44,240 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741832_1008 src: /192.168.28.131:59440 dest: /192.168.28.131:50010
2017-12-26 22:18:44,278 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.131:59440, dest: /192.168.28.131:50010, bytes: 1330, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-66565203_29, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741832_1008, duration: 20636820
2017-12-26 22:18:44,278 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741832_1008, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.28.132:50010] terminating
2017-12-26 22:18:44,939 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741833_1009 src: /192.168.28.131:59504 dest: /192.168.28.131:50010
2017-12-26 22:18:44,981 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.131:59504, dest: /192.168.28.131:50010, bytes: 1337, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-66565203_29, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741833_1009, duration: 33561776
2017-12-26 22:18:44,982 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741833_1009, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.28.132:50010] terminating
2017-12-26 22:18:45,175 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741834_1010 src: /192.168.28.131:59524 dest: /192.168.28.131:50010
2017-12-26 22:18:45,257 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.131:59524, dest: /192.168.28.131:50010, bytes: 1338, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-66565203_29, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741834_1010, duration: 41851713
2017-12-26 22:18:45,257 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741834_1010, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.28.132:50010] terminating
2017-12-26 22:18:45,512 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741835_1011 src: /192.168.28.131:59546 dest: /192.168.28.131:50010
2017-12-26 22:18:45,566 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.131:59546, dest: /192.168.28.131:50010, bytes: 1338, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-66565203_29, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741835_1011, duration: 36695964
2017-12-26 22:18:45,566 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741835_1011, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.28.132:50010] terminating
2017-12-26 22:18:45,809 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741836_1012 src: /192.168.28.131:59568 dest: /192.168.28.131:50010
2017-12-26 22:18:45,859 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.131:59568, dest: /192.168.28.131:50010, bytes: 1338, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-66565203_29, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741836_1012, duration: 35311176
2017-12-26 22:18:45,860 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741836_1012, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.28.132:50010] terminating
2017-12-26 22:18:46,079 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741837_1013 src: /192.168.28.131:59588 dest: /192.168.28.131:50010
2017-12-26 22:18:46,169 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.131:59588, dest: /192.168.28.131:50010, bytes: 1338, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-66565203_29, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741837_1013, duration: 75953493
2017-12-26 22:18:46,169 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741837_1013, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.28.132:50010] terminating
2017-12-26 22:18:49,353 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741838_1014 src: /192.168.28.131:59620 dest: /192.168.28.131:50010
2017-12-26 22:18:51,362 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.131:59620, dest: /192.168.28.131:50010, bytes: 260, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-66565203_29, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741838_1014, duration: 2004046475
2017-12-26 22:18:51,363 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741838_1014, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.28.132:50010] terminating
2017-12-26 22:18:54,465 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741839_1015 src: /192.168.28.131:59626 dest: /192.168.28.131:50010
2017-12-26 22:19:07,986 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.131:59626, dest: /192.168.28.131:50010, bytes: 260, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-66565203_29, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741839_1015, duration: 13516147641
2017-12-26 22:19:07,987 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741839_1015, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.28.132:50010] terminating
2017-12-26 22:21:24,101 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741840_1016 src: /192.168.28.131:59638 dest: /192.168.28.131:50010
2017-12-26 22:21:24,855 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.131:59638, dest: /192.168.28.131:50010, bytes: 271, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-270104887_29, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741840_1016, duration: 748459227
2017-12-26 22:21:24,855 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741840_1016, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.28.132:50010] terminating
2017-12-26 22:21:26,066 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741841_1017 src: /192.168.28.131:59806 dest: /192.168.28.131:50010
2017-12-26 22:21:26,101 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.131:59806, dest: /192.168.28.131:50010, bytes: 1362, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-270104887_29, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741841_1017, duration: 26932511
2017-12-26 22:21:26,101 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741841_1017, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.28.132:50010] terminating
2017-12-26 22:21:29,708 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741842_1018 src: /192.168.28.131:59846 dest: /192.168.28.131:50010
2017-12-26 22:21:30,258 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.131:59846, dest: /192.168.28.131:50010, bytes: 271, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-270104887_29, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741842_1018, duration: 540413896
2017-12-26 22:21:30,258 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741842_1018, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.28.132:50010] terminating
2017-12-26 22:21:33,428 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741843_1019 src: /192.168.28.131:59850 dest: /192.168.28.131:50010
2017-12-26 22:22:00,429 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.131:59850, dest: /192.168.28.131:50010, bytes: 271, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-270104887_29, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741843_1019, duration: 26988330128
2017-12-26 22:22:00,431 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741843_1019, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.28.132:50010] terminating
2017-12-26 22:44:34,653 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "hadoop-worker01.local/192.168.28.131"; destination host is: "hadoop-master":8020; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1487)
	at org.apache.hadoop.ipc.Client.call(Client.java:1429)
	at org.apache.hadoop.ipc.Client.call(Client.java:1339)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:154)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:459)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:581)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:775)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1788)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1157)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1053)
2017-12-26 22:44:37,962 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2017-12-26 22:44:37,971 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at hadoop-worker01.local/192.168.28.131
************************************************************/
2018-01-03 13:16:27,396 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   user = hadoop
STARTUP_MSG:   host = hadoop-worker01.local/192.168.28.131
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.2
STARTUP_MSG:   classpath = /opt/hadoop-2.8.2/etc/hadoop:/opt/hadoop/share/hadoop/common/lib/activation-1.1.jar:/opt/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/opt/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/opt/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/opt/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/opt/hadoop/share/hadoop/common/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/opt/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/opt/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/opt/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/opt/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/opt/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-annotations-2.8.2.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-auth-2.8.2.jar:/opt/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/opt/hadoop/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/opt/hadoop/share/hadoop/common/lib/httpclient-4.5.2.jar:/opt/hadoop/share/hadoop/common/lib/httpcore-4.4.4.jar:/opt/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/common/lib/jcip-annotations-1.0.jar:/opt/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/opt/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/opt/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jsch-0.1.54.jar:/opt/hadoop/share/hadoop/common/lib/json-smart-1.1.1.jar:/opt/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/opt/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/common/lib/junit-4.11.jar:/opt/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/opt/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-3.9.jar:/opt/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/opt/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/opt/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/opt/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/opt/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/opt/hadoop/share/hadoop/common/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/opt/hadoop/share/hadoop/common/hadoop-common-2.8.2-tests.jar:/opt/hadoop/share/hadoop/common/hadoop-common-2.8.2.jar:/opt/hadoop/share/hadoop/common/hadoop-nfs-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs:/opt/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/okio-1.4.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/opt/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/opt/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-math-2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/opt/hadoop/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/opt/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/opt/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/opt/hadoop/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/opt/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/opt/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/opt/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/opt/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/opt/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/opt/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/opt/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/opt/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.2-tests.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.2.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 66c47f2a01ad9637879e95f80c41f798373828fb; compiled by 'jdu' on 2017-10-19T20:39Z
STARTUP_MSG:   java = 1.8.0_152
************************************************************/
2018-01-03 13:16:27,419 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2018-01-03 13:16:30,717 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2018-01-03 13:16:31,144 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2018-01-03 13:16:31,144 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2018-01-03 13:16:31,163 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2018-01-03 13:16:31,186 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is hadoop-worker01.local
2018-01-03 13:16:31,194 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2018-01-03 13:16:31,292 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2018-01-03 13:16:31,297 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 10485760 bytes/s
2018-01-03 13:16:31,297 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 50
2018-01-03 13:16:31,985 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2018-01-03 13:16:32,052 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2018-01-03 13:16:32,081 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2018-01-03 13:16:32,093 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2018-01-03 13:16:32,096 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2018-01-03 13:16:32,096 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2018-01-03 13:16:32,097 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2018-01-03 13:16:32,142 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 39489
2018-01-03 13:16:32,142 INFO org.mortbay.log: jetty-6.1.26
2018-01-03 13:16:33,238 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:39489
2018-01-03 13:16:33,877 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2018-01-03 13:16:33,896 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2018-01-03 13:16:34,754 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hadoop
2018-01-03 13:16:34,755 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2018-01-03 13:16:34,977 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2018-01-03 13:16:35,016 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2018-01-03 13:16:35,473 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2018-01-03 13:16:35,545 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2018-01-03 13:16:35,619 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2018-01-03 13:16:35,646 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to hadoop-master/192.168.28.129:8020 starting to offer service
2018-01-03 13:16:35,996 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2018-01-03 13:16:36,021 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2018-01-03 13:16:36,985 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to hadoop-master/192.168.28.129:8020
2018-01-03 13:16:37,055 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2018-01-03 13:16:37,068 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /var/lib/hadoop2/dfs/data/1/in_use.lock acquired by nodename 1938@hadoop-worker01.local
2018-01-03 13:16:37,082 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /var/lib/hadoop2/dfs/data/2/in_use.lock acquired by nodename 1938@hadoop-worker01.local
2018-01-03 13:16:37,357 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-199680249-192.168.28.129-1514169681712
2018-01-03 13:16:37,358 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712
2018-01-03 13:16:38,036 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-199680249-192.168.28.129-1514169681712
2018-01-03 13:16:38,036 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712
2018-01-03 13:16:38,056 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1794692519;bpid=BP-199680249-192.168.28.129-1514169681712;lv=-57;nsInfo=lv=-63;cid=CID-a84222ca-b260-441d-81c3-93c4f4f5d131;nsid=1794692519;c=1514169681712;bpid=BP-199680249-192.168.28.129-1514169681712;dnuuid=46e3cb45-0c76-4bc1-9342-af90beb9d892
2018-01-03 13:16:38,339 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-db2b9f5a-2dbc-404d-8b87-9ff8e79ccdd2
2018-01-03 13:16:38,339 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /var/lib/hadoop2/dfs/data/1/current, StorageType: DISK
2018-01-03 13:16:38,342 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-99e39a6f-a3c8-4fc0-9709-0aafdcfc87d6
2018-01-03 13:16:38,342 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /var/lib/hadoop2/dfs/data/2/current, StorageType: DISK
2018-01-03 13:16:38,352 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2018-01-03 13:16:38,385 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Volume reference is released.
2018-01-03 13:16:38,385 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-199680249-192.168.28.129-1514169681712
2018-01-03 13:16:38,394 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current...
2018-01-03 13:16:38,396 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current...
2018-01-03 13:16:38,641 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-199680249-192.168.28.129-1514169681712 on /var/lib/hadoop2/dfs/data/2/current: 245ms
2018-01-03 13:16:38,643 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-199680249-192.168.28.129-1514169681712 on /var/lib/hadoop2/dfs/data/1/current: 245ms
2018-01-03 13:16:38,645 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-199680249-192.168.28.129-1514169681712: 260ms
2018-01-03 13:16:38,649 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current...
2018-01-03 13:16:38,649 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current...
2018-01-03 13:16:38,649 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/replicas doesn't exist 
2018-01-03 13:16:38,649 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/replicas doesn't exist 
2018-01-03 13:16:38,657 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current: 8ms
2018-01-03 13:16:38,669 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current: 19ms
2018-01-03 13:16:38,669 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 20ms
2018-01-03 13:16:38,789 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/var/lib/hadoop2/dfs/data/2, DS-99e39a6f-a3c8-4fc0-9709-0aafdcfc87d6): no suitable block pools found to scan.  Waiting 1031127939 ms.
2018-01-03 13:16:38,799 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/var/lib/hadoop2/dfs/data/1, DS-db2b9f5a-2dbc-404d-8b87-9ff8e79ccdd2): no suitable block pools found to scan.  Waiting 1031127929 ms.
2018-01-03 13:16:38,838 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1/3/18 6:01 PM with interval of 21600000ms
2018-01-03 13:16:38,921 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-199680249-192.168.28.129-1514169681712 (Datanode Uuid 46e3cb45-0c76-4bc1-9342-af90beb9d892) service to hadoop-master/192.168.28.129:8020 beginning handshake with NN
2018-01-03 13:16:38,994 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-199680249-192.168.28.129-1514169681712 (Datanode Uuid 46e3cb45-0c76-4bc1-9342-af90beb9d892) service to hadoop-master/192.168.28.129:8020 successfully registered with NN
2018-01-03 13:16:38,994 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode hadoop-master/192.168.28.129:8020 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2018-01-03 13:16:39,392 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xde3032413ef55e96,  containing 2 storage report(s), of which we sent 2. The reports had 16 total blocks and used 1 RPC(s). This took 30 msec to generate and 209 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-01-03 13:16:39,392 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-199680249-192.168.28.129-1514169681712
2018-01-03 13:41:10,472 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741844_1020 src: /192.168.28.132:58036 dest: /192.168.28.131:50010
2018-01-03 13:41:10,566 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.132:58036, dest: /192.168.28.131:50010, bytes: 30, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1833690103_1, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741844_1020, duration: 62278051
2018-01-03 13:41:10,567 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741844_1020, type=LAST_IN_PIPELINE terminating
2018-01-03 13:52:33,020 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741845_1021 src: /192.168.28.132:58318 dest: /192.168.28.131:50010
2018-01-03 13:52:36,981 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.132:58318, dest: /192.168.28.131:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1069073985_12, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741845_1021, duration: 3956582915
2018-01-03 13:52:36,982 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741845_1021, type=LAST_IN_PIPELINE terminating
2018-01-03 13:52:37,050 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741846_1022 src: /192.168.28.132:58322 dest: /192.168.28.131:50010
2018-01-03 13:52:38,968 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.132:58322, dest: /192.168.28.131:50010, bytes: 74090694, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1069073985_12, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741846_1022, duration: 1914425273
2018-01-03 13:52:38,969 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741846_1022, type=LAST_IN_PIPELINE terminating
2018-01-03 13:52:39,207 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741847_1023 src: /192.168.28.132:58326 dest: /192.168.28.131:50010
2018-01-03 13:52:39,232 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.132:58326, dest: /192.168.28.131:50010, bytes: 65912, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1069073985_12, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741847_1023, duration: 10838612
2018-01-03 13:52:39,232 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741847_1023, type=LAST_IN_PIPELINE terminating
2018-01-03 13:54:04,224 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741845_1021 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741845 for deletion
2018-01-03 13:54:04,227 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741846_1022 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741846 for deletion
2018-01-03 13:54:04,228 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741847_1023 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741847 for deletion
2018-01-03 13:54:04,230 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741846_1022 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741846
2018-01-03 13:54:04,231 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741845_1021 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741845
2018-01-03 13:54:04,232 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741847_1023 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741847
2018-01-03 13:55:03,119 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741848_1024 src: /192.168.28.132:58516 dest: /192.168.28.131:50010
2018-01-03 13:55:06,062 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.132:58516, dest: /192.168.28.131:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1245991119_12, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741848_1024, duration: 2941613986
2018-01-03 13:55:06,064 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741848_1024, type=LAST_IN_PIPELINE terminating
2018-01-03 13:55:06,139 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741849_1025 src: /192.168.28.132:58520 dest: /192.168.28.131:50010
2018-01-03 13:55:07,941 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.132:58520, dest: /192.168.28.131:50010, bytes: 74090694, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1245991119_12, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741849_1025, duration: 1800191066
2018-01-03 13:55:07,941 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741849_1025, type=LAST_IN_PIPELINE terminating
2018-01-03 13:55:08,176 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741850_1026 src: /192.168.28.132:58524 dest: /192.168.28.131:50010
2018-01-03 13:55:08,187 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.132:58524, dest: /192.168.28.131:50010, bytes: 65912, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1245991119_12, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741850_1026, duration: 7907115
2018-01-03 13:55:08,189 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741850_1026, type=LAST_IN_PIPELINE terminating
2018-01-03 14:00:16,740 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "hadoop-worker01.local/192.168.28.131"; destination host is: "hadoop-master":8020; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1487)
	at org.apache.hadoop.ipc.Client.call(Client.java:1429)
	at org.apache.hadoop.ipc.Client.call(Client.java:1339)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:154)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:459)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:581)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:775)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1788)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1157)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1053)
2018-01-03 14:00:19,556 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2018-01-03 14:00:19,565 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at hadoop-worker01.local/192.168.28.131
************************************************************/
2018-01-03 14:03:03,053 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   user = hadoop
STARTUP_MSG:   host = hadoop-worker01.local/192.168.28.131
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.2
STARTUP_MSG:   classpath = /opt/hadoop-2.8.2/etc/hadoop:/opt/hadoop/share/hadoop/common/lib/activation-1.1.jar:/opt/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/opt/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/opt/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/opt/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/opt/hadoop/share/hadoop/common/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/opt/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/opt/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/opt/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/opt/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/opt/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-annotations-2.8.2.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-auth-2.8.2.jar:/opt/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/opt/hadoop/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/opt/hadoop/share/hadoop/common/lib/httpclient-4.5.2.jar:/opt/hadoop/share/hadoop/common/lib/httpcore-4.4.4.jar:/opt/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/common/lib/jcip-annotations-1.0.jar:/opt/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/opt/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/opt/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jsch-0.1.54.jar:/opt/hadoop/share/hadoop/common/lib/json-smart-1.1.1.jar:/opt/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/opt/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/common/lib/junit-4.11.jar:/opt/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/opt/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-3.9.jar:/opt/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/opt/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/opt/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/opt/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/opt/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/opt/hadoop/share/hadoop/common/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/opt/hadoop/share/hadoop/common/hadoop-common-2.8.2-tests.jar:/opt/hadoop/share/hadoop/common/hadoop-common-2.8.2.jar:/opt/hadoop/share/hadoop/common/hadoop-nfs-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs:/opt/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/okio-1.4.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/opt/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/opt/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-math-2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/opt/hadoop/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/opt/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/opt/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/opt/hadoop/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/opt/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/opt/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/opt/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/opt/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/opt/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/opt/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/opt/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/opt/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.2-tests.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.2.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 66c47f2a01ad9637879e95f80c41f798373828fb; compiled by 'jdu' on 2017-10-19T20:39Z
STARTUP_MSG:   java = 1.8.0_152
************************************************************/
2018-01-03 14:03:03,081 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2018-01-03 14:03:05,357 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2018-01-03 14:03:05,558 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2018-01-03 14:03:05,558 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2018-01-03 14:03:05,576 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2018-01-03 14:03:05,581 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is hadoop-worker01.local
2018-01-03 14:03:05,591 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2018-01-03 14:03:05,689 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2018-01-03 14:03:05,702 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 10485760 bytes/s
2018-01-03 14:03:05,702 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 50
2018-01-03 14:03:06,570 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2018-01-03 14:03:06,737 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2018-01-03 14:03:06,776 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2018-01-03 14:03:06,792 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2018-01-03 14:03:06,806 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2018-01-03 14:03:06,806 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2018-01-03 14:03:06,806 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2018-01-03 14:03:06,906 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 37552
2018-01-03 14:03:06,907 INFO org.mortbay.log: jetty-6.1.26
2018-01-03 14:03:08,220 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:37552
2018-01-03 14:03:09,609 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2018-01-03 14:03:09,615 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2018-01-03 14:03:10,645 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hadoop
2018-01-03 14:03:10,646 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2018-01-03 14:03:10,952 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2018-01-03 14:03:11,002 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2018-01-03 14:03:11,232 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2018-01-03 14:03:11,254 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2018-01-03 14:03:11,378 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2018-01-03 14:03:11,439 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to hadoop-master/192.168.28.129:8020 starting to offer service
2018-01-03 14:03:11,491 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2018-01-03 14:03:11,501 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2018-01-03 14:03:12,554 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to hadoop-master/192.168.28.129:8020
2018-01-03 14:03:12,564 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2018-01-03 14:03:12,593 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /var/lib/hadoop2/dfs/data/1/in_use.lock acquired by nodename 3059@hadoop-worker01.local
2018-01-03 14:03:12,690 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /var/lib/hadoop2/dfs/data/2/in_use.lock acquired by nodename 3059@hadoop-worker01.local
2018-01-03 14:03:13,199 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-199680249-192.168.28.129-1514169681712
2018-01-03 14:03:13,199 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712
2018-01-03 14:03:13,679 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-199680249-192.168.28.129-1514169681712
2018-01-03 14:03:13,679 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712
2018-01-03 14:03:13,863 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1794692519;bpid=BP-199680249-192.168.28.129-1514169681712;lv=-57;nsInfo=lv=-63;cid=CID-a84222ca-b260-441d-81c3-93c4f4f5d131;nsid=1794692519;c=1514169681712;bpid=BP-199680249-192.168.28.129-1514169681712;dnuuid=46e3cb45-0c76-4bc1-9342-af90beb9d892
2018-01-03 14:03:14,204 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-db2b9f5a-2dbc-404d-8b87-9ff8e79ccdd2
2018-01-03 14:03:14,204 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /var/lib/hadoop2/dfs/data/1/current, StorageType: DISK
2018-01-03 14:03:14,212 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-99e39a6f-a3c8-4fc0-9709-0aafdcfc87d6
2018-01-03 14:03:14,212 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /var/lib/hadoop2/dfs/data/2/current, StorageType: DISK
2018-01-03 14:03:14,383 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2018-01-03 14:03:14,398 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Volume reference is released.
2018-01-03 14:03:14,398 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-199680249-192.168.28.129-1514169681712
2018-01-03 14:03:14,413 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current...
2018-01-03 14:03:14,516 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current...
2018-01-03 14:03:14,653 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current: 270770176
2018-01-03 14:03:14,655 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current: 149413888
2018-01-03 14:03:14,672 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-199680249-192.168.28.129-1514169681712 on /var/lib/hadoop2/dfs/data/2/current: 156ms
2018-01-03 14:03:14,777 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-199680249-192.168.28.129-1514169681712 on /var/lib/hadoop2/dfs/data/1/current: 327ms
2018-01-03 14:03:14,778 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-199680249-192.168.28.129-1514169681712: 380ms
2018-01-03 14:03:14,789 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current...
2018-01-03 14:03:14,790 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/replicas doesn't exist 
2018-01-03 14:03:14,805 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current...
2018-01-03 14:03:14,808 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current: 18ms
2018-01-03 14:03:14,806 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/replicas doesn't exist 
2018-01-03 14:03:14,813 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current: 7ms
2018-01-03 14:03:14,813 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 31ms
2018-01-03 14:03:15,116 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/var/lib/hadoop2/dfs/data/2, DS-99e39a6f-a3c8-4fc0-9709-0aafdcfc87d6): no suitable block pools found to scan.  Waiting 1028331612 ms.
2018-01-03 14:03:15,121 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/var/lib/hadoop2/dfs/data/1, DS-db2b9f5a-2dbc-404d-8b87-9ff8e79ccdd2): no suitable block pools found to scan.  Waiting 1028331607 ms.
2018-01-03 14:03:15,192 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1/3/18 4:10 PM with interval of 21600000ms
2018-01-03 14:03:15,231 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-199680249-192.168.28.129-1514169681712 (Datanode Uuid 46e3cb45-0c76-4bc1-9342-af90beb9d892) service to hadoop-master/192.168.28.129:8020 beginning handshake with NN
2018-01-03 14:03:15,386 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-199680249-192.168.28.129-1514169681712 (Datanode Uuid 46e3cb45-0c76-4bc1-9342-af90beb9d892) service to hadoop-master/192.168.28.129:8020 successfully registered with NN
2018-01-03 14:03:15,386 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode hadoop-master/192.168.28.129:8020 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2018-01-03 14:03:15,815 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xd03205243561c729,  containing 2 storage report(s), of which we sent 2. The reports had 20 total blocks and used 1 RPC(s). This took 7 msec to generate and 260 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-01-03 14:03:15,816 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-199680249-192.168.28.129-1514169681712
2018-01-03 14:04:27,746 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741851_1027 src: /192.168.28.132:58800 dest: /192.168.28.131:50010
2018-01-03 14:04:31,989 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.132:58800, dest: /192.168.28.131:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1767299747_12, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741851_1027, duration: 4128185925
2018-01-03 14:04:31,990 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741851_1027, type=LAST_IN_PIPELINE terminating
2018-01-03 14:04:32,021 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741852_1028 src: /192.168.28.132:58804 dest: /192.168.28.131:50010
2018-01-03 14:04:33,660 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.132:58804, dest: /192.168.28.131:50010, bytes: 74090694, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1767299747_12, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741852_1028, duration: 1636379782
2018-01-03 14:04:33,661 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741852_1028, type=LAST_IN_PIPELINE terminating
2018-01-03 14:04:34,019 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741853_1029 src: /192.168.28.132:58808 dest: /192.168.28.131:50010
2018-01-03 14:04:34,039 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.132:58808, dest: /192.168.28.131:50010, bytes: 66101, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1767299747_12, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741853_1029, duration: 6377569
2018-01-03 14:04:34,042 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741853_1029, type=LAST_IN_PIPELINE terminating
2018-01-03 14:08:07,215 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741851_1027 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741851 for deletion
2018-01-03 14:08:07,221 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741852_1028 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741852 for deletion
2018-01-03 14:08:07,222 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741853_1029 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741853 for deletion
2018-01-03 14:08:07,223 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741851_1027 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741851
2018-01-03 14:08:07,224 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741852_1028 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741852
2018-01-03 14:08:07,225 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741853_1029 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741853
2018-01-03 14:08:25,860 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741854_1030 src: /192.168.28.132:58990 dest: /192.168.28.131:50010
2018-01-03 14:08:28,745 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.132:58990, dest: /192.168.28.131:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1449670756_12, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741854_1030, duration: 2883003916
2018-01-03 14:08:28,746 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741854_1030, type=LAST_IN_PIPELINE terminating
2018-01-03 14:08:28,786 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741855_1031 src: /192.168.28.132:58994 dest: /192.168.28.131:50010
2018-01-03 14:08:30,308 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.132:58994, dest: /192.168.28.131:50010, bytes: 74090694, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1449670756_12, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741855_1031, duration: 1498466574
2018-01-03 14:08:30,308 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741855_1031, type=LAST_IN_PIPELINE terminating
2018-01-03 14:08:30,627 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741856_1032 src: /192.168.28.132:58998 dest: /192.168.28.131:50010
2018-01-03 14:08:30,635 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.132:58998, dest: /192.168.28.131:50010, bytes: 66101, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1449670756_12, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741856_1032, duration: 4201796
2018-01-03 14:08:30,636 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741856_1032, type=LAST_IN_PIPELINE terminating
2018-01-03 14:09:46,996 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741856_1032 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741856 for deletion
2018-01-03 14:09:47,004 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741856_1032 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741856
2018-01-03 14:09:47,012 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741854_1030 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741854 for deletion
2018-01-03 14:09:47,016 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741855_1031 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741855 for deletion
2018-01-03 14:09:47,053 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741855_1031 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741855
2018-01-03 14:09:47,063 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741854_1030 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741854
2018-01-03 14:10:12,071 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741857_1033 src: /192.168.28.132:59196 dest: /192.168.28.131:50010
2018-01-03 14:10:14,902 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.132:59196, dest: /192.168.28.131:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-703799157_12, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741857_1033, duration: 2828018448
2018-01-03 14:10:14,903 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741857_1033, type=LAST_IN_PIPELINE terminating
2018-01-03 14:10:15,015 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741858_1034 src: /192.168.28.132:59200 dest: /192.168.28.131:50010
2018-01-03 14:10:16,484 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.132:59200, dest: /192.168.28.131:50010, bytes: 74090694, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-703799157_12, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741858_1034, duration: 1443354761
2018-01-03 14:10:16,484 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741858_1034, type=LAST_IN_PIPELINE terminating
2018-01-03 14:10:16,808 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741859_1035 src: /192.168.28.132:59204 dest: /192.168.28.131:50010
2018-01-03 14:10:16,818 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.132:59204, dest: /192.168.28.131:50010, bytes: 66101, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-703799157_12, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741859_1035, duration: 6097882
2018-01-03 14:10:16,819 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741859_1035, type=LAST_IN_PIPELINE terminating
2018-01-03 14:13:00,886 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xd03205243561c72a,  containing 2 storage report(s), of which we sent 2. The reports had 23 total blocks and used 1 RPC(s). This took 0 msec to generate and 6 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-01-03 14:13:00,887 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-199680249-192.168.28.129-1514169681712
2018-01-03 14:13:03,888 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741828_1004 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741828 for deletion
2018-01-03 14:13:03,890 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741829_1005 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741829 for deletion
2018-01-03 14:13:03,891 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741828_1004 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741828
2018-01-03 14:13:03,892 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741830_1006 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741830 for deletion
2018-01-03 14:13:03,894 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741830_1006 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741830
2018-01-03 14:13:03,897 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741829_1005 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741829
2018-01-03 14:19:47,880 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741857_1033 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741857 for deletion
2018-01-03 14:19:47,881 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741858_1034 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741858 for deletion
2018-01-03 14:19:47,883 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741859_1035 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741859 for deletion
2018-01-03 14:19:47,887 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741857_1033 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741857
2018-01-03 14:19:47,887 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741858_1034 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741858
2018-01-03 14:19:47,887 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741859_1035 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741859
2018-01-03 14:20:13,471 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741860_1036 src: /192.168.28.132:59484 dest: /192.168.28.131:50010
2018-01-03 14:20:16,519 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.132:59484, dest: /192.168.28.131:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_777984038_12, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741860_1036, duration: 3046956446
2018-01-03 14:20:16,520 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741860_1036, type=LAST_IN_PIPELINE terminating
2018-01-03 14:20:16,571 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741861_1037 src: /192.168.28.132:59488 dest: /192.168.28.131:50010
2018-01-03 14:20:17,824 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.132:59488, dest: /192.168.28.131:50010, bytes: 74090694, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_777984038_12, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741861_1037, duration: 1245621634
2018-01-03 14:20:17,824 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741861_1037, type=LAST_IN_PIPELINE terminating
2018-01-03 14:20:18,078 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741862_1038 src: /192.168.28.132:59492 dest: /192.168.28.131:50010
2018-01-03 14:20:18,087 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.132:59492, dest: /192.168.28.131:50010, bytes: 66101, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_777984038_12, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741862_1038, duration: 7061258
2018-01-03 14:20:18,087 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741862_1038, type=LAST_IN_PIPELINE terminating
2018-01-03 14:26:51,823 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741860_1036 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741860 for deletion
2018-01-03 14:26:51,826 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741861_1037 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741861 for deletion
2018-01-03 14:26:51,826 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741862_1038 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741862 for deletion
2018-01-03 14:26:51,851 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741861_1037 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741861
2018-01-03 14:26:51,876 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741860_1036 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741860
2018-01-03 14:26:51,876 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741862_1038 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741862
2018-01-03 14:29:18,115 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741863_1039 src: /192.168.28.132:59538 dest: /192.168.28.131:50010
2018-01-03 14:29:20,773 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.132:59538, dest: /192.168.28.131:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1274515486_12, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741863_1039, duration: 2655839252
2018-01-03 14:29:20,774 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741863_1039, type=LAST_IN_PIPELINE terminating
2018-01-03 14:29:20,891 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741864_1040 src: /192.168.28.132:59542 dest: /192.168.28.131:50010
2018-01-03 14:29:22,276 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.132:59542, dest: /192.168.28.131:50010, bytes: 74090694, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1274515486_12, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741864_1040, duration: 1382342381
2018-01-03 14:29:22,276 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741864_1040, type=LAST_IN_PIPELINE terminating
2018-01-03 14:29:22,563 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741865_1041 src: /192.168.28.132:59546 dest: /192.168.28.131:50010
2018-01-03 14:29:22,578 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.132:59546, dest: /192.168.28.131:50010, bytes: 66101, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1274515486_12, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741865_1041, duration: 8779676
2018-01-03 14:29:22,580 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741865_1041, type=LAST_IN_PIPELINE terminating
2018-01-03 14:35:31,089 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741863_1039 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741863 for deletion
2018-01-03 14:35:31,092 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741864_1040 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741864 for deletion
2018-01-03 14:35:31,104 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741865_1041 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741865 for deletion
2018-01-03 14:35:31,135 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741864_1040 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741864
2018-01-03 14:35:31,150 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741863_1039 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741863
2018-01-03 14:35:31,150 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741865_1041 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741865
2018-01-03 15:18:30,841 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741866_1042 src: /192.168.28.132:59554 dest: /192.168.28.131:50010
2018-01-03 15:18:36,261 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.132:59554, dest: /192.168.28.131:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1373847299_12, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741866_1042, duration: 5414836070
2018-01-03 15:18:36,263 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741866_1042, type=LAST_IN_PIPELINE terminating
2018-01-03 15:18:36,310 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741867_1043 src: /192.168.28.132:59556 dest: /192.168.28.131:50010
2018-01-03 15:18:38,663 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.132:59556, dest: /192.168.28.131:50010, bytes: 74090694, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1373847299_12, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741867_1043, duration: 2341880601
2018-01-03 15:18:38,663 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741867_1043, type=LAST_IN_PIPELINE terminating
2018-01-03 15:18:39,036 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741868_1044 src: /192.168.28.130:60908 dest: /192.168.28.131:50010
2018-01-03 15:18:39,253 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.130:60908, dest: /192.168.28.131:50010, bytes: 66101, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1373847299_12, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741868_1044, duration: 88043822
2018-01-03 15:18:39,253 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741868_1044, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.28.132:50010] terminating
2018-01-03 15:24:48,539 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741866_1042 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741866 for deletion
2018-01-03 15:24:48,543 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741867_1043 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741867 for deletion
2018-01-03 15:24:48,545 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741868_1044 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741868 for deletion
2018-01-03 15:24:48,613 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741867_1043 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741867
2018-01-03 15:24:48,628 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741866_1042 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741866
2018-01-03 15:24:48,629 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741868_1044 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741868
2018-01-03 15:27:32,817 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741869_1045 src: /192.168.28.130:32894 dest: /192.168.28.131:50010
2018-01-03 15:27:37,472 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.130:32894, dest: /192.168.28.131:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-85016196_12, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741869_1045, duration: 4646044609
2018-01-03 15:27:37,472 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741869_1045, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.28.132:50010] terminating
2018-01-03 15:27:37,491 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741870_1046 src: /192.168.28.130:32896 dest: /192.168.28.131:50010
2018-01-03 15:27:39,852 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.130:32896, dest: /192.168.28.131:50010, bytes: 74090694, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-85016196_12, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741870_1046, duration: 2353522477
2018-01-03 15:27:39,853 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741870_1046, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.28.132:50010] terminating
2018-01-03 15:27:40,549 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741871_1047 src: /192.168.28.132:59558 dest: /192.168.28.131:50010
2018-01-03 15:27:40,559 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.132:59558, dest: /192.168.28.131:50010, bytes: 66101, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-85016196_12, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741871_1047, duration: 8301287
2018-01-03 15:27:40,559 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741871_1047, type=LAST_IN_PIPELINE terminating
2018-01-03 15:34:17,477 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741869_1045 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741869 for deletion
2018-01-03 15:34:17,479 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741870_1046 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741870 for deletion
2018-01-03 15:34:17,483 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741871_1047 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741871 for deletion
2018-01-03 15:34:17,541 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741870_1046 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741870
2018-01-03 15:34:17,568 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741869_1045 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741869
2018-01-03 15:34:17,569 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741871_1047 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741871
2018-01-03 15:36:07,642 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741872_1048 src: /192.168.28.130:32924 dest: /192.168.28.131:50010
2018-01-03 15:36:11,525 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.130:32924, dest: /192.168.28.131:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-938491162_1, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741872_1048, duration: 3875662326
2018-01-03 15:36:11,525 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741872_1048, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.28.132:50010] terminating
2018-01-03 15:36:11,599 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741873_1049 src: /192.168.28.130:32926 dest: /192.168.28.131:50010
2018-01-03 15:36:13,893 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.130:32926, dest: /192.168.28.131:50010, bytes: 74090689, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-938491162_1, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741873_1049, duration: 2287175531
2018-01-03 15:36:13,894 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741873_1049, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.28.132:50010] terminating
2018-01-03 15:36:14,130 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741874_1050 src: /192.168.28.130:32928 dest: /192.168.28.131:50010
2018-01-03 15:36:14,148 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.130:32928, dest: /192.168.28.131:50010, bytes: 66174, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-938491162_1, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741874_1050, duration: 10230783
2018-01-03 15:36:14,148 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741874_1050, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.28.132:50010] terminating
2018-01-03 15:38:35,112 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741875_1051 src: /192.168.28.130:32950 dest: /192.168.28.131:50010
2018-01-03 15:38:39,139 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.130:32950, dest: /192.168.28.131:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1509510445_1, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741875_1051, duration: 4003343678
2018-01-03 15:38:39,139 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741875_1051, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.28.132:50010] terminating
2018-01-03 15:38:39,246 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741876_1052 src: /192.168.28.132:60868 dest: /192.168.28.131:50010
2018-01-03 15:38:41,407 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.132:60868, dest: /192.168.28.131:50010, bytes: 74090689, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1509510445_1, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741876_1052, duration: 2158939773
2018-01-03 15:38:41,408 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741876_1052, type=LAST_IN_PIPELINE terminating
2018-01-03 15:38:41,756 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741877_1053 src: /192.168.28.132:60916 dest: /192.168.28.131:50010
2018-01-03 15:38:41,774 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.132:60916, dest: /192.168.28.131:50010, bytes: 66173, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1509510445_1, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741877_1053, duration: 13468128
2018-01-03 15:38:41,775 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741877_1053, type=LAST_IN_PIPELINE terminating
2018-01-03 15:41:05,839 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741872_1048 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741872 for deletion
2018-01-03 15:41:05,881 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741873_1049 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741873 for deletion
2018-01-03 15:41:05,884 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741872_1048 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741872
2018-01-03 15:41:05,886 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741874_1050 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741874 for deletion
2018-01-03 15:41:05,887 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741874_1050 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741874
2018-01-03 15:41:05,889 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741873_1049 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741873
2018-01-03 15:44:18,098 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741875_1051 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741875 for deletion
2018-01-03 15:44:18,100 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741876_1052 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741876 for deletion
2018-01-03 15:44:18,101 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741877_1053 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741877 for deletion
2018-01-03 15:44:18,106 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741876_1052 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741876
2018-01-03 15:44:18,108 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741875_1051 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741875
2018-01-03 15:44:18,109 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741877_1053 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741877
2018-01-03 15:45:40,527 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741878_1054 src: /192.168.28.132:33176 dest: /192.168.28.131:50010
2018-01-03 15:45:43,703 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.132:33176, dest: /192.168.28.131:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1910030958_1, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741878_1054, duration: 3153156433
2018-01-03 15:45:43,704 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741878_1054, type=LAST_IN_PIPELINE terminating
2018-01-03 15:45:43,717 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741879_1055 src: /192.168.28.130:32978 dest: /192.168.28.131:50010
2018-01-03 15:45:45,257 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.130:32978, dest: /192.168.28.131:50010, bytes: 75090816, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1910030958_1, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741879_1055, duration: 1533010924
2018-01-03 15:45:45,257 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741879_1055, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.28.132:50010] terminating
2018-01-03 15:45:45,548 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741880_1056 src: /192.168.28.132:33178 dest: /192.168.28.131:50010
2018-01-03 15:45:45,560 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.132:33178, dest: /192.168.28.131:50010, bytes: 66174, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1910030958_1, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741880_1056, duration: 9858573
2018-01-03 15:45:45,560 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741880_1056, type=LAST_IN_PIPELINE terminating
2018-01-03 15:47:04,990 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741878_1054 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741878 for deletion
2018-01-03 15:47:04,993 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741879_1055 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741879 for deletion
2018-01-03 15:47:04,995 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741880_1056 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741880 for deletion
2018-01-03 15:47:05,001 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741878_1054 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741878
2018-01-03 15:47:05,002 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741880_1056 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741880
2018-01-03 15:47:05,002 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741879_1055 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741879
2018-01-03 15:48:07,040 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741881_1057 src: /192.168.28.130:33184 dest: /192.168.28.131:50010
2018-01-03 15:48:10,326 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.130:33184, dest: /192.168.28.131:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-108787594_12, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741881_1057, duration: 3278477907
2018-01-03 15:48:10,327 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741881_1057, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.28.132:50010] terminating
2018-01-03 15:48:10,351 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741882_1058 src: /192.168.28.130:33186 dest: /192.168.28.131:50010
2018-01-03 15:48:12,241 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.130:33186, dest: /192.168.28.131:50010, bytes: 75090821, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-108787594_12, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741882_1058, duration: 1875943964
2018-01-03 15:48:12,242 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741882_1058, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.28.132:50010] terminating
2018-01-03 15:48:12,481 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741883_1059 src: /192.168.28.130:33188 dest: /192.168.28.131:50010
2018-01-03 15:48:12,499 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.130:33188, dest: /192.168.28.131:50010, bytes: 66101, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-108787594_12, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741883_1059, duration: 11528955
2018-01-03 15:48:12,499 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741883_1059, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.28.132:50010] terminating
2018-01-03 15:51:26,486 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "hadoop-worker01.local/192.168.28.131"; destination host is: "hadoop-master":8020; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1487)
	at org.apache.hadoop.ipc.Client.call(Client.java:1429)
	at org.apache.hadoop.ipc.Client.call(Client.java:1339)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:154)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:459)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:581)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:775)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1788)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1157)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1053)
2018-01-03 15:51:30,458 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop-master/192.168.28.129:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-01-03 15:51:31,459 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop-master/192.168.28.129:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-01-03 15:51:31,870 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2018-01-03 15:51:31,887 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at hadoop-worker01.local/192.168.28.131
************************************************************/
2018-01-05 14:57:20,509 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   user = hadoop
STARTUP_MSG:   host = hadoop-worker01.local/192.168.28.131
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.2
STARTUP_MSG:   classpath = /opt/hadoop-2.8.2/etc/hadoop:/opt/hadoop/share/hadoop/common/lib/activation-1.1.jar:/opt/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/opt/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/opt/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/opt/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/opt/hadoop/share/hadoop/common/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/opt/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/opt/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/opt/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/opt/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/opt/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-annotations-2.8.2.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-auth-2.8.2.jar:/opt/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/opt/hadoop/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/opt/hadoop/share/hadoop/common/lib/httpclient-4.5.2.jar:/opt/hadoop/share/hadoop/common/lib/httpcore-4.4.4.jar:/opt/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/common/lib/jcip-annotations-1.0.jar:/opt/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/opt/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/opt/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jsch-0.1.54.jar:/opt/hadoop/share/hadoop/common/lib/json-smart-1.1.1.jar:/opt/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/opt/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/common/lib/junit-4.11.jar:/opt/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/opt/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-3.9.jar:/opt/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/opt/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/opt/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/opt/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/opt/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/opt/hadoop/share/hadoop/common/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/opt/hadoop/share/hadoop/common/hadoop-common-2.8.2-tests.jar:/opt/hadoop/share/hadoop/common/hadoop-common-2.8.2.jar:/opt/hadoop/share/hadoop/common/hadoop-nfs-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs:/opt/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/okio-1.4.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/opt/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/opt/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-math-2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/opt/hadoop/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/opt/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/opt/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/opt/hadoop/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/opt/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/opt/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/opt/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/opt/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/opt/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/opt/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/opt/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/opt/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.2-tests.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.2.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 66c47f2a01ad9637879e95f80c41f798373828fb; compiled by 'jdu' on 2017-10-19T20:39Z
STARTUP_MSG:   java = 1.8.0_152
************************************************************/
2018-01-05 14:57:20,536 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2018-01-05 14:57:23,966 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2018-01-05 14:57:24,538 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2018-01-05 14:57:24,539 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2018-01-05 14:57:24,578 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2018-01-05 14:57:24,590 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is hadoop-worker01.local
2018-01-05 14:57:24,601 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2018-01-05 14:57:24,695 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2018-01-05 14:57:24,701 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 10485760 bytes/s
2018-01-05 14:57:24,701 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 50
2018-01-05 14:57:25,763 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2018-01-05 14:57:25,943 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2018-01-05 14:57:25,969 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2018-01-05 14:57:25,985 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2018-01-05 14:57:25,998 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2018-01-05 14:57:25,998 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2018-01-05 14:57:25,998 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2018-01-05 14:57:26,058 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 41405
2018-01-05 14:57:26,059 INFO org.mortbay.log: jetty-6.1.26
2018-01-05 14:57:26,925 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:41405
2018-01-05 14:57:27,456 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2018-01-05 14:57:27,470 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2018-01-05 14:57:28,323 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hadoop
2018-01-05 14:57:28,323 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2018-01-05 14:57:28,639 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2018-01-05 14:57:28,701 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2018-01-05 14:57:29,057 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2018-01-05 14:57:29,078 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2018-01-05 14:57:29,121 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2018-01-05 14:57:29,173 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to hadoop-master/192.168.28.129:8020 starting to offer service
2018-01-05 14:57:29,286 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2018-01-05 14:57:29,294 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2018-01-05 14:57:30,590 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to hadoop-master/192.168.28.129:8020
2018-01-05 14:57:30,612 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2018-01-05 14:57:30,701 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /var/lib/hadoop2/dfs/data/1/in_use.lock acquired by nodename 1785@hadoop-worker01.local
2018-01-05 14:57:30,803 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /var/lib/hadoop2/dfs/data/2/in_use.lock acquired by nodename 1785@hadoop-worker01.local
2018-01-05 14:57:31,104 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-199680249-192.168.28.129-1514169681712
2018-01-05 14:57:31,104 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712
2018-01-05 14:57:31,579 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-199680249-192.168.28.129-1514169681712
2018-01-05 14:57:31,579 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712
2018-01-05 14:57:31,597 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1794692519;bpid=BP-199680249-192.168.28.129-1514169681712;lv=-57;nsInfo=lv=-63;cid=CID-a84222ca-b260-441d-81c3-93c4f4f5d131;nsid=1794692519;c=1514169681712;bpid=BP-199680249-192.168.28.129-1514169681712;dnuuid=46e3cb45-0c76-4bc1-9342-af90beb9d892
2018-01-05 14:57:31,880 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-db2b9f5a-2dbc-404d-8b87-9ff8e79ccdd2
2018-01-05 14:57:31,881 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /var/lib/hadoop2/dfs/data/1/current, StorageType: DISK
2018-01-05 14:57:31,903 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-99e39a6f-a3c8-4fc0-9709-0aafdcfc87d6
2018-01-05 14:57:31,903 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /var/lib/hadoop2/dfs/data/2/current, StorageType: DISK
2018-01-05 14:57:32,004 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2018-01-05 14:57:32,153 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Volume reference is released.
2018-01-05 14:57:32,153 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-199680249-192.168.28.129-1514169681712
2018-01-05 14:57:32,176 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current...
2018-01-05 14:57:32,171 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current...
2018-01-05 14:57:32,615 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-199680249-192.168.28.129-1514169681712 on /var/lib/hadoop2/dfs/data/1/current: 436ms
2018-01-05 14:57:32,812 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-199680249-192.168.28.129-1514169681712 on /var/lib/hadoop2/dfs/data/2/current: 636ms
2018-01-05 14:57:32,858 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-199680249-192.168.28.129-1514169681712: 704ms
2018-01-05 14:57:32,866 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current...
2018-01-05 14:57:32,867 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/replicas doesn't exist 
2018-01-05 14:57:32,907 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current...
2018-01-05 14:57:32,911 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current: 44ms
2018-01-05 14:57:32,907 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/replicas doesn't exist 
2018-01-05 14:57:33,010 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current: 103ms
2018-01-05 14:57:33,014 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 152ms
2018-01-05 14:57:33,172 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/var/lib/hadoop2/dfs/data/2, DS-99e39a6f-a3c8-4fc0-9709-0aafdcfc87d6): no suitable block pools found to scan.  Waiting 852273556 ms.
2018-01-05 14:57:33,264 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/var/lib/hadoop2/dfs/data/1, DS-db2b9f5a-2dbc-404d-8b87-9ff8e79ccdd2): no suitable block pools found to scan.  Waiting 852273464 ms.
2018-01-05 14:57:33,359 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1/5/18 6:19 PM with interval of 21600000ms
2018-01-05 14:57:33,421 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-199680249-192.168.28.129-1514169681712 (Datanode Uuid 46e3cb45-0c76-4bc1-9342-af90beb9d892) service to hadoop-master/192.168.28.129:8020 beginning handshake with NN
2018-01-05 14:57:33,732 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-199680249-192.168.28.129-1514169681712 (Datanode Uuid 46e3cb45-0c76-4bc1-9342-af90beb9d892) service to hadoop-master/192.168.28.129:8020 successfully registered with NN
2018-01-05 14:57:33,732 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode hadoop-master/192.168.28.129:8020 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2018-01-05 14:57:34,395 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x96318680e0ce4cab,  containing 2 storage report(s), of which we sent 2. The reports had 20 total blocks and used 1 RPC(s). This took 18 msec to generate and 339 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-01-05 14:57:34,395 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-199680249-192.168.28.129-1514169681712
2018-01-05 15:37:06,648 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741884_1060 src: /192.168.28.130:44092 dest: /192.168.28.131:50010
2018-01-05 15:37:07,066 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.130:44092, dest: /192.168.28.131:50010, bytes: 63213, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_989782417_1, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741884_1060, duration: 126112484
2018-01-05 15:37:07,067 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741884_1060, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.28.132:50010] terminating
2018-01-05 15:38:52,145 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741885_1061 src: /192.168.28.130:44300 dest: /192.168.28.131:50010
2018-01-05 15:38:59,234 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.130:44300, dest: /192.168.28.131:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1259734289_12, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741885_1061, duration: 7050183994
2018-01-05 15:38:59,236 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741885_1061, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.28.132:50010] terminating
2018-01-05 15:38:59,452 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741886_1062 src: /192.168.28.132:45278 dest: /192.168.28.131:50010
2018-01-05 15:39:03,995 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.132:45278, dest: /192.168.28.131:50010, bytes: 75090821, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1259734289_12, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741886_1062, duration: 4538289044
2018-01-05 15:39:04,025 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741886_1062, type=LAST_IN_PIPELINE terminating
2018-01-05 15:39:04,786 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741887_1063 src: /192.168.28.132:45280 dest: /192.168.28.131:50010
2018-01-05 15:39:04,800 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.132:45280, dest: /192.168.28.131:50010, bytes: 66101, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1259734289_12, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741887_1063, duration: 9527702
2018-01-05 15:39:04,801 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741887_1063, type=LAST_IN_PIPELINE terminating
2018-01-05 15:39:48,440 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1640ms
No GCs detected
2018-01-05 15:40:11,503 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 4220ms
No GCs detected
2018-01-05 15:40:24,985 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1318ms
No GCs detected
2018-01-05 15:40:28,267 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2780ms
No GCs detected
2018-01-05 16:58:47,562 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 5920ms
No GCs detected
2018-01-05 16:59:01,293 WARN org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 13229ms
No GCs detected
2018-01-05 16:59:06,904 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1533ms
No GCs detected
2018-01-05 17:02:22,470 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741885_1061 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741885 for deletion
2018-01-05 17:02:22,474 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741886_1062 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741886 for deletion
2018-01-05 17:02:22,475 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741887_1063 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741887 for deletion
2018-01-05 17:02:22,476 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741885_1061 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741885
2018-01-05 17:02:22,477 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741886_1062 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741886
2018-01-05 17:02:22,477 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741887_1063 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741887
2018-01-05 17:02:47,592 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741888_1064 src: /192.168.28.132:45320 dest: /192.168.28.131:50010
2018-01-05 17:02:52,370 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.132:45320, dest: /192.168.28.131:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-502258946_12, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741888_1064, duration: 4776010145
2018-01-05 17:02:52,372 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741888_1064, type=LAST_IN_PIPELINE terminating
2018-01-05 17:02:52,537 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741889_1065 src: /192.168.28.132:45322 dest: /192.168.28.131:50010
2018-01-05 17:02:55,117 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.132:45322, dest: /192.168.28.131:50010, bytes: 75090821, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-502258946_12, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741889_1065, duration: 2578232500
2018-01-05 17:02:55,118 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741889_1065, type=LAST_IN_PIPELINE terminating
2018-01-05 17:02:55,722 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741890_1066 src: /192.168.28.130:44582 dest: /192.168.28.131:50010
2018-01-05 17:02:55,737 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.130:44582, dest: /192.168.28.131:50010, bytes: 66101, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-502258946_12, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741890_1066, duration: 9089692
2018-01-05 17:02:55,738 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741890_1066, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.28.132:50010] terminating
2018-01-05 17:04:05,916 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 8400ms
No GCs detected
2018-01-05 17:06:50,589 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741891_1067 src: /192.168.28.130:44638 dest: /192.168.28.131:50010
2018-01-05 17:06:50,642 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.130:44638, dest: /192.168.28.131:50010, bytes: 63213, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_253152837_1, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741891_1067, duration: 41772769
2018-01-05 17:06:50,642 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741891_1067, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.28.132:50010] terminating
2018-01-05 17:06:54,365 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741884_1060 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741884 for deletion
2018-01-05 17:06:54,367 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741884_1060 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741884
2018-01-05 17:15:19,202 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1692ms
No GCs detected
2018-01-05 17:34:18,584 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741888_1064 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741888 for deletion
2018-01-05 17:34:18,587 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741889_1065 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741889 for deletion
2018-01-05 17:34:18,588 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741888_1064 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741888
2018-01-05 17:34:18,589 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741890_1066 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741890 for deletion
2018-01-05 17:34:18,589 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741889_1065 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741889
2018-01-05 17:34:18,590 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741890_1066 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741890
2018-01-05 17:35:15,170 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741892_1068 src: /192.168.28.130:44946 dest: /192.168.28.131:50010
2018-01-05 17:35:19,998 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.130:44946, dest: /192.168.28.131:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1973549_12, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741892_1068, duration: 4717830767
2018-01-05 17:35:19,999 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741892_1068, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.28.132:50010] terminating
2018-01-05 17:35:20,092 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741893_1069 src: /192.168.28.132:45348 dest: /192.168.28.131:50010
2018-01-05 17:35:22,465 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.132:45348, dest: /192.168.28.131:50010, bytes: 75090821, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1973549_12, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741893_1069, duration: 2361899859
2018-01-05 17:35:22,465 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741893_1069, type=LAST_IN_PIPELINE terminating
2018-01-05 17:35:22,804 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741894_1070 src: /192.168.28.130:44950 dest: /192.168.28.131:50010
2018-01-05 17:35:22,994 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.130:44950, dest: /192.168.28.131:50010, bytes: 66101, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1973549_12, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741894_1070, duration: 147431334
2018-01-05 17:35:22,995 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741894_1070, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.28.132:50010] terminating
2018-01-05 17:37:12,466 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1375ms
No GCs detected
2018-01-05 17:37:19,158 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 4500ms
No GCs detected
2018-01-05 17:37:21,848 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2190ms
No GCs detected
2018-01-05 17:37:41,367 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 8394ms
No GCs detected
2018-01-05 17:37:44,006 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1098ms
No GCs detected
2018-01-05 17:39:07,385 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2542ms
No GCs detected
2018-01-05 17:41:06,937 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741892_1068 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741892 for deletion
2018-01-05 17:41:06,938 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741893_1069 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741893 for deletion
2018-01-05 17:41:06,940 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741892_1068 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741892
2018-01-05 17:41:06,941 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741894_1070 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741894 for deletion
2018-01-05 17:41:06,941 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741894_1070 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741894
2018-01-05 17:41:06,944 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741893_1069 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741893
2018-01-05 17:48:36,897 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 3079ms
No GCs detected
2018-01-05 19:01:36,320 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "hadoop-worker01.local/192.168.28.131"; destination host is: "hadoop-master":8020; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1487)
	at org.apache.hadoop.ipc.Client.call(Client.java:1429)
	at org.apache.hadoop.ipc.Client.call(Client.java:1339)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:154)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:459)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:581)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:775)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1788)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1157)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1053)
2018-01-05 19:01:40,289 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop-master/192.168.28.129:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-01-05 19:01:41,291 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop-master/192.168.28.129:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-01-05 19:01:41,925 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2018-01-05 19:01:41,947 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at hadoop-worker01.local/192.168.28.131
************************************************************/
2018-01-07 12:44:24,907 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   user = hadoop
STARTUP_MSG:   host = hadoop-worker01.local/192.168.28.131
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.2
STARTUP_MSG:   classpath = /opt/hadoop-2.8.2/etc/hadoop:/opt/hadoop/share/hadoop/common/lib/activation-1.1.jar:/opt/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/opt/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/opt/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/opt/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/opt/hadoop/share/hadoop/common/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/opt/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/opt/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/opt/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/opt/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/opt/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-annotations-2.8.2.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-auth-2.8.2.jar:/opt/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/opt/hadoop/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/opt/hadoop/share/hadoop/common/lib/httpclient-4.5.2.jar:/opt/hadoop/share/hadoop/common/lib/httpcore-4.4.4.jar:/opt/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/common/lib/jcip-annotations-1.0.jar:/opt/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/opt/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/opt/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jsch-0.1.54.jar:/opt/hadoop/share/hadoop/common/lib/json-smart-1.1.1.jar:/opt/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/opt/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/common/lib/junit-4.11.jar:/opt/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/opt/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-3.9.jar:/opt/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/opt/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/opt/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/opt/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/opt/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/opt/hadoop/share/hadoop/common/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/opt/hadoop/share/hadoop/common/hadoop-common-2.8.2-tests.jar:/opt/hadoop/share/hadoop/common/hadoop-common-2.8.2.jar:/opt/hadoop/share/hadoop/common/hadoop-nfs-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs:/opt/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/okio-1.4.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/opt/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/opt/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-math-2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/opt/hadoop/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/opt/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/opt/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/opt/hadoop/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/opt/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/opt/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/opt/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/opt/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/opt/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/opt/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/opt/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/opt/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.2-tests.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.2.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 66c47f2a01ad9637879e95f80c41f798373828fb; compiled by 'jdu' on 2017-10-19T20:39Z
STARTUP_MSG:   java = 1.8.0_152
************************************************************/
2018-01-07 12:44:24,922 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2018-01-07 12:44:35,697 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2018-01-07 12:44:36,147 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2018-01-07 12:44:36,147 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2018-01-07 12:44:36,212 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2018-01-07 12:44:36,239 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is hadoop-worker01.local
2018-01-07 12:44:36,247 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2018-01-07 12:44:36,322 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2018-01-07 12:44:36,330 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 10485760 bytes/s
2018-01-07 12:44:36,330 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 50
2018-01-07 12:44:37,216 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2018-01-07 12:44:37,259 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2018-01-07 12:44:37,281 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2018-01-07 12:44:37,296 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2018-01-07 12:44:37,299 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2018-01-07 12:44:37,299 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2018-01-07 12:44:37,300 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2018-01-07 12:44:37,348 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 44662
2018-01-07 12:44:37,348 INFO org.mortbay.log: jetty-6.1.26
2018-01-07 12:44:38,302 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:44662
2018-01-07 12:44:39,910 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2018-01-07 12:44:39,975 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2018-01-07 12:44:41,350 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hadoop
2018-01-07 12:44:41,350 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2018-01-07 12:44:41,756 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2018-01-07 12:44:41,866 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2018-01-07 12:44:42,831 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2018-01-07 12:44:42,847 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2018-01-07 12:44:42,976 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2018-01-07 12:44:43,032 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to hadoop-master/192.168.28.129:8020 starting to offer service
2018-01-07 12:44:43,147 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2018-01-07 12:44:43,159 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2018-01-07 12:44:46,098 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to hadoop-master/192.168.28.129:8020
2018-01-07 12:44:46,115 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2018-01-07 12:44:46,708 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /var/lib/hadoop2/dfs/data/1/in_use.lock acquired by nodename 1683@hadoop-worker01.local
2018-01-07 12:44:46,743 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /var/lib/hadoop2/dfs/data/2/in_use.lock acquired by nodename 1683@hadoop-worker01.local
2018-01-07 12:44:47,359 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-199680249-192.168.28.129-1514169681712
2018-01-07 12:44:47,371 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712
2018-01-07 12:44:47,826 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-199680249-192.168.28.129-1514169681712
2018-01-07 12:44:47,826 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712
2018-01-07 12:44:47,871 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1794692519;bpid=BP-199680249-192.168.28.129-1514169681712;lv=-57;nsInfo=lv=-63;cid=CID-a84222ca-b260-441d-81c3-93c4f4f5d131;nsid=1794692519;c=1514169681712;bpid=BP-199680249-192.168.28.129-1514169681712;dnuuid=46e3cb45-0c76-4bc1-9342-af90beb9d892
2018-01-07 12:44:48,595 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-db2b9f5a-2dbc-404d-8b87-9ff8e79ccdd2
2018-01-07 12:44:48,599 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /var/lib/hadoop2/dfs/data/1/current, StorageType: DISK
2018-01-07 12:44:48,609 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-99e39a6f-a3c8-4fc0-9709-0aafdcfc87d6
2018-01-07 12:44:48,609 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /var/lib/hadoop2/dfs/data/2/current, StorageType: DISK
2018-01-07 12:44:48,810 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2018-01-07 12:44:48,991 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Volume reference is released.
2018-01-07 12:44:48,991 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-199680249-192.168.28.129-1514169681712
2018-01-07 12:44:49,023 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current...
2018-01-07 12:44:49,239 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current...
2018-01-07 12:44:49,654 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-199680249-192.168.28.129-1514169681712 on /var/lib/hadoop2/dfs/data/2/current: 440ms
2018-01-07 12:44:49,880 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-199680249-192.168.28.129-1514169681712 on /var/lib/hadoop2/dfs/data/1/current: 641ms
2018-01-07 12:44:49,881 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-199680249-192.168.28.129-1514169681712: 889ms
2018-01-07 12:44:49,887 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current...
2018-01-07 12:44:49,888 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/replicas doesn't exist 
2018-01-07 12:44:49,892 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current...
2018-01-07 12:44:49,893 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/replicas doesn't exist 
2018-01-07 12:44:49,928 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current: 40ms
2018-01-07 12:44:50,027 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current: 135ms
2018-01-07 12:44:50,082 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 196ms
2018-01-07 12:44:50,219 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/var/lib/hadoop2/dfs/data/2, DS-99e39a6f-a3c8-4fc0-9709-0aafdcfc87d6): no suitable block pools found to scan.  Waiting 687436509 ms.
2018-01-07 12:44:50,223 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/var/lib/hadoop2/dfs/data/1, DS-db2b9f5a-2dbc-404d-8b87-9ff8e79ccdd2): no suitable block pools found to scan.  Waiting 687436505 ms.
2018-01-07 12:44:50,277 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1/7/18 2:15 PM with interval of 21600000ms
2018-01-07 12:44:50,487 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-199680249-192.168.28.129-1514169681712 (Datanode Uuid 46e3cb45-0c76-4bc1-9342-af90beb9d892) service to hadoop-master/192.168.28.129:8020 beginning handshake with NN
2018-01-07 12:44:50,697 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-199680249-192.168.28.129-1514169681712 (Datanode Uuid 46e3cb45-0c76-4bc1-9342-af90beb9d892) service to hadoop-master/192.168.28.129:8020 successfully registered with NN
2018-01-07 12:44:50,697 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode hadoop-master/192.168.28.129:8020 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2018-01-07 12:44:51,736 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x8563cdab4df25149,  containing 2 storage report(s), of which we sent 2. The reports had 21 total blocks and used 1 RPC(s). This took 12 msec to generate and 328 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-01-07 12:44:51,737 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-199680249-192.168.28.129-1514169681712
2018-01-07 13:07:44,707 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.IOException: Failed on local exception: java.io.IOException: Connection reset by peer; Host Details : local host is: "hadoop-worker01.local/192.168.28.131"; destination host is: "hadoop-master":8020; 
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:782)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1487)
	at org.apache.hadoop.ipc.Client.call(Client.java:1429)
	at org.apache.hadoop.ipc.Client.call(Client.java:1339)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:154)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:459)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:581)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:775)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
	at org.apache.hadoop.net.SocketInputStream$Reader.performIO(SocketInputStream.java:57)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:265)
	at java.io.FilterInputStream.read(FilterInputStream.java:83)
	at java.io.FilterInputStream.read(FilterInputStream.java:83)
	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:554)
	at java.io.DataInputStream.readInt(DataInputStream.java:387)
	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1788)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1157)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1053)
2018-01-07 13:07:48,698 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop-master/192.168.28.129:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-01-07 13:07:49,704 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop-master/192.168.28.129:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-01-07 13:07:50,710 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop-master/192.168.28.129:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-01-07 13:07:50,885 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2018-01-07 13:07:50,896 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at hadoop-worker01.local/192.168.28.131
************************************************************/
2018-01-07 13:08:34,205 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   user = hadoop
STARTUP_MSG:   host = hadoop-worker01.local/192.168.28.131
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.2
STARTUP_MSG:   classpath = /opt/hadoop/etc/hadoop:/opt/hadoop/share/hadoop/common/lib/activation-1.1.jar:/opt/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/opt/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/opt/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/opt/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/opt/hadoop/share/hadoop/common/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/opt/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/opt/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/opt/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/opt/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/opt/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-annotations-2.8.2.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-auth-2.8.2.jar:/opt/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/opt/hadoop/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/opt/hadoop/share/hadoop/common/lib/httpclient-4.5.2.jar:/opt/hadoop/share/hadoop/common/lib/httpcore-4.4.4.jar:/opt/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/common/lib/jcip-annotations-1.0.jar:/opt/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/opt/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/opt/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jsch-0.1.54.jar:/opt/hadoop/share/hadoop/common/lib/json-smart-1.1.1.jar:/opt/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/opt/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/common/lib/junit-4.11.jar:/opt/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/opt/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-3.9.jar:/opt/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/opt/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/opt/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/opt/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/opt/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/opt/hadoop/share/hadoop/common/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/opt/hadoop/share/hadoop/common/hadoop-common-2.8.2-tests.jar:/opt/hadoop/share/hadoop/common/hadoop-common-2.8.2.jar:/opt/hadoop/share/hadoop/common/hadoop-nfs-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs:/opt/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/okio-1.4.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/opt/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/opt/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-math-2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/opt/hadoop/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/opt/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/opt/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/opt/hadoop/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/opt/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/opt/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/opt/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/opt/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/opt/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/opt/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/opt/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/opt/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.2-tests.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.2.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 66c47f2a01ad9637879e95f80c41f798373828fb; compiled by 'jdu' on 2017-10-19T20:39Z
STARTUP_MSG:   java = 1.8.0_152
************************************************************/
2018-01-07 13:08:34,231 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2018-01-07 13:08:36,177 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2018-01-07 13:08:36,396 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2018-01-07 13:08:36,396 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2018-01-07 13:08:36,410 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2018-01-07 13:08:36,412 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is hadoop-worker01.local
2018-01-07 13:08:36,419 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2018-01-07 13:08:36,569 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2018-01-07 13:08:36,573 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 10485760 bytes/s
2018-01-07 13:08:36,573 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 50
2018-01-07 13:08:37,116 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2018-01-07 13:08:37,164 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2018-01-07 13:08:37,200 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2018-01-07 13:08:37,211 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2018-01-07 13:08:37,238 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2018-01-07 13:08:37,238 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2018-01-07 13:08:37,238 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2018-01-07 13:08:37,416 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 41883
2018-01-07 13:08:37,417 INFO org.mortbay.log: jetty-6.1.26
2018-01-07 13:08:38,099 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:41883
2018-01-07 13:08:38,988 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2018-01-07 13:08:38,993 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2018-01-07 13:08:39,928 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hadoop
2018-01-07 13:08:39,928 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2018-01-07 13:08:40,083 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2018-01-07 13:08:40,180 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2018-01-07 13:08:40,400 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2018-01-07 13:08:40,417 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2018-01-07 13:08:40,573 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2018-01-07 13:08:40,626 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to hadoop-master/192.168.28.129:8020 starting to offer service
2018-01-07 13:08:41,048 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2018-01-07 13:08:41,069 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2018-01-07 13:08:42,122 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to hadoop-master/192.168.28.129:8020
2018-01-07 13:08:42,138 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2018-01-07 13:08:42,147 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /var/lib/hadoop2/dfs/data/1/in_use.lock acquired by nodename 2392@hadoop-worker01.local
2018-01-07 13:08:42,157 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /var/lib/hadoop2/dfs/data/2/in_use.lock acquired by nodename 2392@hadoop-worker01.local
2018-01-07 13:08:42,614 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-199680249-192.168.28.129-1514169681712
2018-01-07 13:08:42,614 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712
2018-01-07 13:08:42,844 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-199680249-192.168.28.129-1514169681712
2018-01-07 13:08:42,844 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712
2018-01-07 13:08:42,848 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1794692519;bpid=BP-199680249-192.168.28.129-1514169681712;lv=-57;nsInfo=lv=-63;cid=CID-a84222ca-b260-441d-81c3-93c4f4f5d131;nsid=1794692519;c=1514169681712;bpid=BP-199680249-192.168.28.129-1514169681712;dnuuid=46e3cb45-0c76-4bc1-9342-af90beb9d892
2018-01-07 13:08:43,374 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-db2b9f5a-2dbc-404d-8b87-9ff8e79ccdd2
2018-01-07 13:08:43,375 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /var/lib/hadoop2/dfs/data/1/current, StorageType: DISK
2018-01-07 13:08:43,410 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-99e39a6f-a3c8-4fc0-9709-0aafdcfc87d6
2018-01-07 13:08:43,410 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /var/lib/hadoop2/dfs/data/2/current, StorageType: DISK
2018-01-07 13:08:43,417 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2018-01-07 13:08:43,458 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Volume reference is released.
2018-01-07 13:08:43,458 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-199680249-192.168.28.129-1514169681712
2018-01-07 13:08:43,460 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current...
2018-01-07 13:08:43,461 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current...
2018-01-07 13:08:43,501 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current: 270770176
2018-01-07 13:08:43,507 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-199680249-192.168.28.129-1514169681712 on /var/lib/hadoop2/dfs/data/1/current: 45ms
2018-01-07 13:08:43,510 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current: 150491136
2018-01-07 13:08:43,541 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-199680249-192.168.28.129-1514169681712 on /var/lib/hadoop2/dfs/data/2/current: 79ms
2018-01-07 13:08:43,555 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-199680249-192.168.28.129-1514169681712: 97ms
2018-01-07 13:08:43,564 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current...
2018-01-07 13:08:43,564 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/replicas doesn't exist 
2018-01-07 13:08:43,626 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current...
2018-01-07 13:08:43,629 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current: 64ms
2018-01-07 13:08:43,626 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/replicas doesn't exist 
2018-01-07 13:08:43,643 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current: 18ms
2018-01-07 13:08:43,644 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 85ms
2018-01-07 13:08:43,853 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/var/lib/hadoop2/dfs/data/2, DS-99e39a6f-a3c8-4fc0-9709-0aafdcfc87d6): no suitable block pools found to scan.  Waiting 686002875 ms.
2018-01-07 13:08:43,865 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1/7/18 1:52 PM with interval of 21600000ms
2018-01-07 13:08:43,875 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/var/lib/hadoop2/dfs/data/1, DS-db2b9f5a-2dbc-404d-8b87-9ff8e79ccdd2): no suitable block pools found to scan.  Waiting 686002853 ms.
2018-01-07 13:08:44,038 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-199680249-192.168.28.129-1514169681712 (Datanode Uuid 46e3cb45-0c76-4bc1-9342-af90beb9d892) service to hadoop-master/192.168.28.129:8020 beginning handshake with NN
2018-01-07 13:08:44,433 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-199680249-192.168.28.129-1514169681712 (Datanode Uuid 46e3cb45-0c76-4bc1-9342-af90beb9d892) service to hadoop-master/192.168.28.129:8020 successfully registered with NN
2018-01-07 13:08:44,433 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode hadoop-master/192.168.28.129:8020 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2018-01-07 13:08:45,100 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x3cabc8aecb642c39,  containing 2 storage report(s), of which we sent 2. The reports had 21 total blocks and used 1 RPC(s). This took 9 msec to generate and 228 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-01-07 13:08:45,100 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-199680249-192.168.28.129-1514169681712
2018-01-07 13:10:59,817 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741895_1071 src: /192.168.28.130:36926 dest: /192.168.28.131:50010
2018-01-07 13:11:00,868 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.130:36926, dest: /192.168.28.131:50010, bytes: 399, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-347199629_45, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741895_1071, duration: 283251201
2018-01-07 13:11:00,869 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741895_1071, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.28.132:50010] terminating
2018-01-07 13:13:30,443 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741896_1072 src: /192.168.28.130:37076 dest: /192.168.28.131:50010
2018-01-07 13:13:39,387 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:304ms (threshold=300ms)
2018-01-07 13:13:39,496 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.130:37076, dest: /192.168.28.131:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1585732224_12, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741896_1072, duration: 9044705962
2018-01-07 13:13:39,499 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741896_1072, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.28.132:50010] terminating
2018-01-07 13:13:39,602 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741897_1073 src: /192.168.28.132:52488 dest: /192.168.28.131:50010
2018-01-07 13:13:44,955 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.132:52488, dest: /192.168.28.131:50010, bytes: 75090821, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1585732224_12, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741897_1073, duration: 5350421718
2018-01-07 13:13:44,956 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741897_1073, type=LAST_IN_PIPELINE terminating
2018-01-07 13:13:45,868 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741898_1074 src: /192.168.28.132:52490 dest: /192.168.28.131:50010
2018-01-07 13:13:45,905 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.132:52490, dest: /192.168.28.131:50010, bytes: 66091, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1585732224_12, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741898_1074, duration: 16712206
2018-01-07 13:13:45,907 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741898_1074, type=LAST_IN_PIPELINE terminating
2018-01-07 13:14:24,990 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2514ms
No GCs detected
2018-01-07 13:14:30,138 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 3073ms
No GCs detected
2018-01-07 13:14:37,380 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1681ms
No GCs detected
2018-01-07 13:17:13,903 WARN org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 147735ms
No GCs detected
2018-01-07 13:17:46,314 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 3721ms
No GCs detected
2018-01-07 13:17:50,100 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 3285ms
No GCs detected
2018-01-07 13:17:56,573 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2582ms
No GCs detected
2018-01-07 13:18:06,891 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 9817ms
No GCs detected
2018-01-07 13:18:21,092 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 9445ms
No GCs detected
2018-01-07 13:18:27,727 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1575ms
No GCs detected
2018-01-07 13:18:40,426 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 6849ms
No GCs detected
2018-01-07 13:18:46,562 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1640ms
No GCs detected
2018-01-07 13:18:57,444 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1143ms
No GCs detected
2018-01-07 13:19:15,694 WARN org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 17235ms
No GCs detected
2018-01-07 13:19:48,763 WARN org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 32568ms
No GCs detected
2018-01-07 13:19:53,121 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 3857ms
No GCs detected
2018-01-07 13:20:11,972 WARN org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 18351ms
No GCs detected
2018-01-07 13:20:39,024 WARN org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 26551ms
No GCs detected
2018-01-07 13:20:54,786 WARN org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 15261ms
No GCs detected
2018-01-07 13:20:59,681 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 4395ms
No GCs detected
2018-01-07 13:21:26,360 WARN org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 26178ms
No GCs detected
2018-01-07 14:18:17,263 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   user = hadoop
STARTUP_MSG:   host = hadoop-worker01.local/192.168.28.131
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.2
STARTUP_MSG:   classpath = /opt/hadoop-2.8.2/etc/hadoop:/opt/hadoop/share/hadoop/common/lib/activation-1.1.jar:/opt/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/opt/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/opt/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/opt/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/opt/hadoop/share/hadoop/common/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/opt/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/opt/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/opt/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/opt/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/opt/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-annotations-2.8.2.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-auth-2.8.2.jar:/opt/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/opt/hadoop/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/opt/hadoop/share/hadoop/common/lib/httpclient-4.5.2.jar:/opt/hadoop/share/hadoop/common/lib/httpcore-4.4.4.jar:/opt/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/common/lib/jcip-annotations-1.0.jar:/opt/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/opt/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/opt/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jsch-0.1.54.jar:/opt/hadoop/share/hadoop/common/lib/json-smart-1.1.1.jar:/opt/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/opt/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/common/lib/junit-4.11.jar:/opt/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/opt/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-3.9.jar:/opt/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/opt/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/opt/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/opt/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/opt/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/opt/hadoop/share/hadoop/common/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/opt/hadoop/share/hadoop/common/hadoop-common-2.8.2-tests.jar:/opt/hadoop/share/hadoop/common/hadoop-common-2.8.2.jar:/opt/hadoop/share/hadoop/common/hadoop-nfs-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs:/opt/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/okio-1.4.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/opt/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/opt/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-math-2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/opt/hadoop/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/opt/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/opt/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/opt/hadoop/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/opt/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/opt/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/opt/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/opt/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/opt/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/opt/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/opt/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/opt/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.2-tests.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.2.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 66c47f2a01ad9637879e95f80c41f798373828fb; compiled by 'jdu' on 2017-10-19T20:39Z
STARTUP_MSG:   java = 1.8.0_152
************************************************************/
2018-01-07 14:18:17,291 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2018-01-07 14:18:19,457 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2018-01-07 14:18:19,755 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2018-01-07 14:18:19,755 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2018-01-07 14:18:19,772 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2018-01-07 14:18:19,792 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is hadoop-worker01.local
2018-01-07 14:18:19,811 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2018-01-07 14:18:19,883 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2018-01-07 14:18:19,887 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 10485760 bytes/s
2018-01-07 14:18:19,887 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 50
2018-01-07 14:18:20,685 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2018-01-07 14:18:20,759 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2018-01-07 14:18:20,777 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2018-01-07 14:18:20,791 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2018-01-07 14:18:20,794 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2018-01-07 14:18:20,794 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2018-01-07 14:18:20,794 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2018-01-07 14:18:20,863 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 35919
2018-01-07 14:18:20,863 INFO org.mortbay.log: jetty-6.1.26
2018-01-07 14:18:21,522 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:35919
2018-01-07 14:18:22,553 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2018-01-07 14:18:22,565 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2018-01-07 14:18:23,251 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hadoop
2018-01-07 14:18:23,256 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2018-01-07 14:18:23,532 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2018-01-07 14:18:23,580 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2018-01-07 14:18:23,770 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2018-01-07 14:18:23,797 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2018-01-07 14:18:23,981 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2018-01-07 14:18:24,036 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to hadoop-master/192.168.28.129:8020 starting to offer service
2018-01-07 14:18:24,107 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2018-01-07 14:18:24,112 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2018-01-07 14:18:24,985 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to hadoop-master/192.168.28.129:8020
2018-01-07 14:18:25,016 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2018-01-07 14:18:25,085 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /var/lib/hadoop2/dfs/data/1/in_use.lock acquired by nodename 1741@hadoop-worker01.local
2018-01-07 14:18:25,183 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /var/lib/hadoop2/dfs/data/2/in_use.lock acquired by nodename 1741@hadoop-worker01.local
2018-01-07 14:18:25,535 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-199680249-192.168.28.129-1514169681712
2018-01-07 14:18:25,536 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712
2018-01-07 14:18:26,007 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-199680249-192.168.28.129-1514169681712
2018-01-07 14:18:26,008 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712
2018-01-07 14:18:26,022 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1794692519;bpid=BP-199680249-192.168.28.129-1514169681712;lv=-57;nsInfo=lv=-63;cid=CID-a84222ca-b260-441d-81c3-93c4f4f5d131;nsid=1794692519;c=1514169681712;bpid=BP-199680249-192.168.28.129-1514169681712;dnuuid=46e3cb45-0c76-4bc1-9342-af90beb9d892
2018-01-07 14:18:26,357 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-db2b9f5a-2dbc-404d-8b87-9ff8e79ccdd2
2018-01-07 14:18:26,358 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /var/lib/hadoop2/dfs/data/1/current, StorageType: DISK
2018-01-07 14:18:26,362 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-99e39a6f-a3c8-4fc0-9709-0aafdcfc87d6
2018-01-07 14:18:26,362 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /var/lib/hadoop2/dfs/data/2/current, StorageType: DISK
2018-01-07 14:18:26,459 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2018-01-07 14:18:26,505 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Volume reference is released.
2018-01-07 14:18:26,506 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-199680249-192.168.28.129-1514169681712
2018-01-07 14:18:26,524 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current...
2018-01-07 14:18:26,529 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current...
2018-01-07 14:18:26,819 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-199680249-192.168.28.129-1514169681712 on /var/lib/hadoop2/dfs/data/2/current: 271ms
2018-01-07 14:18:26,854 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-199680249-192.168.28.129-1514169681712 on /var/lib/hadoop2/dfs/data/1/current: 310ms
2018-01-07 14:18:26,855 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-199680249-192.168.28.129-1514169681712: 348ms
2018-01-07 14:18:26,888 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current...
2018-01-07 14:18:26,889 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/replicas doesn't exist 
2018-01-07 14:18:26,925 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current...
2018-01-07 14:18:26,925 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current: 37ms
2018-01-07 14:18:26,926 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/replicas doesn't exist 
2018-01-07 14:18:26,946 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current: 22ms
2018-01-07 14:18:26,948 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 80ms
2018-01-07 14:18:27,184 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/var/lib/hadoop2/dfs/data/2, DS-99e39a6f-a3c8-4fc0-9709-0aafdcfc87d6): no suitable block pools found to scan.  Waiting 681819544 ms.
2018-01-07 14:18:27,269 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1/7/18 4:43 PM with interval of 21600000ms
2018-01-07 14:18:27,271 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/var/lib/hadoop2/dfs/data/1, DS-db2b9f5a-2dbc-404d-8b87-9ff8e79ccdd2): no suitable block pools found to scan.  Waiting 681819457 ms.
2018-01-07 14:18:27,322 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-199680249-192.168.28.129-1514169681712 (Datanode Uuid 46e3cb45-0c76-4bc1-9342-af90beb9d892) service to hadoop-master/192.168.28.129:8020 beginning handshake with NN
2018-01-07 14:18:27,482 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-199680249-192.168.28.129-1514169681712 (Datanode Uuid 46e3cb45-0c76-4bc1-9342-af90beb9d892) service to hadoop-master/192.168.28.129:8020 successfully registered with NN
2018-01-07 14:18:27,482 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode hadoop-master/192.168.28.129:8020 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2018-01-07 14:18:27,842 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x8d6dfe8d9dcdd1d2,  containing 2 storage report(s), of which we sent 2. The reports had 25 total blocks and used 1 RPC(s). This took 15 msec to generate and 141 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-01-07 14:18:27,842 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-199680249-192.168.28.129-1514169681712
2018-01-07 14:21:13,750 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741899_1075 src: /192.168.28.130:59352 dest: /192.168.28.131:50010
2018-01-07 14:21:19,970 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.130:59352, dest: /192.168.28.131:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_353534513_12, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741899_1075, duration: 6017415546
2018-01-07 14:21:19,970 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741899_1075, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.28.132:50010] terminating
2018-01-07 14:21:19,994 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741900_1076 src: /192.168.28.130:59354 dest: /192.168.28.131:50010
2018-01-07 14:21:22,816 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.130:59354, dest: /192.168.28.131:50010, bytes: 75090821, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_353534513_12, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741900_1076, duration: 2813289287
2018-01-07 14:21:22,816 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741900_1076, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.28.132:50010] terminating
2018-01-07 14:21:23,128 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741901_1077 src: /192.168.28.132:55652 dest: /192.168.28.131:50010
2018-01-07 14:21:23,145 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.132:55652, dest: /192.168.28.131:50010, bytes: 66091, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_353534513_12, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741901_1077, duration: 11541145
2018-01-07 14:21:23,146 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741901_1077, type=LAST_IN_PIPELINE terminating
2018-01-07 15:57:13,635 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x8d6dfe8d9dcdd1d3,  containing 2 storage report(s), of which we sent 2. The reports had 28 total blocks and used 1 RPC(s). This took 1 msec to generate and 29 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-01-07 15:57:13,641 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-199680249-192.168.28.129-1514169681712
2018-01-07 16:13:24,104 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "hadoop-worker01.local/192.168.28.131"; destination host is: "hadoop-master":8020; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1487)
	at org.apache.hadoop.ipc.Client.call(Client.java:1429)
	at org.apache.hadoop.ipc.Client.call(Client.java:1339)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:154)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:459)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:581)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:775)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1788)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1157)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1053)
2018-01-07 16:13:28,085 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop-master/192.168.28.129:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-01-07 16:13:28,649 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2018-01-07 16:13:28,676 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at hadoop-worker01.local/192.168.28.131
************************************************************/
2018-01-07 21:10:29,706 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   user = hadoop
STARTUP_MSG:   host = hadoop-worker01.local/192.168.28.131
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.2
STARTUP_MSG:   classpath = /opt/hadoop-2.8.2/etc/hadoop:/opt/hadoop/share/hadoop/common/lib/activation-1.1.jar:/opt/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/opt/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/opt/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/opt/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/opt/hadoop/share/hadoop/common/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/opt/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/opt/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/opt/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/opt/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/opt/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-annotations-2.8.2.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-auth-2.8.2.jar:/opt/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/opt/hadoop/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/opt/hadoop/share/hadoop/common/lib/httpclient-4.5.2.jar:/opt/hadoop/share/hadoop/common/lib/httpcore-4.4.4.jar:/opt/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/common/lib/jcip-annotations-1.0.jar:/opt/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/opt/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/opt/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jsch-0.1.54.jar:/opt/hadoop/share/hadoop/common/lib/json-smart-1.1.1.jar:/opt/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/opt/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/common/lib/junit-4.11.jar:/opt/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/opt/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-3.9.jar:/opt/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/opt/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/opt/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/opt/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/opt/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/opt/hadoop/share/hadoop/common/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/opt/hadoop/share/hadoop/common/hadoop-common-2.8.2-tests.jar:/opt/hadoop/share/hadoop/common/hadoop-common-2.8.2.jar:/opt/hadoop/share/hadoop/common/hadoop-nfs-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs:/opt/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/okio-1.4.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/opt/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/opt/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-math-2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/opt/hadoop/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/opt/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/opt/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/opt/hadoop/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/opt/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/opt/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/opt/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/opt/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/opt/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/opt/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/opt/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/opt/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.2-tests.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.2.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 66c47f2a01ad9637879e95f80c41f798373828fb; compiled by 'jdu' on 2017-10-19T20:39Z
STARTUP_MSG:   java = 1.8.0_152
************************************************************/
2018-01-07 21:10:29,732 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2018-01-07 21:10:32,398 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2018-01-07 21:10:32,905 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2018-01-07 21:10:32,905 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2018-01-07 21:10:32,926 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2018-01-07 21:10:32,931 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is hadoop-worker01.local
2018-01-07 21:10:32,939 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2018-01-07 21:10:33,082 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2018-01-07 21:10:33,088 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 10485760 bytes/s
2018-01-07 21:10:33,089 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 50
2018-01-07 21:10:33,824 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2018-01-07 21:10:33,856 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2018-01-07 21:10:33,884 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2018-01-07 21:10:33,898 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2018-01-07 21:10:33,902 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2018-01-07 21:10:33,902 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2018-01-07 21:10:33,902 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2018-01-07 21:10:33,959 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 36559
2018-01-07 21:10:33,959 INFO org.mortbay.log: jetty-6.1.26
2018-01-07 21:10:34,817 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:36559
2018-01-07 21:10:35,608 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2018-01-07 21:10:35,675 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2018-01-07 21:10:36,606 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hadoop
2018-01-07 21:10:36,607 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2018-01-07 21:10:36,844 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2018-01-07 21:10:36,904 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2018-01-07 21:10:37,105 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2018-01-07 21:10:37,142 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2018-01-07 21:10:37,239 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2018-01-07 21:10:37,265 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to hadoop-master/192.168.28.129:8020 starting to offer service
2018-01-07 21:10:37,313 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2018-01-07 21:10:37,324 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2018-01-07 21:10:38,061 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to hadoop-master/192.168.28.129:8020
2018-01-07 21:10:38,080 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2018-01-07 21:10:38,145 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /var/lib/hadoop2/dfs/data/1/in_use.lock acquired by nodename 1684@hadoop-worker01.local
2018-01-07 21:10:38,183 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /var/lib/hadoop2/dfs/data/2/in_use.lock acquired by nodename 1684@hadoop-worker01.local
2018-01-07 21:10:38,542 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-199680249-192.168.28.129-1514169681712
2018-01-07 21:10:38,542 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712
2018-01-07 21:10:38,850 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-199680249-192.168.28.129-1514169681712
2018-01-07 21:10:38,851 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712
2018-01-07 21:10:38,860 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1794692519;bpid=BP-199680249-192.168.28.129-1514169681712;lv=-57;nsInfo=lv=-63;cid=CID-a84222ca-b260-441d-81c3-93c4f4f5d131;nsid=1794692519;c=1514169681712;bpid=BP-199680249-192.168.28.129-1514169681712;dnuuid=46e3cb45-0c76-4bc1-9342-af90beb9d892
2018-01-07 21:10:39,077 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-db2b9f5a-2dbc-404d-8b87-9ff8e79ccdd2
2018-01-07 21:10:39,077 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /var/lib/hadoop2/dfs/data/1/current, StorageType: DISK
2018-01-07 21:10:39,082 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-99e39a6f-a3c8-4fc0-9709-0aafdcfc87d6
2018-01-07 21:10:39,092 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /var/lib/hadoop2/dfs/data/2/current, StorageType: DISK
2018-01-07 21:10:39,121 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2018-01-07 21:10:39,132 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Volume reference is released.
2018-01-07 21:10:39,132 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-199680249-192.168.28.129-1514169681712
2018-01-07 21:10:39,136 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current...
2018-01-07 21:10:39,150 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current...
2018-01-07 21:10:39,289 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-199680249-192.168.28.129-1514169681712 on /var/lib/hadoop2/dfs/data/2/current: 139ms
2018-01-07 21:10:39,317 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-199680249-192.168.28.129-1514169681712 on /var/lib/hadoop2/dfs/data/1/current: 168ms
2018-01-07 21:10:39,318 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-199680249-192.168.28.129-1514169681712: 186ms
2018-01-07 21:10:39,328 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current...
2018-01-07 21:10:39,328 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/replicas doesn't exist 
2018-01-07 21:10:39,339 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current...
2018-01-07 21:10:39,340 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/replicas doesn't exist 
2018-01-07 21:10:39,346 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current: 17ms
2018-01-07 21:10:39,365 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current: 26ms
2018-01-07 21:10:39,365 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 38ms
2018-01-07 21:10:39,531 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/var/lib/hadoop2/dfs/data/2, DS-99e39a6f-a3c8-4fc0-9709-0aafdcfc87d6): no suitable block pools found to scan.  Waiting 657087197 ms.
2018-01-07 21:10:39,532 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/var/lib/hadoop2/dfs/data/1, DS-db2b9f5a-2dbc-404d-8b87-9ff8e79ccdd2): no suitable block pools found to scan.  Waiting 657087196 ms.
2018-01-07 21:10:39,700 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1/7/18 11:52 PM with interval of 21600000ms
2018-01-07 21:10:39,732 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-199680249-192.168.28.129-1514169681712 (Datanode Uuid 46e3cb45-0c76-4bc1-9342-af90beb9d892) service to hadoop-master/192.168.28.129:8020 beginning handshake with NN
2018-01-07 21:10:39,825 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-199680249-192.168.28.129-1514169681712 (Datanode Uuid 46e3cb45-0c76-4bc1-9342-af90beb9d892) service to hadoop-master/192.168.28.129:8020 successfully registered with NN
2018-01-07 21:10:39,825 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode hadoop-master/192.168.28.129:8020 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2018-01-07 21:10:40,177 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xf2f6ae2656608a8d,  containing 2 storage report(s), of which we sent 2. The reports had 28 total blocks and used 1 RPC(s). This took 10 msec to generate and 153 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-01-07 21:10:40,177 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-199680249-192.168.28.129-1514169681712
2018-01-07 23:35:18,501 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "hadoop-worker01.local/192.168.28.131"; destination host is: "hadoop-master":8020; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1487)
	at org.apache.hadoop.ipc.Client.call(Client.java:1429)
	at org.apache.hadoop.ipc.Client.call(Client.java:1339)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:154)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:459)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:581)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:775)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1788)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1157)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1053)
2018-01-07 23:35:21,972 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2018-01-07 23:35:21,988 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at hadoop-worker01.local/192.168.28.131
************************************************************/
2018-01-08 07:14:08,979 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   user = hadoop
STARTUP_MSG:   host = hadoop-worker01.local/192.168.28.131
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.2
STARTUP_MSG:   classpath = /opt/hadoop-2.8.2/etc/hadoop:/opt/hadoop/share/hadoop/common/lib/activation-1.1.jar:/opt/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/opt/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/opt/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/opt/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/opt/hadoop/share/hadoop/common/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/opt/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/opt/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/opt/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/opt/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/opt/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-annotations-2.8.2.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-auth-2.8.2.jar:/opt/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/opt/hadoop/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/opt/hadoop/share/hadoop/common/lib/httpclient-4.5.2.jar:/opt/hadoop/share/hadoop/common/lib/httpcore-4.4.4.jar:/opt/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/common/lib/jcip-annotations-1.0.jar:/opt/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/opt/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/opt/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jsch-0.1.54.jar:/opt/hadoop/share/hadoop/common/lib/json-smart-1.1.1.jar:/opt/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/opt/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/common/lib/junit-4.11.jar:/opt/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/opt/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-3.9.jar:/opt/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/opt/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/opt/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/opt/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/opt/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/opt/hadoop/share/hadoop/common/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/opt/hadoop/share/hadoop/common/hadoop-common-2.8.2-tests.jar:/opt/hadoop/share/hadoop/common/hadoop-common-2.8.2.jar:/opt/hadoop/share/hadoop/common/hadoop-nfs-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs:/opt/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/okio-1.4.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/opt/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/opt/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-math-2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/opt/hadoop/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/opt/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/opt/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/opt/hadoop/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/opt/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/opt/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/opt/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/opt/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/opt/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/opt/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/opt/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/opt/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.2-tests.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.2.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 66c47f2a01ad9637879e95f80c41f798373828fb; compiled by 'jdu' on 2017-10-19T20:39Z
STARTUP_MSG:   java = 1.8.0_152
************************************************************/
2018-01-08 07:14:08,997 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2018-01-08 07:14:11,272 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2018-01-08 07:14:11,537 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2018-01-08 07:14:11,537 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2018-01-08 07:14:11,550 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2018-01-08 07:14:11,559 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is hadoop-worker01.local
2018-01-08 07:14:11,569 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2018-01-08 07:14:11,642 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2018-01-08 07:14:11,646 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 10485760 bytes/s
2018-01-08 07:14:11,646 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 50
2018-01-08 07:14:12,266 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2018-01-08 07:14:12,301 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2018-01-08 07:14:12,342 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2018-01-08 07:14:12,358 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2018-01-08 07:14:12,362 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2018-01-08 07:14:12,362 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2018-01-08 07:14:12,362 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2018-01-08 07:14:12,435 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 39408
2018-01-08 07:14:12,435 INFO org.mortbay.log: jetty-6.1.26
2018-01-08 07:14:13,185 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:39408
2018-01-08 07:14:14,204 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2018-01-08 07:14:14,231 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2018-01-08 07:14:15,047 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hadoop
2018-01-08 07:14:15,047 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2018-01-08 07:14:15,274 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2018-01-08 07:14:15,320 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2018-01-08 07:14:15,571 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2018-01-08 07:14:15,605 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2018-01-08 07:14:15,672 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2018-01-08 07:14:15,705 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to hadoop-master/192.168.28.129:8020 starting to offer service
2018-01-08 07:14:15,760 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2018-01-08 07:14:15,783 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2018-01-08 07:14:16,723 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to hadoop-master/192.168.28.129:8020
2018-01-08 07:14:16,788 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2018-01-08 07:14:16,820 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /var/lib/hadoop2/dfs/data/1/in_use.lock acquired by nodename 1679@hadoop-worker01.local
2018-01-08 07:14:16,834 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /var/lib/hadoop2/dfs/data/2/in_use.lock acquired by nodename 1679@hadoop-worker01.local
2018-01-08 07:14:17,063 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-199680249-192.168.28.129-1514169681712
2018-01-08 07:14:17,065 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712
2018-01-08 07:14:17,768 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-199680249-192.168.28.129-1514169681712
2018-01-08 07:14:17,768 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712
2018-01-08 07:14:17,802 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1794692519;bpid=BP-199680249-192.168.28.129-1514169681712;lv=-57;nsInfo=lv=-63;cid=CID-a84222ca-b260-441d-81c3-93c4f4f5d131;nsid=1794692519;c=1514169681712;bpid=BP-199680249-192.168.28.129-1514169681712;dnuuid=46e3cb45-0c76-4bc1-9342-af90beb9d892
2018-01-08 07:14:17,961 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-db2b9f5a-2dbc-404d-8b87-9ff8e79ccdd2
2018-01-08 07:14:17,961 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /var/lib/hadoop2/dfs/data/1/current, StorageType: DISK
2018-01-08 07:14:17,966 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-99e39a6f-a3c8-4fc0-9709-0aafdcfc87d6
2018-01-08 07:14:17,966 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /var/lib/hadoop2/dfs/data/2/current, StorageType: DISK
2018-01-08 07:14:18,110 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2018-01-08 07:14:18,123 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Volume reference is released.
2018-01-08 07:14:18,123 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-199680249-192.168.28.129-1514169681712
2018-01-08 07:14:18,125 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current...
2018-01-08 07:14:18,137 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current...
2018-01-08 07:14:18,246 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-199680249-192.168.28.129-1514169681712 on /var/lib/hadoop2/dfs/data/2/current: 109ms
2018-01-08 07:14:18,260 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-199680249-192.168.28.129-1514169681712 on /var/lib/hadoop2/dfs/data/1/current: 131ms
2018-01-08 07:14:18,260 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-199680249-192.168.28.129-1514169681712: 137ms
2018-01-08 07:14:18,266 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current...
2018-01-08 07:14:18,267 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/replicas doesn't exist 
2018-01-08 07:14:18,305 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current...
2018-01-08 07:14:18,306 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/replicas doesn't exist 
2018-01-08 07:14:18,319 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current: 12ms
2018-01-08 07:14:18,321 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current: 55ms
2018-01-08 07:14:18,323 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 57ms
2018-01-08 07:14:18,542 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/var/lib/hadoop2/dfs/data/2, DS-99e39a6f-a3c8-4fc0-9709-0aafdcfc87d6): no suitable block pools found to scan.  Waiting 620868186 ms.
2018-01-08 07:14:18,544 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/var/lib/hadoop2/dfs/data/1, DS-db2b9f5a-2dbc-404d-8b87-9ff8e79ccdd2): no suitable block pools found to scan.  Waiting 620868184 ms.
2018-01-08 07:14:18,597 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1/8/18 7:20 AM with interval of 21600000ms
2018-01-08 07:14:18,712 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-199680249-192.168.28.129-1514169681712 (Datanode Uuid 46e3cb45-0c76-4bc1-9342-af90beb9d892) service to hadoop-master/192.168.28.129:8020 beginning handshake with NN
2018-01-08 07:14:18,869 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-199680249-192.168.28.129-1514169681712 (Datanode Uuid 46e3cb45-0c76-4bc1-9342-af90beb9d892) service to hadoop-master/192.168.28.129:8020 successfully registered with NN
2018-01-08 07:14:18,869 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode hadoop-master/192.168.28.129:8020 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2018-01-08 07:14:19,236 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x9a4e7f6090a9dbcf,  containing 2 storage report(s), of which we sent 2. The reports had 28 total blocks and used 1 RPC(s). This took 9 msec to generate and 163 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-01-08 07:14:19,236 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-199680249-192.168.28.129-1514169681712
2018-01-08 07:20:55,878 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-199680249-192.168.28.129-1514169681712 Total blocks: 28, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2018-01-08 07:41:27,621 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741902_1078 src: /192.168.28.130:52906 dest: /192.168.28.131:50010
2018-01-08 07:41:34,207 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.130:52906, dest: /192.168.28.131:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1583035122_12, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741902_1078, duration: 6388443093
2018-01-08 07:41:34,207 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741902_1078, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.28.132:50010] terminating
2018-01-08 07:41:34,239 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741903_1079 src: /192.168.28.130:52908 dest: /192.168.28.131:50010
2018-01-08 07:41:36,800 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.130:52908, dest: /192.168.28.131:50010, bytes: 75090821, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1583035122_12, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741903_1079, duration: 2543131774
2018-01-08 07:41:36,800 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741903_1079, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.28.132:50010] terminating
2018-01-08 07:41:37,077 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741904_1080 src: /192.168.28.130:52910 dest: /192.168.28.131:50010
2018-01-08 07:41:37,112 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.130:52910, dest: /192.168.28.131:50010, bytes: 66091, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1583035122_12, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741904_1080, duration: 19763765
2018-01-08 07:41:37,114 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741904_1080, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.28.132:50010] terminating
2018-01-08 07:45:27,408 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "hadoop-worker01.local/192.168.28.131"; destination host is: "hadoop-master":8020; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1487)
	at org.apache.hadoop.ipc.Client.call(Client.java:1429)
	at org.apache.hadoop.ipc.Client.call(Client.java:1339)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:154)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:459)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:581)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:775)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1788)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1157)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1053)
2018-01-08 07:45:31,405 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop-master/192.168.28.129:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-01-08 07:45:32,353 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2018-01-08 07:45:32,366 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at hadoop-worker01.local/192.168.28.131
************************************************************/
2018-01-08 10:13:18,497 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   user = hadoop
STARTUP_MSG:   host = hadoop-worker01.local/192.168.28.131
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.2
STARTUP_MSG:   classpath = /opt/hadoop-2.8.2/etc/hadoop:/opt/hadoop/share/hadoop/common/lib/activation-1.1.jar:/opt/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/opt/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/opt/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/opt/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/opt/hadoop/share/hadoop/common/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/opt/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/opt/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/opt/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/opt/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/opt/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-annotations-2.8.2.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-auth-2.8.2.jar:/opt/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/opt/hadoop/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/opt/hadoop/share/hadoop/common/lib/httpclient-4.5.2.jar:/opt/hadoop/share/hadoop/common/lib/httpcore-4.4.4.jar:/opt/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/common/lib/jcip-annotations-1.0.jar:/opt/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/opt/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/opt/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jsch-0.1.54.jar:/opt/hadoop/share/hadoop/common/lib/json-smart-1.1.1.jar:/opt/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/opt/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/common/lib/junit-4.11.jar:/opt/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/opt/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-3.9.jar:/opt/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/opt/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/opt/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/opt/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/opt/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/opt/hadoop/share/hadoop/common/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/opt/hadoop/share/hadoop/common/hadoop-common-2.8.2-tests.jar:/opt/hadoop/share/hadoop/common/hadoop-common-2.8.2.jar:/opt/hadoop/share/hadoop/common/hadoop-nfs-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs:/opt/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/okio-1.4.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/opt/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/opt/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-math-2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/opt/hadoop/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/opt/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/opt/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/opt/hadoop/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/opt/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/opt/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/opt/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/opt/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/opt/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/opt/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/opt/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/opt/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.2-tests.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.2.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 66c47f2a01ad9637879e95f80c41f798373828fb; compiled by 'jdu' on 2017-10-19T20:39Z
STARTUP_MSG:   java = 1.8.0_152
************************************************************/
2018-01-08 10:13:18,517 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2018-01-08 10:13:24,117 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2018-01-08 10:13:25,184 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2018-01-08 10:13:25,184 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2018-01-08 10:13:25,235 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2018-01-08 10:13:25,251 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is hadoop-worker01.local
2018-01-08 10:13:25,257 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2018-01-08 10:13:25,384 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2018-01-08 10:13:25,388 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 10485760 bytes/s
2018-01-08 10:13:25,388 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 50
2018-01-08 10:13:26,291 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2018-01-08 10:13:26,349 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2018-01-08 10:13:26,400 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2018-01-08 10:13:26,421 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2018-01-08 10:13:26,424 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2018-01-08 10:13:26,424 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2018-01-08 10:13:26,424 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2018-01-08 10:13:26,502 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 36941
2018-01-08 10:13:26,502 INFO org.mortbay.log: jetty-6.1.26
2018-01-08 10:13:27,427 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:36941
2018-01-08 10:13:28,197 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2018-01-08 10:13:28,218 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2018-01-08 10:13:30,488 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hadoop
2018-01-08 10:13:30,488 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2018-01-08 10:13:30,781 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2018-01-08 10:13:30,827 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2018-01-08 10:13:31,179 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2018-01-08 10:13:31,226 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2018-01-08 10:13:31,295 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2018-01-08 10:13:31,331 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to hadoop-master/192.168.28.129:8020 starting to offer service
2018-01-08 10:13:31,488 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2018-01-08 10:13:31,490 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2018-01-08 10:13:33,332 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to hadoop-master/192.168.28.129:8020
2018-01-08 10:13:33,466 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2018-01-08 10:13:33,612 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /var/lib/hadoop2/dfs/data/1/in_use.lock acquired by nodename 1755@hadoop-worker01.local
2018-01-08 10:13:34,190 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /var/lib/hadoop2/dfs/data/2/in_use.lock acquired by nodename 1755@hadoop-worker01.local
2018-01-08 10:13:34,637 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-199680249-192.168.28.129-1514169681712
2018-01-08 10:13:34,637 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712
2018-01-08 10:13:35,152 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-199680249-192.168.28.129-1514169681712
2018-01-08 10:13:35,152 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712
2018-01-08 10:13:35,175 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1794692519;bpid=BP-199680249-192.168.28.129-1514169681712;lv=-57;nsInfo=lv=-63;cid=CID-a84222ca-b260-441d-81c3-93c4f4f5d131;nsid=1794692519;c=1514169681712;bpid=BP-199680249-192.168.28.129-1514169681712;dnuuid=46e3cb45-0c76-4bc1-9342-af90beb9d892
2018-01-08 10:13:35,464 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-db2b9f5a-2dbc-404d-8b87-9ff8e79ccdd2
2018-01-08 10:13:35,464 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /var/lib/hadoop2/dfs/data/1/current, StorageType: DISK
2018-01-08 10:13:35,470 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-99e39a6f-a3c8-4fc0-9709-0aafdcfc87d6
2018-01-08 10:13:35,470 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /var/lib/hadoop2/dfs/data/2/current, StorageType: DISK
2018-01-08 10:13:35,545 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2018-01-08 10:13:35,703 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Volume reference is released.
2018-01-08 10:13:35,704 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-199680249-192.168.28.129-1514169681712
2018-01-08 10:13:35,714 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current...
2018-01-08 10:13:35,757 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current...
2018-01-08 10:13:36,208 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-199680249-192.168.28.129-1514169681712 on /var/lib/hadoop2/dfs/data/1/current: 480ms
2018-01-08 10:13:36,245 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-199680249-192.168.28.129-1514169681712 on /var/lib/hadoop2/dfs/data/2/current: 488ms
2018-01-08 10:13:36,250 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-199680249-192.168.28.129-1514169681712: 546ms
2018-01-08 10:13:36,311 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current...
2018-01-08 10:13:36,311 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/replicas doesn't exist 
2018-01-08 10:13:36,312 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current...
2018-01-08 10:13:36,312 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/replicas doesn't exist 
2018-01-08 10:13:36,443 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current: 132ms
2018-01-08 10:13:36,589 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current: 277ms
2018-01-08 10:13:36,616 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 335ms
2018-01-08 10:13:36,777 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/var/lib/hadoop2/dfs/data/2, DS-99e39a6f-a3c8-4fc0-9709-0aafdcfc87d6): no suitable block pools found to scan.  Waiting 610109951 ms.
2018-01-08 10:13:36,820 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/var/lib/hadoop2/dfs/data/1, DS-db2b9f5a-2dbc-404d-8b87-9ff8e79ccdd2): no suitable block pools found to scan.  Waiting 610109909 ms.
2018-01-08 10:13:36,866 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1/8/18 1:14 PM with interval of 21600000ms
2018-01-08 10:13:36,994 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-199680249-192.168.28.129-1514169681712 (Datanode Uuid 46e3cb45-0c76-4bc1-9342-af90beb9d892) service to hadoop-master/192.168.28.129:8020 beginning handshake with NN
2018-01-08 10:13:37,086 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-199680249-192.168.28.129-1514169681712 (Datanode Uuid 46e3cb45-0c76-4bc1-9342-af90beb9d892) service to hadoop-master/192.168.28.129:8020 successfully registered with NN
2018-01-08 10:13:37,087 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode hadoop-master/192.168.28.129:8020 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2018-01-08 10:13:37,382 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xc4135447b75beed0,  containing 2 storage report(s), of which we sent 2. The reports had 31 total blocks and used 1 RPC(s). This took 13 msec to generate and 70 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-01-08 10:13:37,382 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-199680249-192.168.28.129-1514169681712
2018-01-08 10:19:18,150 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741891_1067 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741891 for deletion
2018-01-08 10:19:18,152 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741891_1067 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741891
2018-01-08 10:40:19,910 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741905_1081 src: /192.168.28.132:48182 dest: /192.168.28.131:50010
2018-01-08 10:40:19,990 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.132:48182, dest: /192.168.28.131:50010, bytes: 63213, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1759612168_1, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741905_1081, duration: 55065377
2018-01-08 10:40:19,990 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741905_1081, type=LAST_IN_PIPELINE terminating
2018-01-08 10:41:02,346 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741905_1081 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741905 for deletion
2018-01-08 10:41:02,350 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741905_1081 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741905
2018-01-08 10:41:23,415 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741906_1082 src: /192.168.28.130:53326 dest: /192.168.28.131:50010
2018-01-08 10:41:23,458 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.130:53326, dest: /192.168.28.131:50010, bytes: 63213, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1759612168_1, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741906_1082, duration: 15366920
2018-01-08 10:41:23,458 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741906_1082, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.28.132:50010] terminating
2018-01-08 10:42:30,943 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741907_1083 src: /192.168.28.132:48184 dest: /192.168.28.131:50010
2018-01-08 10:42:36,103 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.132:48184, dest: /192.168.28.131:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-22806767_12, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741907_1083, duration: 5158344698
2018-01-08 10:42:36,104 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741907_1083, type=LAST_IN_PIPELINE terminating
2018-01-08 10:42:36,128 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741908_1084 src: /192.168.28.130:53512 dest: /192.168.28.131:50010
2018-01-08 10:42:39,143 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.130:53512, dest: /192.168.28.131:50010, bytes: 75090821, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-22806767_12, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741908_1084, duration: 3006910683
2018-01-08 10:42:39,144 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741908_1084, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.28.132:50010] terminating
2018-01-08 10:42:39,923 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741909_1085 src: /192.168.28.132:48186 dest: /192.168.28.131:50010
2018-01-08 10:42:39,940 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.132:48186, dest: /192.168.28.131:50010, bytes: 66091, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-22806767_12, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741909_1085, duration: 14118492
2018-01-08 10:42:39,942 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741909_1085, type=LAST_IN_PIPELINE terminating
2018-01-08 10:43:10,478 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1408ms
No GCs detected
2018-01-08 10:52:40,461 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741907_1083 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741907 for deletion
2018-01-08 10:52:40,465 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741908_1084 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741908 for deletion
2018-01-08 10:52:40,466 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741909_1085 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741909 for deletion
2018-01-08 10:52:40,470 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741908_1084 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741908
2018-01-08 10:52:40,470 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741907_1083 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741907
2018-01-08 10:52:40,471 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741909_1085 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741909
2018-01-08 10:58:07,088 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741910_1086 src: /192.168.28.130:53714 dest: /192.168.28.131:50010
2018-01-08 10:58:10,625 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.130:53714, dest: /192.168.28.131:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_11353863_12, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741910_1086, duration: 3529610098
2018-01-08 10:58:10,626 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741910_1086, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.28.132:50010] terminating
2018-01-08 10:58:10,646 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741911_1087 src: /192.168.28.132:48216 dest: /192.168.28.131:50010
2018-01-08 10:58:13,122 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.132:48216, dest: /192.168.28.131:50010, bytes: 75090821, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_11353863_12, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741911_1087, duration: 2472994658
2018-01-08 10:58:13,127 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741911_1087, type=LAST_IN_PIPELINE terminating
2018-01-08 10:58:13,423 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741912_1088 src: /192.168.28.132:48218 dest: /192.168.28.131:50010
2018-01-08 10:58:13,433 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.132:48218, dest: /192.168.28.131:50010, bytes: 66091, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_11353863_12, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741912_1088, duration: 5571345
2018-01-08 10:58:13,433 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741912_1088, type=LAST_IN_PIPELINE terminating
2018-01-08 11:12:25,202 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741910_1086 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741910 for deletion
2018-01-08 11:12:25,208 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741911_1087 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741911 for deletion
2018-01-08 11:12:25,209 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741912_1088 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741912 for deletion
2018-01-08 11:12:25,217 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741911_1087 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741911
2018-01-08 11:12:25,217 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741910_1086 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741910
2018-01-08 11:12:25,218 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741912_1088 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741912
2018-01-08 11:13:07,245 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "hadoop-worker01.local/192.168.28.131"; destination host is: "hadoop-master":8020; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1487)
	at org.apache.hadoop.ipc.Client.call(Client.java:1429)
	at org.apache.hadoop.ipc.Client.call(Client.java:1339)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:154)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:459)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:581)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:775)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1788)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1157)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1053)
2018-01-08 11:13:09,813 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2018-01-08 11:13:09,833 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at hadoop-worker01.local/192.168.28.131
************************************************************/
2018-01-11 11:42:47,437 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   user = hadoop
STARTUP_MSG:   host = hadoop-worker01.local/192.168.28.131
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.2
STARTUP_MSG:   classpath = /opt/hadoop-2.8.2/etc/hadoop:/opt/hadoop/share/hadoop/common/lib/activation-1.1.jar:/opt/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/opt/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/opt/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/opt/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/opt/hadoop/share/hadoop/common/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/opt/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/opt/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/opt/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/opt/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/opt/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-annotations-2.8.2.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-auth-2.8.2.jar:/opt/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/opt/hadoop/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/opt/hadoop/share/hadoop/common/lib/httpclient-4.5.2.jar:/opt/hadoop/share/hadoop/common/lib/httpcore-4.4.4.jar:/opt/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/common/lib/jcip-annotations-1.0.jar:/opt/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/opt/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/opt/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jsch-0.1.54.jar:/opt/hadoop/share/hadoop/common/lib/json-smart-1.1.1.jar:/opt/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/opt/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/common/lib/junit-4.11.jar:/opt/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/opt/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-3.9.jar:/opt/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/opt/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/opt/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/opt/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/opt/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/opt/hadoop/share/hadoop/common/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/opt/hadoop/share/hadoop/common/hadoop-common-2.8.2-tests.jar:/opt/hadoop/share/hadoop/common/hadoop-common-2.8.2.jar:/opt/hadoop/share/hadoop/common/hadoop-nfs-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs:/opt/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/okio-1.4.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/opt/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/opt/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-math-2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/opt/hadoop/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/opt/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/opt/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/opt/hadoop/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/opt/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/opt/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/opt/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/opt/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/opt/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/opt/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/opt/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/opt/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.2-tests.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.2.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 66c47f2a01ad9637879e95f80c41f798373828fb; compiled by 'jdu' on 2017-10-19T20:39Z
STARTUP_MSG:   java = 1.8.0_152
************************************************************/
2018-01-11 11:42:47,456 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2018-01-11 11:42:50,998 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2018-01-11 11:42:51,463 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2018-01-11 11:42:51,464 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2018-01-11 11:42:51,552 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2018-01-11 11:42:51,579 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is hadoop-worker01.local
2018-01-11 11:42:51,605 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2018-01-11 11:42:51,785 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2018-01-11 11:42:51,789 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 10485760 bytes/s
2018-01-11 11:42:51,789 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 50
2018-01-11 11:42:54,317 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2018-01-11 11:42:54,384 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2018-01-11 11:42:54,403 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2018-01-11 11:42:54,416 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2018-01-11 11:42:54,424 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2018-01-11 11:42:54,424 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2018-01-11 11:42:54,424 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2018-01-11 11:42:54,459 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 34015
2018-01-11 11:42:54,459 INFO org.mortbay.log: jetty-6.1.26
2018-01-11 11:42:55,391 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:34015
2018-01-11 11:42:56,733 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2018-01-11 11:42:56,858 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2018-01-11 11:42:58,117 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hadoop
2018-01-11 11:42:58,118 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2018-01-11 11:42:58,379 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2018-01-11 11:42:58,417 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2018-01-11 11:42:58,822 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2018-01-11 11:42:58,854 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2018-01-11 11:42:58,891 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2018-01-11 11:42:58,921 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to hadoop-master/192.168.28.129:8020 starting to offer service
2018-01-11 11:42:58,998 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2018-01-11 11:42:59,004 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2018-01-11 11:42:59,769 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to hadoop-master/192.168.28.129:8020
2018-01-11 11:42:59,806 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2018-01-11 11:42:59,834 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /var/lib/hadoop2/dfs/data/1/in_use.lock acquired by nodename 1780@hadoop-worker01.local
2018-01-11 11:42:59,921 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /var/lib/hadoop2/dfs/data/2/in_use.lock acquired by nodename 1780@hadoop-worker01.local
2018-01-11 11:43:00,263 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-199680249-192.168.28.129-1514169681712
2018-01-11 11:43:00,264 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712
2018-01-11 11:43:00,989 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-199680249-192.168.28.129-1514169681712
2018-01-11 11:43:00,989 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712
2018-01-11 11:43:00,999 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1794692519;bpid=BP-199680249-192.168.28.129-1514169681712;lv=-57;nsInfo=lv=-63;cid=CID-a84222ca-b260-441d-81c3-93c4f4f5d131;nsid=1794692519;c=1514169681712;bpid=BP-199680249-192.168.28.129-1514169681712;dnuuid=46e3cb45-0c76-4bc1-9342-af90beb9d892
2018-01-11 11:43:01,409 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-db2b9f5a-2dbc-404d-8b87-9ff8e79ccdd2
2018-01-11 11:43:01,410 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /var/lib/hadoop2/dfs/data/1/current, StorageType: DISK
2018-01-11 11:43:01,419 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-99e39a6f-a3c8-4fc0-9709-0aafdcfc87d6
2018-01-11 11:43:01,419 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /var/lib/hadoop2/dfs/data/2/current, StorageType: DISK
2018-01-11 11:43:01,447 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2018-01-11 11:43:01,466 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Volume reference is released.
2018-01-11 11:43:01,472 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-199680249-192.168.28.129-1514169681712
2018-01-11 11:43:01,479 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current...
2018-01-11 11:43:01,480 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current...
2018-01-11 11:43:01,846 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-199680249-192.168.28.129-1514169681712 on /var/lib/hadoop2/dfs/data/2/current: 351ms
2018-01-11 11:43:01,888 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-199680249-192.168.28.129-1514169681712 on /var/lib/hadoop2/dfs/data/1/current: 395ms
2018-01-11 11:43:01,893 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-199680249-192.168.28.129-1514169681712: 421ms
2018-01-11 11:43:01,931 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current...
2018-01-11 11:43:01,931 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/replicas doesn't exist 
2018-01-11 11:43:01,939 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current...
2018-01-11 11:43:01,939 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/replicas doesn't exist 
2018-01-11 11:43:02,179 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current: 239ms
2018-01-11 11:43:02,299 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current: 368ms
2018-01-11 11:43:02,314 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 411ms
2018-01-11 11:43:02,409 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/var/lib/hadoop2/dfs/data/2, DS-99e39a6f-a3c8-4fc0-9709-0aafdcfc87d6): no suitable block pools found to scan.  Waiting 345544319 ms.
2018-01-11 11:43:02,523 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1/11/18 4:08 PM with interval of 21600000ms
2018-01-11 11:43:02,514 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/var/lib/hadoop2/dfs/data/1, DS-db2b9f5a-2dbc-404d-8b87-9ff8e79ccdd2): no suitable block pools found to scan.  Waiting 345544215 ms.
2018-01-11 11:43:02,601 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-199680249-192.168.28.129-1514169681712 (Datanode Uuid 46e3cb45-0c76-4bc1-9342-af90beb9d892) service to hadoop-master/192.168.28.129:8020 beginning handshake with NN
2018-01-11 11:43:02,974 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-199680249-192.168.28.129-1514169681712 (Datanode Uuid 46e3cb45-0c76-4bc1-9342-af90beb9d892) service to hadoop-master/192.168.28.129:8020 successfully registered with NN
2018-01-11 11:43:02,974 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode hadoop-master/192.168.28.129:8020 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2018-01-11 11:43:03,649 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x76694cafb0c2d9ec,  containing 2 storage report(s), of which we sent 2. The reports had 31 total blocks and used 1 RPC(s). This took 11 msec to generate and 124 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-01-11 11:43:03,649 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-199680249-192.168.28.129-1514169681712
2018-01-11 11:44:15,064 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "hadoop-worker01.local/192.168.28.131"; destination host is: "hadoop-master":8020; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1487)
	at org.apache.hadoop.ipc.Client.call(Client.java:1429)
	at org.apache.hadoop.ipc.Client.call(Client.java:1339)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:154)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:459)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:581)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:775)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1788)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1157)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1053)
2018-01-11 11:44:19,065 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop-master/192.168.28.129:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-01-11 11:44:20,067 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop-master/192.168.28.129:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-01-11 11:44:20,346 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2018-01-11 11:44:20,351 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at hadoop-worker01.local/192.168.28.131
************************************************************/
2018-01-11 11:50:18,247 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   user = hadoop
STARTUP_MSG:   host = hadoop-worker01.local/192.168.28.131
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.2
STARTUP_MSG:   classpath = /opt/hadoop-2.8.2/etc/hadoop:/opt/hadoop/share/hadoop/common/lib/activation-1.1.jar:/opt/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/opt/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/opt/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/opt/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/opt/hadoop/share/hadoop/common/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/opt/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/opt/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/opt/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/opt/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/opt/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-annotations-2.8.2.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-auth-2.8.2.jar:/opt/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/opt/hadoop/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/opt/hadoop/share/hadoop/common/lib/httpclient-4.5.2.jar:/opt/hadoop/share/hadoop/common/lib/httpcore-4.4.4.jar:/opt/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/common/lib/jcip-annotations-1.0.jar:/opt/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/opt/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/opt/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jsch-0.1.54.jar:/opt/hadoop/share/hadoop/common/lib/json-smart-1.1.1.jar:/opt/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/opt/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/common/lib/junit-4.11.jar:/opt/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/opt/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-3.9.jar:/opt/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/opt/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/opt/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/opt/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/opt/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/opt/hadoop/share/hadoop/common/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/opt/hadoop/share/hadoop/common/hadoop-common-2.8.2-tests.jar:/opt/hadoop/share/hadoop/common/hadoop-common-2.8.2.jar:/opt/hadoop/share/hadoop/common/hadoop-nfs-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs:/opt/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/okio-1.4.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/opt/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/opt/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-math-2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/opt/hadoop/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/opt/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/opt/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/opt/hadoop/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/opt/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/opt/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/opt/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/opt/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/opt/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/opt/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/opt/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/opt/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.2-tests.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.2.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 66c47f2a01ad9637879e95f80c41f798373828fb; compiled by 'jdu' on 2017-10-19T20:39Z
STARTUP_MSG:   java = 1.8.0_152
************************************************************/
2018-01-11 11:50:18,258 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2018-01-11 11:50:20,953 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2018-01-11 11:50:21,389 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2018-01-11 11:50:21,389 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2018-01-11 11:50:21,400 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2018-01-11 11:50:21,402 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is hadoop-worker01.local
2018-01-11 11:50:21,493 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2018-01-11 11:50:21,598 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2018-01-11 11:50:21,602 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 10485760 bytes/s
2018-01-11 11:50:21,602 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 50
2018-01-11 11:50:22,579 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2018-01-11 11:50:22,639 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2018-01-11 11:50:22,666 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2018-01-11 11:50:22,675 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2018-01-11 11:50:22,679 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2018-01-11 11:50:22,679 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2018-01-11 11:50:22,679 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2018-01-11 11:50:22,710 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 42492
2018-01-11 11:50:22,710 INFO org.mortbay.log: jetty-6.1.26
2018-01-11 11:50:23,546 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:42492
2018-01-11 11:50:25,119 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2018-01-11 11:50:25,140 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2018-01-11 11:50:26,031 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hadoop
2018-01-11 11:50:26,032 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2018-01-11 11:50:26,530 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2018-01-11 11:50:26,567 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2018-01-11 11:50:26,879 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2018-01-11 11:50:26,904 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2018-01-11 11:50:26,966 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2018-01-11 11:50:27,053 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to hadoop-master/192.168.28.129:8020 starting to offer service
2018-01-11 11:50:27,162 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2018-01-11 11:50:27,177 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2018-01-11 11:50:28,772 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to hadoop-master/192.168.28.129:8020
2018-01-11 11:50:28,901 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2018-01-11 11:50:29,074 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /var/lib/hadoop2/dfs/data/1/in_use.lock acquired by nodename 2441@hadoop-worker01.local
2018-01-11 11:50:29,134 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /var/lib/hadoop2/dfs/data/2/in_use.lock acquired by nodename 2441@hadoop-worker01.local
2018-01-11 11:50:29,820 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-199680249-192.168.28.129-1514169681712
2018-01-11 11:50:29,821 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712
2018-01-11 11:50:30,280 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-199680249-192.168.28.129-1514169681712
2018-01-11 11:50:30,280 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712
2018-01-11 11:50:30,288 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1794692519;bpid=BP-199680249-192.168.28.129-1514169681712;lv=-57;nsInfo=lv=-63;cid=CID-a84222ca-b260-441d-81c3-93c4f4f5d131;nsid=1794692519;c=1514169681712;bpid=BP-199680249-192.168.28.129-1514169681712;dnuuid=46e3cb45-0c76-4bc1-9342-af90beb9d892
2018-01-11 11:50:30,445 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-db2b9f5a-2dbc-404d-8b87-9ff8e79ccdd2
2018-01-11 11:50:30,445 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /var/lib/hadoop2/dfs/data/1/current, StorageType: DISK
2018-01-11 11:50:30,449 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-99e39a6f-a3c8-4fc0-9709-0aafdcfc87d6
2018-01-11 11:50:30,449 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /var/lib/hadoop2/dfs/data/2/current, StorageType: DISK
2018-01-11 11:50:30,612 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2018-01-11 11:50:30,653 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Volume reference is released.
2018-01-11 11:50:30,653 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-199680249-192.168.28.129-1514169681712
2018-01-11 11:50:30,687 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current...
2018-01-11 11:50:30,706 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current...
2018-01-11 11:50:30,760 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current: 617148416
2018-01-11 11:50:30,762 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current: 437198848
2018-01-11 11:50:30,777 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-199680249-192.168.28.129-1514169681712 on /var/lib/hadoop2/dfs/data/1/current: 71ms
2018-01-11 11:50:30,800 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-199680249-192.168.28.129-1514169681712 on /var/lib/hadoop2/dfs/data/2/current: 93ms
2018-01-11 11:50:30,802 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-199680249-192.168.28.129-1514169681712: 150ms
2018-01-11 11:50:30,807 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current...
2018-01-11 11:50:30,808 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/replicas doesn't exist 
2018-01-11 11:50:30,810 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current...
2018-01-11 11:50:30,810 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/replicas doesn't exist 
2018-01-11 11:50:30,822 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current: 12ms
2018-01-11 11:50:30,824 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current: 17ms
2018-01-11 11:50:30,825 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 18ms
2018-01-11 11:50:30,988 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/var/lib/hadoop2/dfs/data/1, DS-db2b9f5a-2dbc-404d-8b87-9ff8e79ccdd2): no suitable block pools found to scan.  Waiting 345095740 ms.
2018-01-11 11:50:31,008 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1/11/18 2:03 PM with interval of 21600000ms
2018-01-11 11:50:30,988 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/var/lib/hadoop2/dfs/data/2, DS-99e39a6f-a3c8-4fc0-9709-0aafdcfc87d6): no suitable block pools found to scan.  Waiting 345095740 ms.
2018-01-11 11:50:31,034 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-199680249-192.168.28.129-1514169681712 (Datanode Uuid 46e3cb45-0c76-4bc1-9342-af90beb9d892) service to hadoop-master/192.168.28.129:8020 beginning handshake with NN
2018-01-11 11:50:31,090 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-199680249-192.168.28.129-1514169681712 (Datanode Uuid 46e3cb45-0c76-4bc1-9342-af90beb9d892) service to hadoop-master/192.168.28.129:8020 successfully registered with NN
2018-01-11 11:50:31,091 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode hadoop-master/192.168.28.129:8020 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2018-01-11 11:50:31,274 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x1fee7322cd0c633c,  containing 2 storage report(s), of which we sent 2. The reports had 31 total blocks and used 1 RPC(s). This took 9 msec to generate and 51 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-01-11 11:50:31,274 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-199680249-192.168.28.129-1514169681712
2018-01-11 11:51:01,180 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.28.131:50010, datanodeUuid=46e3cb45-0c76-4bc1-9342-af90beb9d892, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-57;cid=CID-a84222ca-b260-441d-81c3-93c4f4f5d131;nsid=1794692519;c=1514169681712) Starting thread to transfer BP-199680249-192.168.28.129-1514169681712:blk_1073741895_1071 to 192.168.28.134:50010 
2018-01-11 11:51:01,184 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.28.131:50010, datanodeUuid=46e3cb45-0c76-4bc1-9342-af90beb9d892, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-57;cid=CID-a84222ca-b260-441d-81c3-93c4f4f5d131;nsid=1794692519;c=1514169681712) Starting thread to transfer BP-199680249-192.168.28.129-1514169681712:blk_1073741897_1073 to 192.168.28.134:50010 
2018-01-11 11:51:01,377 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer, at hadoop-worker01.local:50010: Transmitted BP-199680249-192.168.28.129-1514169681712:blk_1073741895_1071 (numBytes=399) to /192.168.28.134:50010
2018-01-11 11:51:02,909 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer, at hadoop-worker01.local:50010: Transmitted BP-199680249-192.168.28.129-1514169681712:blk_1073741897_1073 (numBytes=75090821) to /192.168.28.134:50010
2018-01-11 11:51:04,147 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.28.131:50010, datanodeUuid=46e3cb45-0c76-4bc1-9342-af90beb9d892, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-57;cid=CID-a84222ca-b260-441d-81c3-93c4f4f5d131;nsid=1794692519;c=1514169681712) Starting thread to transfer BP-199680249-192.168.28.129-1514169681712:blk_1073741901_1077 to 192.168.28.134:50010 
2018-01-11 11:51:04,151 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.28.131:50010, datanodeUuid=46e3cb45-0c76-4bc1-9342-af90beb9d892, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-57;cid=CID-a84222ca-b260-441d-81c3-93c4f4f5d131;nsid=1794692519;c=1514169681712) Starting thread to transfer BP-199680249-192.168.28.129-1514169681712:blk_1073741839_1015 to 192.168.28.134:50010 
2018-01-11 11:51:04,158 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer, at hadoop-worker01.local:50010: Transmitted BP-199680249-192.168.28.129-1514169681712:blk_1073741839_1015 (numBytes=260) to /192.168.28.134:50010
2018-01-11 11:51:04,162 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer, at hadoop-worker01.local:50010: Transmitted BP-199680249-192.168.28.129-1514169681712:blk_1073741901_1077 (numBytes=66091) to /192.168.28.134:50010
2018-01-11 11:51:07,198 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.28.131:50010, datanodeUuid=46e3cb45-0c76-4bc1-9342-af90beb9d892, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-57;cid=CID-a84222ca-b260-441d-81c3-93c4f4f5d131;nsid=1794692519;c=1514169681712) Starting thread to transfer BP-199680249-192.168.28.129-1514169681712:blk_1073741843_1019 to 192.168.28.134:50010 
2018-01-11 11:51:07,203 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.28.131:50010, datanodeUuid=46e3cb45-0c76-4bc1-9342-af90beb9d892, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-57;cid=CID-a84222ca-b260-441d-81c3-93c4f4f5d131;nsid=1794692519;c=1514169681712) Starting thread to transfer BP-199680249-192.168.28.129-1514169681712:blk_1073741844_1020 to 192.168.28.134:50010 
2018-01-11 11:51:07,225 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer, at hadoop-worker01.local:50010: Transmitted BP-199680249-192.168.28.129-1514169681712:blk_1073741843_1019 (numBytes=271) to /192.168.28.134:50010
2018-01-11 11:51:07,225 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer, at hadoop-worker01.local:50010: Transmitted BP-199680249-192.168.28.129-1514169681712:blk_1073741844_1020 (numBytes=30) to /192.168.28.134:50010
2018-01-11 11:51:10,158 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.28.131:50010, datanodeUuid=46e3cb45-0c76-4bc1-9342-af90beb9d892, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-57;cid=CID-a84222ca-b260-441d-81c3-93c4f4f5d131;nsid=1794692519;c=1514169681712) Starting thread to transfer BP-199680249-192.168.28.129-1514169681712:blk_1073741832_1008 to 192.168.28.134:50010 
2018-01-11 11:51:10,159 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.28.131:50010, datanodeUuid=46e3cb45-0c76-4bc1-9342-af90beb9d892, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-57;cid=CID-a84222ca-b260-441d-81c3-93c4f4f5d131;nsid=1794692519;c=1514169681712) Starting thread to transfer BP-199680249-192.168.28.129-1514169681712:blk_1073741834_1010 to 192.168.28.134:50010 
2018-01-11 11:51:10,171 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer, at hadoop-worker01.local:50010: Transmitted BP-199680249-192.168.28.129-1514169681712:blk_1073741832_1008 (numBytes=1330) to /192.168.28.134:50010
2018-01-11 11:51:10,171 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer, at hadoop-worker01.local:50010: Transmitted BP-199680249-192.168.28.129-1514169681712:blk_1073741834_1010 (numBytes=1338) to /192.168.28.134:50010
2018-01-11 11:51:13,161 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.28.131:50010, datanodeUuid=46e3cb45-0c76-4bc1-9342-af90beb9d892, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-57;cid=CID-a84222ca-b260-441d-81c3-93c4f4f5d131;nsid=1794692519;c=1514169681712) Starting thread to transfer BP-199680249-192.168.28.129-1514169681712:blk_1073741838_1014 to 192.168.28.134:50010 
2018-01-11 11:51:13,163 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.28.131:50010, datanodeUuid=46e3cb45-0c76-4bc1-9342-af90beb9d892, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-57;cid=CID-a84222ca-b260-441d-81c3-93c4f4f5d131;nsid=1794692519;c=1514169681712) Starting thread to transfer BP-199680249-192.168.28.129-1514169681712:blk_1073741840_1016 to 192.168.28.134:50010 
2018-01-11 11:51:13,174 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer, at hadoop-worker01.local:50010: Transmitted BP-199680249-192.168.28.129-1514169681712:blk_1073741840_1016 (numBytes=271) to /192.168.28.134:50010
2018-01-11 11:51:13,175 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer, at hadoop-worker01.local:50010: Transmitted BP-199680249-192.168.28.129-1514169681712:blk_1073741838_1014 (numBytes=260) to /192.168.28.134:50010
2018-01-11 11:51:23,771 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.28.131:50010, datanodeUuid=46e3cb45-0c76-4bc1-9342-af90beb9d892, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-57;cid=CID-a84222ca-b260-441d-81c3-93c4f4f5d131;nsid=1794692519;c=1514169681712) Starting thread to transfer BP-199680249-192.168.28.129-1514169681712:blk_1073741835_1011 to 192.168.28.134:50010 
2018-01-11 11:51:23,772 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.28.131:50010, datanodeUuid=46e3cb45-0c76-4bc1-9342-af90beb9d892, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-57;cid=CID-a84222ca-b260-441d-81c3-93c4f4f5d131;nsid=1794692519;c=1514169681712) Starting thread to transfer BP-199680249-192.168.28.129-1514169681712:blk_1073741841_1017 to 192.168.28.134:50010 
2018-01-11 11:51:23,785 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer, at hadoop-worker01.local:50010: Transmitted BP-199680249-192.168.28.129-1514169681712:blk_1073741835_1011 (numBytes=1338) to /192.168.28.134:50010
2018-01-11 11:51:23,785 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer, at hadoop-worker01.local:50010: Transmitted BP-199680249-192.168.28.129-1514169681712:blk_1073741841_1017 (numBytes=1362) to /192.168.28.134:50010
2018-01-11 11:51:26,773 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.28.131:50010, datanodeUuid=46e3cb45-0c76-4bc1-9342-af90beb9d892, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-57;cid=CID-a84222ca-b260-441d-81c3-93c4f4f5d131;nsid=1794692519;c=1514169681712) Starting thread to transfer BP-199680249-192.168.28.129-1514169681712:blk_1073741836_1012 to 192.168.28.134:50010 
2018-01-11 11:51:26,774 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.28.131:50010, datanodeUuid=46e3cb45-0c76-4bc1-9342-af90beb9d892, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-57;cid=CID-a84222ca-b260-441d-81c3-93c4f4f5d131;nsid=1794692519;c=1514169681712) Starting thread to transfer BP-199680249-192.168.28.129-1514169681712:blk_1073741842_1018 to 192.168.28.134:50010 
2018-01-11 11:51:26,781 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer, at hadoop-worker01.local:50010: Transmitted BP-199680249-192.168.28.129-1514169681712:blk_1073741836_1012 (numBytes=1338) to /192.168.28.134:50010
2018-01-11 11:51:26,781 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer, at hadoop-worker01.local:50010: Transmitted BP-199680249-192.168.28.129-1514169681712:blk_1073741842_1018 (numBytes=271) to /192.168.28.134:50010
2018-01-11 11:51:32,775 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(192.168.28.131:50010, datanodeUuid=46e3cb45-0c76-4bc1-9342-af90beb9d892, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-57;cid=CID-a84222ca-b260-441d-81c3-93c4f4f5d131;nsid=1794692519;c=1514169681712) Starting thread to transfer BP-199680249-192.168.28.129-1514169681712:blk_1073741881_1057 to 192.168.28.134:50010 
2018-01-11 11:51:34,019 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer, at hadoop-worker01.local:50010: Transmitted BP-199680249-192.168.28.129-1514169681712:blk_1073741881_1057 (numBytes=134217728) to /192.168.28.134:50010
2018-01-11 12:43:19,852 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "hadoop-worker01.local/192.168.28.131"; destination host is: "hadoop-master":8020; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1487)
	at org.apache.hadoop.ipc.Client.call(Client.java:1429)
	at org.apache.hadoop.ipc.Client.call(Client.java:1339)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:154)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:459)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:581)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:775)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1788)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1157)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1053)
2018-01-11 12:43:22,856 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2018-01-11 12:43:22,866 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at hadoop-worker01.local/192.168.28.131
************************************************************/
2018-01-11 16:22:33,040 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   user = hadoop
STARTUP_MSG:   host = hadoop-worker01.local/192.168.28.131
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.2
STARTUP_MSG:   classpath = /opt/hadoop-2.8.2/etc/hadoop:/opt/hadoop/share/hadoop/common/lib/activation-1.1.jar:/opt/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/opt/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/opt/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/opt/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/opt/hadoop/share/hadoop/common/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/opt/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/opt/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/opt/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/opt/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/opt/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-annotations-2.8.2.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-auth-2.8.2.jar:/opt/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/opt/hadoop/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/opt/hadoop/share/hadoop/common/lib/httpclient-4.5.2.jar:/opt/hadoop/share/hadoop/common/lib/httpcore-4.4.4.jar:/opt/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/common/lib/jcip-annotations-1.0.jar:/opt/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/opt/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/opt/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jsch-0.1.54.jar:/opt/hadoop/share/hadoop/common/lib/json-smart-1.1.1.jar:/opt/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/opt/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/common/lib/junit-4.11.jar:/opt/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/opt/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-3.9.jar:/opt/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/opt/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/opt/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/opt/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/opt/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/opt/hadoop/share/hadoop/common/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/opt/hadoop/share/hadoop/common/hadoop-common-2.8.2-tests.jar:/opt/hadoop/share/hadoop/common/hadoop-common-2.8.2.jar:/opt/hadoop/share/hadoop/common/hadoop-nfs-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs:/opt/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/okio-1.4.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/opt/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/opt/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-math-2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/opt/hadoop/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/opt/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/opt/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/opt/hadoop/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/opt/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/opt/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/opt/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/opt/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/opt/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/opt/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/opt/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/opt/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.2-tests.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.2.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 66c47f2a01ad9637879e95f80c41f798373828fb; compiled by 'jdu' on 2017-10-19T20:39Z
STARTUP_MSG:   java = 1.8.0_152
************************************************************/
2018-01-11 16:22:33,071 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2018-01-11 16:22:37,449 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2018-01-11 16:22:38,231 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2018-01-11 16:22:38,231 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2018-01-11 16:22:38,314 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2018-01-11 16:22:38,318 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is hadoop-worker01.local
2018-01-11 16:22:38,327 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2018-01-11 16:22:38,508 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2018-01-11 16:22:38,540 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 10485760 bytes/s
2018-01-11 16:22:38,540 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 50
2018-01-11 16:22:39,562 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2018-01-11 16:22:39,615 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2018-01-11 16:22:39,647 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2018-01-11 16:22:39,664 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2018-01-11 16:22:39,668 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2018-01-11 16:22:39,668 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2018-01-11 16:22:39,669 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2018-01-11 16:22:39,718 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 38807
2018-01-11 16:22:39,718 INFO org.mortbay.log: jetty-6.1.26
2018-01-11 16:22:40,642 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:38807
2018-01-11 16:22:41,806 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2018-01-11 16:22:42,031 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2018-01-11 16:22:43,220 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hadoop
2018-01-11 16:22:43,220 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2018-01-11 16:22:43,414 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2018-01-11 16:22:43,489 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2018-01-11 16:22:43,700 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2018-01-11 16:22:43,717 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2018-01-11 16:22:43,814 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2018-01-11 16:22:43,843 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to hadoop-master/192.168.28.129:8020 starting to offer service
2018-01-11 16:22:43,966 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2018-01-11 16:22:43,887 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2018-01-11 16:22:44,689 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to hadoop-master/192.168.28.129:8020
2018-01-11 16:22:44,757 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2018-01-11 16:22:44,893 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /var/lib/hadoop2/dfs/data/1/in_use.lock acquired by nodename 1709@hadoop-worker01.local
2018-01-11 16:22:45,007 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /var/lib/hadoop2/dfs/data/2/in_use.lock acquired by nodename 1709@hadoop-worker01.local
2018-01-11 16:22:45,302 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-199680249-192.168.28.129-1514169681712
2018-01-11 16:22:45,303 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712
2018-01-11 16:22:45,974 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-199680249-192.168.28.129-1514169681712
2018-01-11 16:22:45,975 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712
2018-01-11 16:22:45,987 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1794692519;bpid=BP-199680249-192.168.28.129-1514169681712;lv=-57;nsInfo=lv=-63;cid=CID-a84222ca-b260-441d-81c3-93c4f4f5d131;nsid=1794692519;c=1514169681712;bpid=BP-199680249-192.168.28.129-1514169681712;dnuuid=46e3cb45-0c76-4bc1-9342-af90beb9d892
2018-01-11 16:22:46,362 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-db2b9f5a-2dbc-404d-8b87-9ff8e79ccdd2
2018-01-11 16:22:46,363 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /var/lib/hadoop2/dfs/data/1/current, StorageType: DISK
2018-01-11 16:22:46,375 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-99e39a6f-a3c8-4fc0-9709-0aafdcfc87d6
2018-01-11 16:22:46,375 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /var/lib/hadoop2/dfs/data/2/current, StorageType: DISK
2018-01-11 16:22:46,566 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2018-01-11 16:22:47,763 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Volume reference is released.
2018-01-11 16:22:47,763 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-199680249-192.168.28.129-1514169681712
2018-01-11 16:22:47,781 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current...
2018-01-11 16:22:47,805 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current...
2018-01-11 16:22:48,487 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-199680249-192.168.28.129-1514169681712 on /var/lib/hadoop2/dfs/data/1/current: 684ms
2018-01-11 16:22:48,583 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-199680249-192.168.28.129-1514169681712 on /var/lib/hadoop2/dfs/data/2/current: 778ms
2018-01-11 16:22:48,584 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-199680249-192.168.28.129-1514169681712: 820ms
2018-01-11 16:22:48,596 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current...
2018-01-11 16:22:48,597 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/replicas doesn't exist 
2018-01-11 16:22:48,596 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current...
2018-01-11 16:22:48,597 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/replicas doesn't exist 
2018-01-11 16:22:48,620 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current: 23ms
2018-01-11 16:22:48,622 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current: 25ms
2018-01-11 16:22:48,623 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 34ms
2018-01-11 16:22:48,800 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/var/lib/hadoop2/dfs/data/2, DS-99e39a6f-a3c8-4fc0-9709-0aafdcfc87d6): no suitable block pools found to scan.  Waiting 328757928 ms.
2018-01-11 16:22:48,972 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/var/lib/hadoop2/dfs/data/1, DS-db2b9f5a-2dbc-404d-8b87-9ff8e79ccdd2): no suitable block pools found to scan.  Waiting 328757756 ms.
2018-01-11 16:22:49,060 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1/11/18 4:40 PM with interval of 21600000ms
2018-01-11 16:22:49,081 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-199680249-192.168.28.129-1514169681712 (Datanode Uuid 46e3cb45-0c76-4bc1-9342-af90beb9d892) service to hadoop-master/192.168.28.129:8020 beginning handshake with NN
2018-01-11 16:22:49,188 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-199680249-192.168.28.129-1514169681712 (Datanode Uuid 46e3cb45-0c76-4bc1-9342-af90beb9d892) service to hadoop-master/192.168.28.129:8020 successfully registered with NN
2018-01-11 16:22:49,188 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode hadoop-master/192.168.28.129:8020 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2018-01-11 16:22:49,614 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xd126fd44c3afebbb,  containing 2 storage report(s), of which we sent 2. The reports had 31 total blocks and used 1 RPC(s). This took 10 msec to generate and 158 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-01-11 16:22:49,615 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-199680249-192.168.28.129-1514169681712
2018-01-11 16:28:03,188 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "hadoop-worker01.local/192.168.28.131"; destination host is: "hadoop-master":8020; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1487)
	at org.apache.hadoop.ipc.Client.call(Client.java:1429)
	at org.apache.hadoop.ipc.Client.call(Client.java:1339)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:154)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:459)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:581)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:775)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1788)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1157)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1053)
2018-01-11 16:28:07,182 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop-master/192.168.28.129:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-01-11 16:28:07,275 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2018-01-11 16:28:07,284 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at hadoop-worker01.local/192.168.28.131
************************************************************/
2018-01-11 17:22:04,987 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   user = hadoop
STARTUP_MSG:   host = hadoop-worker01.local/192.168.28.131
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.2
STARTUP_MSG:   classpath = /opt/hadoop-2.8.2/etc/hadoop:/opt/hadoop/share/hadoop/common/lib/activation-1.1.jar:/opt/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/opt/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/opt/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/opt/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/opt/hadoop/share/hadoop/common/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/opt/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/opt/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/opt/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/opt/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/opt/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-annotations-2.8.2.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-auth-2.8.2.jar:/opt/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/opt/hadoop/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/opt/hadoop/share/hadoop/common/lib/httpclient-4.5.2.jar:/opt/hadoop/share/hadoop/common/lib/httpcore-4.4.4.jar:/opt/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/common/lib/jcip-annotations-1.0.jar:/opt/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/opt/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/opt/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jsch-0.1.54.jar:/opt/hadoop/share/hadoop/common/lib/json-smart-1.1.1.jar:/opt/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/opt/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/common/lib/junit-4.11.jar:/opt/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/opt/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-3.9.jar:/opt/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/opt/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/opt/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/opt/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/opt/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/opt/hadoop/share/hadoop/common/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/opt/hadoop/share/hadoop/common/hadoop-common-2.8.2-tests.jar:/opt/hadoop/share/hadoop/common/hadoop-common-2.8.2.jar:/opt/hadoop/share/hadoop/common/hadoop-nfs-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs:/opt/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/okio-1.4.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/opt/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/opt/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-math-2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/opt/hadoop/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/opt/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/opt/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/opt/hadoop/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/opt/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/opt/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/opt/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/opt/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/opt/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/opt/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/opt/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/opt/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.2-tests.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.2.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 66c47f2a01ad9637879e95f80c41f798373828fb; compiled by 'jdu' on 2017-10-19T20:39Z
STARTUP_MSG:   java = 1.8.0_152
************************************************************/
2018-01-11 17:22:05,006 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2018-01-11 17:22:08,400 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2018-01-11 17:22:08,686 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2018-01-11 17:22:08,686 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2018-01-11 17:22:08,777 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2018-01-11 17:22:08,779 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is hadoop-worker01.local
2018-01-11 17:22:08,802 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2018-01-11 17:22:08,940 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2018-01-11 17:22:08,948 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 10485760 bytes/s
2018-01-11 17:22:08,948 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 50
2018-01-11 17:22:10,063 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2018-01-11 17:22:10,172 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2018-01-11 17:22:10,203 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2018-01-11 17:22:10,217 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2018-01-11 17:22:10,220 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2018-01-11 17:22:10,221 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2018-01-11 17:22:10,221 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2018-01-11 17:22:10,303 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 34675
2018-01-11 17:22:10,303 INFO org.mortbay.log: jetty-6.1.26
2018-01-11 17:22:11,321 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:34675
2018-01-11 17:22:12,435 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2018-01-11 17:22:12,614 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2018-01-11 17:22:13,700 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hadoop
2018-01-11 17:22:13,701 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2018-01-11 17:22:13,897 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2018-01-11 17:22:13,977 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2018-01-11 17:22:14,216 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2018-01-11 17:22:14,260 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2018-01-11 17:22:14,324 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2018-01-11 17:22:14,367 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to hadoop-master/192.168.28.129:8020 starting to offer service
2018-01-11 17:22:14,473 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2018-01-11 17:22:14,507 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2018-01-11 17:22:15,660 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to hadoop-master/192.168.28.129:8020
2018-01-11 17:22:15,741 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2018-01-11 17:22:15,792 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /var/lib/hadoop2/dfs/data/1/in_use.lock acquired by nodename 2487@hadoop-worker01.local
2018-01-11 17:22:15,916 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /var/lib/hadoop2/dfs/data/2/in_use.lock acquired by nodename 2487@hadoop-worker01.local
2018-01-11 17:22:16,254 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-199680249-192.168.28.129-1514169681712
2018-01-11 17:22:16,254 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712
2018-01-11 17:22:16,849 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-199680249-192.168.28.129-1514169681712
2018-01-11 17:22:16,849 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712
2018-01-11 17:22:16,881 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1794692519;bpid=BP-199680249-192.168.28.129-1514169681712;lv=-57;nsInfo=lv=-63;cid=CID-a84222ca-b260-441d-81c3-93c4f4f5d131;nsid=1794692519;c=1514169681712;bpid=BP-199680249-192.168.28.129-1514169681712;dnuuid=46e3cb45-0c76-4bc1-9342-af90beb9d892
2018-01-11 17:22:17,362 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-db2b9f5a-2dbc-404d-8b87-9ff8e79ccdd2
2018-01-11 17:22:17,362 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /var/lib/hadoop2/dfs/data/1/current, StorageType: DISK
2018-01-11 17:22:17,375 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-99e39a6f-a3c8-4fc0-9709-0aafdcfc87d6
2018-01-11 17:22:17,375 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /var/lib/hadoop2/dfs/data/2/current, StorageType: DISK
2018-01-11 17:22:17,709 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2018-01-11 17:22:17,718 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Volume reference is released.
2018-01-11 17:22:17,718 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-199680249-192.168.28.129-1514169681712
2018-01-11 17:22:17,749 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current...
2018-01-11 17:22:17,853 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current...
2018-01-11 17:22:18,166 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-199680249-192.168.28.129-1514169681712 on /var/lib/hadoop2/dfs/data/1/current: 346ms
2018-01-11 17:22:18,314 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-199680249-192.168.28.129-1514169681712 on /var/lib/hadoop2/dfs/data/2/current: 461ms
2018-01-11 17:22:18,314 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-199680249-192.168.28.129-1514169681712: 596ms
2018-01-11 17:22:18,319 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current...
2018-01-11 17:22:18,319 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/replicas doesn't exist 
2018-01-11 17:22:18,377 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current...
2018-01-11 17:22:18,377 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/replicas doesn't exist 
2018-01-11 17:22:18,418 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current: 99ms
2018-01-11 17:22:18,422 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current: 44ms
2018-01-11 17:22:18,426 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 108ms
2018-01-11 17:22:18,572 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/var/lib/hadoop2/dfs/data/2, DS-99e39a6f-a3c8-4fc0-9709-0aafdcfc87d6): no suitable block pools found to scan.  Waiting 325188157 ms.
2018-01-11 17:22:18,572 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/var/lib/hadoop2/dfs/data/1, DS-db2b9f5a-2dbc-404d-8b87-9ff8e79ccdd2): no suitable block pools found to scan.  Waiting 325188156 ms.
2018-01-11 17:22:18,600 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1/11/18 9:33 PM with interval of 21600000ms
2018-01-11 17:22:18,864 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-199680249-192.168.28.129-1514169681712 (Datanode Uuid 46e3cb45-0c76-4bc1-9342-af90beb9d892) service to hadoop-master/192.168.28.129:8020 beginning handshake with NN
2018-01-11 17:22:19,150 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-199680249-192.168.28.129-1514169681712 (Datanode Uuid 46e3cb45-0c76-4bc1-9342-af90beb9d892) service to hadoop-master/192.168.28.129:8020 successfully registered with NN
2018-01-11 17:22:19,150 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode hadoop-master/192.168.28.129:8020 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2018-01-11 17:22:19,712 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x5830a7dca92241c5,  containing 2 storage report(s), of which we sent 2. The reports had 31 total blocks and used 1 RPC(s). This took 8 msec to generate and 289 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-01-11 17:22:19,712 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-199680249-192.168.28.129-1514169681712
2018-01-11 17:28:40,733 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "hadoop-worker01.local/192.168.28.131"; destination host is: "hadoop-master":8020; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1487)
	at org.apache.hadoop.ipc.Client.call(Client.java:1429)
	at org.apache.hadoop.ipc.Client.call(Client.java:1339)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:154)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:459)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:581)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:775)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1788)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1157)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1053)
2018-01-11 17:28:44,272 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2018-01-11 17:28:44,277 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at hadoop-worker01.local/192.168.28.131
************************************************************/
2018-01-11 17:34:52,629 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   user = hadoop
STARTUP_MSG:   host = hadoop-worker01.local/192.168.28.131
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.2
STARTUP_MSG:   classpath = /opt/hadoop-2.8.2/etc/hadoop:/opt/hadoop/share/hadoop/common/lib/activation-1.1.jar:/opt/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/opt/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/opt/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/opt/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/opt/hadoop/share/hadoop/common/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/opt/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/opt/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/opt/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/opt/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/opt/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-annotations-2.8.2.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-auth-2.8.2.jar:/opt/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/opt/hadoop/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/opt/hadoop/share/hadoop/common/lib/httpclient-4.5.2.jar:/opt/hadoop/share/hadoop/common/lib/httpcore-4.4.4.jar:/opt/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/common/lib/jcip-annotations-1.0.jar:/opt/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/opt/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/opt/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jsch-0.1.54.jar:/opt/hadoop/share/hadoop/common/lib/json-smart-1.1.1.jar:/opt/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/opt/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/common/lib/junit-4.11.jar:/opt/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/opt/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-3.9.jar:/opt/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/opt/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/opt/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/opt/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/opt/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/opt/hadoop/share/hadoop/common/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/opt/hadoop/share/hadoop/common/hadoop-common-2.8.2-tests.jar:/opt/hadoop/share/hadoop/common/hadoop-common-2.8.2.jar:/opt/hadoop/share/hadoop/common/hadoop-nfs-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs:/opt/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/okio-1.4.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/opt/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/opt/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-math-2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/opt/hadoop/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/opt/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/opt/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/opt/hadoop/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/opt/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/opt/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/opt/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/opt/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/opt/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/opt/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/opt/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/opt/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.2-tests.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.2.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 66c47f2a01ad9637879e95f80c41f798373828fb; compiled by 'jdu' on 2017-10-19T20:39Z
STARTUP_MSG:   java = 1.8.0_152
************************************************************/
2018-01-11 17:34:52,660 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2018-01-11 17:34:55,979 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2018-01-11 17:34:56,579 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2018-01-11 17:34:56,579 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2018-01-11 17:34:56,696 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2018-01-11 17:34:56,713 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is hadoop-worker01.local
2018-01-11 17:34:56,722 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2018-01-11 17:34:56,805 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2018-01-11 17:34:56,826 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 10485760 bytes/s
2018-01-11 17:34:56,826 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 50
2018-01-11 17:34:57,621 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2018-01-11 17:34:57,664 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2018-01-11 17:34:57,723 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2018-01-11 17:34:57,735 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2018-01-11 17:34:57,738 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2018-01-11 17:34:57,738 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2018-01-11 17:34:57,738 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2018-01-11 17:34:57,780 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 36446
2018-01-11 17:34:57,780 INFO org.mortbay.log: jetty-6.1.26
2018-01-11 17:34:58,989 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:36446
2018-01-11 17:34:59,721 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2018-01-11 17:34:59,776 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2018-01-11 17:35:00,696 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hadoop
2018-01-11 17:35:00,696 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2018-01-11 17:35:00,979 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2018-01-11 17:35:01,022 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2018-01-11 17:35:01,426 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2018-01-11 17:35:01,443 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2018-01-11 17:35:01,495 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2018-01-11 17:35:01,552 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to hadoop-master/192.168.28.129:8020 starting to offer service
2018-01-11 17:35:01,843 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2018-01-11 17:35:01,745 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2018-01-11 17:35:02,688 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to hadoop-master/192.168.28.129:8020
2018-01-11 17:35:02,746 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2018-01-11 17:35:02,829 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /var/lib/hadoop2/dfs/data/1/in_use.lock acquired by nodename 1681@hadoop-worker01.local
2018-01-11 17:35:02,913 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /var/lib/hadoop2/dfs/data/2/in_use.lock acquired by nodename 1681@hadoop-worker01.local
2018-01-11 17:35:03,150 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-199680249-192.168.28.129-1514169681712
2018-01-11 17:35:03,150 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712
2018-01-11 17:35:03,589 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-199680249-192.168.28.129-1514169681712
2018-01-11 17:35:03,589 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712
2018-01-11 17:35:03,657 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1794692519;bpid=BP-199680249-192.168.28.129-1514169681712;lv=-57;nsInfo=lv=-63;cid=CID-a84222ca-b260-441d-81c3-93c4f4f5d131;nsid=1794692519;c=1514169681712;bpid=BP-199680249-192.168.28.129-1514169681712;dnuuid=46e3cb45-0c76-4bc1-9342-af90beb9d892
2018-01-11 17:35:04,246 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-db2b9f5a-2dbc-404d-8b87-9ff8e79ccdd2
2018-01-11 17:35:04,246 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /var/lib/hadoop2/dfs/data/1/current, StorageType: DISK
2018-01-11 17:35:04,250 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-99e39a6f-a3c8-4fc0-9709-0aafdcfc87d6
2018-01-11 17:35:04,251 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /var/lib/hadoop2/dfs/data/2/current, StorageType: DISK
2018-01-11 17:35:04,262 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2018-01-11 17:35:04,414 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Volume reference is released.
2018-01-11 17:35:04,414 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-199680249-192.168.28.129-1514169681712
2018-01-11 17:35:04,415 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current...
2018-01-11 17:35:04,454 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current...
2018-01-11 17:35:04,480 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current: 437198848
2018-01-11 17:35:04,481 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current: 617148416
2018-01-11 17:35:04,485 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-199680249-192.168.28.129-1514169681712 on /var/lib/hadoop2/dfs/data/1/current: 32ms
2018-01-11 17:35:04,488 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-199680249-192.168.28.129-1514169681712 on /var/lib/hadoop2/dfs/data/2/current: 34ms
2018-01-11 17:35:04,488 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-199680249-192.168.28.129-1514169681712: 74ms
2018-01-11 17:35:04,493 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current...
2018-01-11 17:35:04,493 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/replicas doesn't exist 
2018-01-11 17:35:04,493 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current...
2018-01-11 17:35:04,493 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/replicas doesn't exist 
2018-01-11 17:35:04,507 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current: 14ms
2018-01-11 17:35:04,511 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current: 18ms
2018-01-11 17:35:04,512 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 21ms
2018-01-11 17:35:04,658 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/var/lib/hadoop2/dfs/data/2, DS-99e39a6f-a3c8-4fc0-9709-0aafdcfc87d6): no suitable block pools found to scan.  Waiting 324422070 ms.
2018-01-11 17:35:04,663 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/var/lib/hadoop2/dfs/data/1, DS-db2b9f5a-2dbc-404d-8b87-9ff8e79ccdd2): no suitable block pools found to scan.  Waiting 324422065 ms.
2018-01-11 17:35:04,688 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1/11/18 10:31 PM with interval of 21600000ms
2018-01-11 17:35:04,939 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-199680249-192.168.28.129-1514169681712 (Datanode Uuid 46e3cb45-0c76-4bc1-9342-af90beb9d892) service to hadoop-master/192.168.28.129:8020 beginning handshake with NN
2018-01-11 17:35:05,181 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-199680249-192.168.28.129-1514169681712 (Datanode Uuid 46e3cb45-0c76-4bc1-9342-af90beb9d892) service to hadoop-master/192.168.28.129:8020 successfully registered with NN
2018-01-11 17:35:05,181 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode hadoop-master/192.168.28.129:8020 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2018-01-11 17:35:05,788 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xab42ac4fa308c8aa,  containing 2 storage report(s), of which we sent 2. The reports had 31 total blocks and used 1 RPC(s). This took 19 msec to generate and 150 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-01-11 17:35:05,788 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-199680249-192.168.28.129-1514169681712
2018-01-11 17:36:55,452 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 7002ms
No GCs detected
2018-01-11 17:42:55,087 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741913_1089 src: /192.168.28.134:48116 dest: /192.168.28.131:50010
2018-01-11 17:42:55,803 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.134:48116, dest: /192.168.28.131:50010, bytes: 63213, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_358121713_50, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741913_1089, duration: 629869147
2018-01-11 17:42:55,805 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741913_1089, type=LAST_IN_PIPELINE terminating
2018-01-11 17:46:28,945 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741906_1082 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741906 for deletion
2018-01-11 17:46:28,946 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741844_1020 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741844 for deletion
2018-01-11 17:46:28,947 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741906_1082 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741906
2018-01-11 17:46:28,949 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741844_1020 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741844
2018-01-11 17:47:16,964 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "hadoop-worker01.local/192.168.28.131"; destination host is: "hadoop-master":8020; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1487)
	at org.apache.hadoop.ipc.Client.call(Client.java:1429)
	at org.apache.hadoop.ipc.Client.call(Client.java:1339)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:154)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:459)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:581)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:775)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1788)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1157)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1053)
2018-01-11 17:47:26,433 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2018-01-11 17:47:26,444 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at hadoop-worker01.local/192.168.28.131
************************************************************/
2018-01-11 18:49:15,404 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   user = hadoop
STARTUP_MSG:   host = hadoop-worker01.local/192.168.28.131
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.2
STARTUP_MSG:   classpath = /opt/hadoop-2.8.2/etc/hadoop:/opt/hadoop/share/hadoop/common/lib/activation-1.1.jar:/opt/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/opt/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/opt/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/opt/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/opt/hadoop/share/hadoop/common/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/opt/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/opt/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/opt/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/opt/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/opt/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-annotations-2.8.2.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-auth-2.8.2.jar:/opt/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/opt/hadoop/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/opt/hadoop/share/hadoop/common/lib/httpclient-4.5.2.jar:/opt/hadoop/share/hadoop/common/lib/httpcore-4.4.4.jar:/opt/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/common/lib/jcip-annotations-1.0.jar:/opt/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/opt/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/opt/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jsch-0.1.54.jar:/opt/hadoop/share/hadoop/common/lib/json-smart-1.1.1.jar:/opt/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/opt/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/common/lib/junit-4.11.jar:/opt/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/opt/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-3.9.jar:/opt/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/opt/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/opt/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/opt/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/opt/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/opt/hadoop/share/hadoop/common/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/opt/hadoop/share/hadoop/common/hadoop-common-2.8.2-tests.jar:/opt/hadoop/share/hadoop/common/hadoop-common-2.8.2.jar:/opt/hadoop/share/hadoop/common/hadoop-nfs-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs:/opt/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/okio-1.4.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/opt/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/opt/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-math-2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/opt/hadoop/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/opt/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/opt/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/opt/hadoop/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/opt/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/opt/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/opt/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/opt/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/opt/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/opt/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/opt/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/opt/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.2-tests.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.2.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 66c47f2a01ad9637879e95f80c41f798373828fb; compiled by 'jdu' on 2017-10-19T20:39Z
STARTUP_MSG:   java = 1.8.0_152
************************************************************/
2018-01-11 18:49:15,433 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2018-01-11 18:49:20,436 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2018-01-11 18:49:20,809 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2018-01-11 18:49:20,809 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2018-01-11 18:49:20,837 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2018-01-11 18:49:20,839 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is hadoop-worker01.local
2018-01-11 18:49:20,846 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2018-01-11 18:49:20,962 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2018-01-11 18:49:20,967 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 10485760 bytes/s
2018-01-11 18:49:20,967 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 50
2018-01-11 18:49:21,656 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2018-01-11 18:49:21,832 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2018-01-11 18:49:21,848 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2018-01-11 18:49:21,858 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2018-01-11 18:49:21,872 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2018-01-11 18:49:21,872 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2018-01-11 18:49:21,872 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2018-01-11 18:49:21,913 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 40716
2018-01-11 18:49:21,914 INFO org.mortbay.log: jetty-6.1.26
2018-01-11 18:49:22,963 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:40716
2018-01-11 18:49:24,551 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2018-01-11 18:49:24,596 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2018-01-11 18:49:25,388 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hadoop
2018-01-11 18:49:25,388 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2018-01-11 18:49:25,753 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2018-01-11 18:49:25,862 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2018-01-11 18:49:26,121 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2018-01-11 18:49:26,159 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2018-01-11 18:49:26,194 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2018-01-11 18:49:26,273 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to hadoop-master/192.168.28.129:8020 starting to offer service
2018-01-11 18:49:26,372 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2018-01-11 18:49:26,374 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2018-01-11 18:49:27,507 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to hadoop-master/192.168.28.129:8020
2018-01-11 18:49:27,632 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2018-01-11 18:49:27,759 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /var/lib/hadoop2/dfs/data/1/in_use.lock acquired by nodename 2450@hadoop-worker01.local
2018-01-11 18:49:27,853 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /var/lib/hadoop2/dfs/data/2/in_use.lock acquired by nodename 2450@hadoop-worker01.local
2018-01-11 18:49:28,281 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-199680249-192.168.28.129-1514169681712
2018-01-11 18:49:28,281 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712
2018-01-11 18:49:28,423 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-199680249-192.168.28.129-1514169681712
2018-01-11 18:49:28,423 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712
2018-01-11 18:49:28,425 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1794692519;bpid=BP-199680249-192.168.28.129-1514169681712;lv=-57;nsInfo=lv=-63;cid=CID-a84222ca-b260-441d-81c3-93c4f4f5d131;nsid=1794692519;c=1514169681712;bpid=BP-199680249-192.168.28.129-1514169681712;dnuuid=46e3cb45-0c76-4bc1-9342-af90beb9d892
2018-01-11 18:49:28,625 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-db2b9f5a-2dbc-404d-8b87-9ff8e79ccdd2
2018-01-11 18:49:28,625 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /var/lib/hadoop2/dfs/data/1/current, StorageType: DISK
2018-01-11 18:49:28,630 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-99e39a6f-a3c8-4fc0-9709-0aafdcfc87d6
2018-01-11 18:49:28,630 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /var/lib/hadoop2/dfs/data/2/current, StorageType: DISK
2018-01-11 18:49:28,702 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2018-01-11 18:49:28,752 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Volume reference is released.
2018-01-11 18:49:28,752 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-199680249-192.168.28.129-1514169681712
2018-01-11 18:49:28,770 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current...
2018-01-11 18:49:28,790 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current...
2018-01-11 18:49:29,081 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-199680249-192.168.28.129-1514169681712 on /var/lib/hadoop2/dfs/data/1/current: 291ms
2018-01-11 18:49:29,112 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-199680249-192.168.28.129-1514169681712 on /var/lib/hadoop2/dfs/data/2/current: 322ms
2018-01-11 18:49:29,113 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-199680249-192.168.28.129-1514169681712: 361ms
2018-01-11 18:49:29,118 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current...
2018-01-11 18:49:29,119 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/replicas doesn't exist 
2018-01-11 18:49:29,119 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current...
2018-01-11 18:49:29,119 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/replicas doesn't exist 
2018-01-11 18:49:29,152 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current: 33ms
2018-01-11 18:49:29,161 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current: 43ms
2018-01-11 18:49:29,162 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 46ms
2018-01-11 18:49:29,292 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/var/lib/hadoop2/dfs/data/2, DS-99e39a6f-a3c8-4fc0-9709-0aafdcfc87d6): no suitable block pools found to scan.  Waiting 319957436 ms.
2018-01-11 18:49:29,297 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/var/lib/hadoop2/dfs/data/1, DS-db2b9f5a-2dbc-404d-8b87-9ff8e79ccdd2): no suitable block pools found to scan.  Waiting 319957431 ms.
2018-01-11 18:49:29,309 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1/12/18 12:10 AM with interval of 21600000ms
2018-01-11 18:49:29,322 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-199680249-192.168.28.129-1514169681712 (Datanode Uuid 46e3cb45-0c76-4bc1-9342-af90beb9d892) service to hadoop-master/192.168.28.129:8020 beginning handshake with NN
2018-01-11 18:49:29,498 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-199680249-192.168.28.129-1514169681712 (Datanode Uuid 46e3cb45-0c76-4bc1-9342-af90beb9d892) service to hadoop-master/192.168.28.129:8020 successfully registered with NN
2018-01-11 18:49:29,498 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode hadoop-master/192.168.28.129:8020 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2018-01-11 18:49:29,899 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x5d5bdbf75e5c9ae7,  containing 2 storage report(s), of which we sent 2. The reports had 30 total blocks and used 1 RPC(s). This took 10 msec to generate and 193 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-01-11 18:49:29,899 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-199680249-192.168.28.129-1514169681712
2018-01-11 19:02:01,906 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741914_1090 src: /192.168.28.130:47228 dest: /192.168.28.131:50010
2018-01-11 19:02:03,471 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.130:47228, dest: /192.168.28.131:50010, bytes: 63213, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1262417118_99, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741914_1090, duration: 919639179
2018-01-11 19:02:03,474 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741914_1090, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[192.168.28.132:50010, 192.168.28.134:50010] terminating
2018-01-11 19:02:06,102 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741913_1089 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741913 for deletion
2018-01-11 19:02:06,114 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741913_1089 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741913
2018-01-11 19:04:44,690 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741914_1090 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741914 for deletion
2018-01-11 19:04:44,693 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741914_1090 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741914
2018-01-11 19:08:14,409 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741915_1091 src: /192.168.28.132:57978 dest: /192.168.28.131:50010
2018-01-11 19:08:14,435 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.132:57978, dest: /192.168.28.131:50010, bytes: 63213, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1353133492_145, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741915_1091, duration: 21515192
2018-01-11 19:08:14,435 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741915_1091, type=LAST_IN_PIPELINE terminating
2018-01-11 19:11:36,952 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741916_1092 src: /192.168.28.134:48138 dest: /192.168.28.131:50010
2018-01-11 19:11:43,510 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.134:48138, dest: /192.168.28.131:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2117759426_12, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741916_1092, duration: 6550653740
2018-01-11 19:11:43,510 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741916_1092, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.28.132:50010] terminating
2018-01-11 19:11:43,546 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741917_1093 src: /192.168.28.130:47468 dest: /192.168.28.131:50010
2018-01-11 19:11:48,000 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.130:47468, dest: /192.168.28.131:50010, bytes: 75090821, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2117759426_12, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741917_1093, duration: 4444170911
2018-01-11 19:11:48,001 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741917_1093, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[192.168.28.132:50010, 192.168.28.134:50010] terminating
2018-01-11 19:11:48,263 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741918_1094 src: /192.168.28.134:48140 dest: /192.168.28.131:50010
2018-01-11 19:11:48,286 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.134:48140, dest: /192.168.28.131:50010, bytes: 66113, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2117759426_12, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741918_1094, duration: 14237623
2018-01-11 19:11:48,286 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741918_1094, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.28.132:50010] terminating
2018-01-11 19:24:12,508 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741916_1092 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741916 for deletion
2018-01-11 19:24:12,510 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741917_1093 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741917 for deletion
2018-01-11 19:24:12,514 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741916_1092 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741916
2018-01-11 19:24:12,515 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741918_1094 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741918 for deletion
2018-01-11 19:24:12,516 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741917_1093 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741917
2018-01-11 19:24:12,518 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741918_1094 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741918
2018-01-11 19:26:37,295 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741919_1095 src: /192.168.28.134:48164 dest: /192.168.28.131:50010
2018-01-11 19:26:42,613 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.134:48164, dest: /192.168.28.131:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-780261031_12, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741919_1095, duration: 5315898703
2018-01-11 19:26:42,614 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741919_1095, type=LAST_IN_PIPELINE terminating
2018-01-11 19:26:42,727 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741920_1096 src: /192.168.28.132:58008 dest: /192.168.28.131:50010
2018-01-11 19:26:46,067 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.132:58008, dest: /192.168.28.131:50010, bytes: 75090821, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-780261031_12, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741920_1096, duration: 3338368422
2018-01-11 19:26:46,068 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741920_1096, type=LAST_IN_PIPELINE terminating
2018-01-11 19:26:46,420 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741921_1097 src: /192.168.28.132:58010 dest: /192.168.28.131:50010
2018-01-11 19:26:46,465 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.132:58010, dest: /192.168.28.131:50010, bytes: 66113, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-780261031_12, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741921_1097, duration: 29547369
2018-01-11 19:26:46,465 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741921_1097, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.28.134:50010] terminating
2018-01-11 19:36:41,116 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 3033ms
No GCs detected
2018-01-11 19:42:50,191 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741920_1096 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741920 for deletion
2018-01-11 19:42:50,191 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741921_1097 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741921 for deletion
2018-01-11 19:42:50,193 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741920_1096 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741920
2018-01-11 19:42:50,193 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741919_1095 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741919 for deletion
2018-01-11 19:42:50,193 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741921_1097 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741921
2018-01-11 19:42:50,195 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741919_1095 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741919
2018-01-11 19:44:02,384 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741922_1098 src: /192.168.28.130:47996 dest: /192.168.28.131:50010
2018-01-11 19:44:08,235 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.130:47996, dest: /192.168.28.131:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-531081096_12, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741922_1098, duration: 5820358532
2018-01-11 19:44:08,236 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741922_1098, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[192.168.28.134:50010, 192.168.28.132:50010] terminating
2018-01-11 19:44:08,271 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741923_1099 src: /192.168.28.130:47998 dest: /192.168.28.131:50010
2018-01-11 19:44:10,910 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.130:47998, dest: /192.168.28.131:50010, bytes: 75090821, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-531081096_12, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741923_1099, duration: 2598049524
2018-01-11 19:44:10,910 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741923_1099, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[192.168.28.132:50010, 192.168.28.134:50010] terminating
2018-01-11 19:44:11,186 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741924_1100 src: /192.168.28.132:58014 dest: /192.168.28.131:50010
2018-01-11 19:44:11,205 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.132:58014, dest: /192.168.28.131:50010, bytes: 66113, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-531081096_12, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741924_1100, duration: 12709994
2018-01-11 19:44:11,205 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741924_1100, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.28.134:50010] terminating
2018-01-11 21:04:32,600 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 4475ms
No GCs detected
2018-01-11 21:04:35,638 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "hadoop-worker01.local/192.168.28.131"; destination host is: "hadoop-master":8020; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1487)
	at org.apache.hadoop.ipc.Client.call(Client.java:1429)
	at org.apache.hadoop.ipc.Client.call(Client.java:1339)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:154)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:459)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:581)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:775)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1788)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1157)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1053)
2018-01-11 21:04:39,613 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hadoop-master/192.168.28.129:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-01-11 21:04:40,516 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2018-01-11 21:04:40,531 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at hadoop-worker01.local/192.168.28.131
************************************************************/
2018-01-25 15:34:22,047 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   user = hadoop
STARTUP_MSG:   host = hadoop-worker01.local/192.168.28.131
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.2
STARTUP_MSG:   classpath = /opt/hadoop-2.8.2/etc/hadoop:/opt/hadoop/share/hadoop/common/lib/activation-1.1.jar:/opt/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/opt/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/opt/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/opt/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/opt/hadoop/share/hadoop/common/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/opt/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/opt/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/opt/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/opt/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/opt/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/opt/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-annotations-2.8.2.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-auth-2.8.2.jar:/opt/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/opt/hadoop/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/opt/hadoop/share/hadoop/common/lib/httpclient-4.5.2.jar:/opt/hadoop/share/hadoop/common/lib/httpcore-4.4.4.jar:/opt/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/common/lib/jcip-annotations-1.0.jar:/opt/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/opt/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/opt/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/common/lib/jsch-0.1.54.jar:/opt/hadoop/share/hadoop/common/lib/json-smart-1.1.1.jar:/opt/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/opt/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/common/lib/junit-4.11.jar:/opt/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/opt/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-3.9.jar:/opt/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/opt/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/opt/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/opt/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/opt/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/opt/hadoop/share/hadoop/common/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/opt/hadoop/share/hadoop/common/hadoop-common-2.8.2-tests.jar:/opt/hadoop/share/hadoop/common/hadoop-common-2.8.2.jar:/opt/hadoop/share/hadoop/common/hadoop-nfs-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs:/opt/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/okio-1.4.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/opt/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.2-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.2.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/opt/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/yarn/lib/commons-math-2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/opt/hadoop/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/opt/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/opt/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/opt/hadoop/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/opt/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/opt/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/opt/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/opt/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/opt/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/opt/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/opt/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/opt/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.2.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/opt/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/opt/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/opt/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/opt/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/opt/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/opt/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/opt/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.2-tests.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.2.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.2.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 66c47f2a01ad9637879e95f80c41f798373828fb; compiled by 'jdu' on 2017-10-19T20:39Z
STARTUP_MSG:   java = 1.8.0_152
************************************************************/
2018-01-25 15:34:22,092 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2018-01-25 15:34:28,948 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2018-01-25 15:34:29,421 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2018-01-25 15:34:29,421 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2018-01-25 15:34:29,443 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2018-01-25 15:34:29,447 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is hadoop-worker01.local
2018-01-25 15:34:29,458 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2018-01-25 15:34:29,566 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2018-01-25 15:34:29,572 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 10485760 bytes/s
2018-01-25 15:34:29,572 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 50
2018-01-25 15:34:30,309 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2018-01-25 15:34:30,411 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2018-01-25 15:34:30,483 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2018-01-25 15:34:30,498 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2018-01-25 15:34:30,501 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2018-01-25 15:34:30,501 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2018-01-25 15:34:30,502 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2018-01-25 15:34:30,556 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 46691
2018-01-25 15:34:30,556 INFO org.mortbay.log: jetty-6.1.26
2018-01-25 15:34:31,944 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:46691
2018-01-25 15:34:33,049 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2018-01-25 15:34:33,149 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2018-01-25 15:34:34,334 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = hadoop
2018-01-25 15:34:34,334 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2018-01-25 15:34:34,883 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2018-01-25 15:34:34,979 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2018-01-25 15:34:35,347 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2018-01-25 15:34:35,409 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2018-01-25 15:34:35,621 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2018-01-25 15:34:35,684 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to hadoop-master/192.168.28.129:8020 starting to offer service
2018-01-25 15:34:35,925 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2018-01-25 15:34:35,925 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2018-01-25 15:34:38,020 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to hadoop-master/192.168.28.129:8020
2018-01-25 15:34:38,084 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2018-01-25 15:34:38,413 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /var/lib/hadoop2/dfs/data/1/in_use.lock acquired by nodename 1773@hadoop-worker01.local
2018-01-25 15:34:38,480 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /var/lib/hadoop2/dfs/data/2/in_use.lock acquired by nodename 1773@hadoop-worker01.local
2018-01-25 15:34:38,793 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-199680249-192.168.28.129-1514169681712
2018-01-25 15:34:38,793 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712
2018-01-25 15:34:39,319 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-199680249-192.168.28.129-1514169681712
2018-01-25 15:34:39,319 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712
2018-01-25 15:34:39,353 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1794692519;bpid=BP-199680249-192.168.28.129-1514169681712;lv=-57;nsInfo=lv=-63;cid=CID-a84222ca-b260-441d-81c3-93c4f4f5d131;nsid=1794692519;c=1514169681712;bpid=BP-199680249-192.168.28.129-1514169681712;dnuuid=46e3cb45-0c76-4bc1-9342-af90beb9d892
2018-01-25 15:34:39,654 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-db2b9f5a-2dbc-404d-8b87-9ff8e79ccdd2
2018-01-25 15:34:39,654 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /var/lib/hadoop2/dfs/data/1/current, StorageType: DISK
2018-01-25 15:34:39,662 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-99e39a6f-a3c8-4fc0-9709-0aafdcfc87d6
2018-01-25 15:34:39,662 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /var/lib/hadoop2/dfs/data/2/current, StorageType: DISK
2018-01-25 15:34:39,930 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2018-01-25 15:34:40,882 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Volume reference is released.
2018-01-25 15:34:40,882 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-199680249-192.168.28.129-1514169681712
2018-01-25 15:34:40,890 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current...
2018-01-25 15:34:40,891 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current...
2018-01-25 15:34:41,121 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-199680249-192.168.28.129-1514169681712 on /var/lib/hadoop2/dfs/data/1/current: 204ms
2018-01-25 15:34:41,122 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-199680249-192.168.28.129-1514169681712 on /var/lib/hadoop2/dfs/data/2/current: 205ms
2018-01-25 15:34:41,122 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-199680249-192.168.28.129-1514169681712: 240ms
2018-01-25 15:34:41,126 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current...
2018-01-25 15:34:41,126 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current...
2018-01-25 15:34:41,126 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/replicas doesn't exist 
2018-01-25 15:34:41,126 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/replicas doesn't exist 
2018-01-25 15:34:41,143 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2/current: 16ms
2018-01-25 15:34:41,225 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1/current: 99ms
2018-01-25 15:34:41,243 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 118ms
2018-01-25 15:34:41,489 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: Now rescanning bpid BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/2, after more than 504 hour(s)
2018-01-25 15:34:41,556 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: Now rescanning bpid BP-199680249-192.168.28.129-1514169681712 on volume /var/lib/hadoop2/dfs/data/1, after more than 504 hour(s)
2018-01-25 15:34:42,190 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1/25/18 8:00 PM with interval of 21600000ms
2018-01-25 15:34:42,233 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-199680249-192.168.28.129-1514169681712 (Datanode Uuid 46e3cb45-0c76-4bc1-9342-af90beb9d892) service to hadoop-master/192.168.28.129:8020 beginning handshake with NN
2018-01-25 15:34:42,360 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-199680249-192.168.28.129-1514169681712 (Datanode Uuid 46e3cb45-0c76-4bc1-9342-af90beb9d892) service to hadoop-master/192.168.28.129:8020 successfully registered with NN
2018-01-25 15:34:42,360 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode hadoop-master/192.168.28.129:8020 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2018-01-25 15:34:42,630 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x5513ffb76bbc8de6,  containing 2 storage report(s), of which we sent 2. The reports had 33 total blocks and used 1 RPC(s). This took 12 msec to generate and 77 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-01-25 15:34:42,630 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-199680249-192.168.28.129-1514169681712
2018-01-25 15:35:17,734 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2581ms
No GCs detected
2018-01-25 15:35:20,579 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1718ms
No GCs detected
2018-01-25 15:35:24,761 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 3681ms
No GCs detected
2018-01-25 15:44:10,510 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/var/lib/hadoop2/dfs/data/2, DS-99e39a6f-a3c8-4fc0-9709-0aafdcfc87d6): finished scanning block pool BP-199680249-192.168.28.129-1514169681712
2018-01-25 15:44:10,517 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/var/lib/hadoop2/dfs/data/2, DS-99e39a6f-a3c8-4fc0-9709-0aafdcfc87d6): no suitable block pools found to scan.  Waiting 1813830972 ms.
2018-01-25 15:48:05,146 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/var/lib/hadoop2/dfs/data/1, DS-db2b9f5a-2dbc-404d-8b87-9ff8e79ccdd2): finished scanning block pool BP-199680249-192.168.28.129-1514169681712
2018-01-25 15:48:05,149 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/var/lib/hadoop2/dfs/data/1, DS-db2b9f5a-2dbc-404d-8b87-9ff8e79ccdd2): no suitable block pools found to scan.  Waiting 1813596407 ms.
2018-01-25 16:42:11,589 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741925_1101 src: /192.168.28.130:60780 dest: /192.168.28.131:50010
2018-01-25 16:42:22,806 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.130:60780, dest: /192.168.28.131:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-376932983_1, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741925_1101, duration: 10738198166
2018-01-25 16:42:22,806 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741925_1101, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[192.168.28.132:50010, 192.168.28.134:50010] terminating
2018-01-25 16:42:22,827 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741926_1102 src: /192.168.28.132:43640 dest: /192.168.28.131:50010
2018-01-25 16:42:35,783 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.132:43640, dest: /192.168.28.131:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-376932983_1, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741926_1102, duration: 8275449869
2018-01-25 16:42:35,784 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741926_1102, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.28.134:50010] terminating
2018-01-25 16:42:35,820 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741927_1103 src: /192.168.28.132:43642 dest: /192.168.28.131:50010
2018-01-25 16:42:42,417 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.132:43642, dest: /192.168.28.131:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-376932983_1, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741927_1103, duration: 6587783399
2018-01-25 16:42:42,417 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741927_1103, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.28.134:50010] terminating
2018-01-25 16:42:42,495 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741928_1104 src: /192.168.28.132:43644 dest: /192.168.28.131:50010
2018-01-25 16:42:47,902 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.132:43644, dest: /192.168.28.131:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-376932983_1, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741928_1104, duration: 5404407319
2018-01-25 16:42:47,903 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741928_1104, type=LAST_IN_PIPELINE terminating
2018-01-25 16:42:47,953 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741929_1105 src: /192.168.28.130:60792 dest: /192.168.28.131:50010
2018-01-25 16:42:52,915 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.130:60792, dest: /192.168.28.131:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-376932983_1, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741929_1105, duration: 4940527453
2018-01-25 16:42:52,915 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741929_1105, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[192.168.28.134:50010, 192.168.28.132:50010] terminating
2018-01-25 16:42:52,949 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741930_1106 src: /192.168.28.134:50964 dest: /192.168.28.131:50010
2018-01-25 16:42:53,391 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.134:50964, dest: /192.168.28.131:50010, bytes: 18324704, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-376932983_1, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741930_1106, duration: 440677607
2018-01-25 16:42:53,392 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741930_1106, type=LAST_IN_PIPELINE terminating
2018-01-25 16:52:02,881 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741931_1107 src: /192.168.28.134:50966 dest: /192.168.28.131:50010
2018-01-25 16:52:02,906 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.134:50966, dest: /192.168.28.131:50010, bytes: 2142, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-116525795_1, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741931_1107, duration: 20695557
2018-01-25 16:52:02,906 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741931_1107, type=LAST_IN_PIPELINE terminating
2018-01-25 16:52:47,325 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741931_1107 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741931 for deletion
2018-01-25 16:52:47,328 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741931_1107 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741931
2018-01-25 16:56:28,498 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741932_1108 src: /192.168.28.132:43650 dest: /192.168.28.131:50010
2018-01-25 16:56:28,525 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.132:43650, dest: /192.168.28.131:50010, bytes: 82492, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-944352120_1, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741932_1108, duration: 24029727
2018-01-25 16:56:28,526 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741932_1108, type=LAST_IN_PIPELINE terminating
2018-01-25 17:18:41,826 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x5513ffb76bbc8de7,  containing 2 storage report(s), of which we sent 2. The reports had 40 total blocks and used 1 RPC(s). This took 3 msec to generate and 24 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2018-01-25 17:18:41,828 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-199680249-192.168.28.129-1514169681712
2018-01-25 17:35:39,996 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741933_1109 src: /192.168.28.134:50970 dest: /192.168.28.131:50010
2018-01-25 17:35:46,348 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.134:50970, dest: /192.168.28.131:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_103600222_12, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741933_1109, duration: 6345642191
2018-01-25 17:35:46,349 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741933_1109, type=LAST_IN_PIPELINE terminating
2018-01-25 17:35:46,431 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741934_1110 src: /192.168.28.130:32896 dest: /192.168.28.131:50010
2018-01-25 17:35:50,200 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.130:32896, dest: /192.168.28.131:50010, bytes: 75090816, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_103600222_12, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741934_1110, duration: 3748028173
2018-01-25 17:35:50,201 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741934_1110, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=2:[192.168.28.134:50010, 192.168.28.132:50010] terminating
2018-01-25 17:35:50,527 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-199680249-192.168.28.129-1514169681712:blk_1073741935_1111 src: /192.168.28.132:43654 dest: /192.168.28.131:50010
2018-01-25 17:35:50,549 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /192.168.28.132:43654, dest: /192.168.28.131:50010, bytes: 66113, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_103600222_12, offset: 0, srvID: 46e3cb45-0c76-4bc1-9342-af90beb9d892, blockid: BP-199680249-192.168.28.129-1514169681712:blk_1073741935_1111, duration: 15483156
2018-01-25 17:35:50,550 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-199680249-192.168.28.129-1514169681712:blk_1073741935_1111, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[192.168.28.134:50010] terminating
2018-01-25 17:38:02,633 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741933_1109 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741933 for deletion
2018-01-25 17:38:02,634 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741934_1110 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741934 for deletion
2018-01-25 17:38:02,635 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741935_1111 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741935 for deletion
2018-01-25 17:38:02,637 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741934_1110 file /var/lib/hadoop2/dfs/data/2/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741934
2018-01-25 17:38:02,637 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741933_1109 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741933
2018-01-25 17:38:02,637 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-199680249-192.168.28.129-1514169681712 blk_1073741935_1111 file /var/lib/hadoop2/dfs/data/1/current/BP-199680249-192.168.28.129-1514169681712/current/finalized/subdir0/subdir0/blk_1073741935
2018-01-25 17:48:05,895 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "hadoop-worker01.local/192.168.28.131"; destination host is: "hadoop-master":8020; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1487)
	at org.apache.hadoop.ipc.Client.call(Client.java:1429)
	at org.apache.hadoop.ipc.Client.call(Client.java:1339)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:154)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:459)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:581)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:775)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1788)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1157)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1053)
2018-01-25 17:48:09,381 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2018-01-25 17:48:09,403 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at hadoop-worker01.local/192.168.28.131
************************************************************/
